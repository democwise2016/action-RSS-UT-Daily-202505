<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:itunes="http://www.itunes.com/dtds/podcast-1.0.dtd" xmlns:googleplay="http://www.google.com/schemas/play-podcasts/1.0" xmlns:media="http://www.rssboard.org/media-rss" version="2.0">
  <channel>
    <title><![CDATA[AI-最佳拍檔[YT+]]]></title>
    <link>http://www.youtube.com/feeds/videos.xml?channel_id=UCGWYKICLOE8Wxy7q3eYXmPA</link>
    <image>
      <url>https://yt3.googleusercontent.com/GpvF9XbD-stx6HR3BySXKvMqm_AySlczqmJKdkdZsloYQ9-rnoaLdCpOn0irmvqi3QYroccHNg=s900-b50-c-k-c0x008A95A5-no-rj</url>
      <title>AI-最佳拍檔[YT+]</title>
      <link>http://www.youtube.com/feeds/videos.xml?channel_id=UCGWYKICLOE8Wxy7q3eYXmPA</link>
    </image>
    <language>en-us</language>
    <atom:link href="https://www.youtube.com/feeds/videos.xml?channel_id=UCGWYKICLOE8Wxy7q3eYXmPA" rel="self" type="application/rss+xml"/>
    <copyright><![CDATA[AI-最佳拍檔[YT+]]]></copyright>
    <itunes:author><![CDATA[AI-最佳拍檔[YT+]]]></itunes:author>
    <itunes:summary>
      <![CDATA[
      <a href="https://www.youtube.com/channel/UCGWYKICLOE8Wxy7q3eYXmPA" target="_blank">https://www.youtube.com/channel/UCGWYKICLOE8Wxy7q3eYXmPA</a><br />
<br />
<a href="https://www.youtube.com/feeds/videos.xml?channel_id=UCGWYKICLOE8Wxy7q3eYXmPA" target="_blank">https://www.youtube.com/feeds/videos.xml?channel_id=UCGWYKICLOE8Wxy7q3eYXmPA</a>
      ]]>
    </itunes:summary>
    <description>
      <![CDATA[
      <a href="https://www.youtube.com/channel/UCGWYKICLOE8Wxy7q3eYXmPA" target="_blank">https://www.youtube.com/channel/UCGWYKICLOE8Wxy7q3eYXmPA</a><br />
<br />
<a href="https://www.youtube.com/feeds/videos.xml?channel_id=UCGWYKICLOE8Wxy7q3eYXmPA" target="_blank">https://www.youtube.com/feeds/videos.xml?channel_id=UCGWYKICLOE8Wxy7q3eYXmPA</a>
      ]]>
    </description>
    <itunes:owner>
      <itunes:name><![CDATA[AI-最佳拍檔[YT+]]]></itunes:name>
    </itunes:owner>
    <itunes:image href="https://yt3.googleusercontent.com/GpvF9XbD-stx6HR3BySXKvMqm_AySlczqmJKdkdZsloYQ9-rnoaLdCpOn0irmvqi3QYroccHNg=s900-b50-c-k-c0x008A95A5-no-rj"/>
<item>
      <title><![CDATA[【人工智能】英伟达的护城河 | 黄仁勋BG2专访 | 三种“缩放定律” | 与华尔街的认知分歧 | 英伟达的增长逻辑 | 战略绑定OpenAI | 极限协同设计 | CUDA | 中美竞争 | 未来]]></title>
      <link>https://www.youtube.com/watch?v=--HbDQk2-jA</link>
      <itunes:title><![CDATA[【人工智能】英伟达的护城河 | 黄仁勋BG2专访 | 三种“缩放定律” | 与华尔街的认知分歧 | 英伟达的增长逻辑 | 战略绑定OpenAI | 极限协同设计 | CUDA | 中美竞争 | 未来]]></itunes:title>
      <itunes:author><![CDATA[最佳拍档]]></itunes:author>
      <itunes:summary>
        <![CDATA[<hr style="clear:both" />

<p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://img.youtube.com/vi/--HbDQk2-jA/maxresdefault.jpg" /></p><p><a href="https://www.youtube.com/watch?v=--HbDQk2-jA">https://www.youtube.com/watch?v=--HbDQk2-jA</a></p><h1>值得閱讀的理由</h1> <ul> <li>深入了解<strong>黃仁勳對AI算力需求的獨到見解</strong>，包括他提出的「三重縮放定律」，揭示AI未來主要算力消耗將從訓練轉向使用。</li> <li>掌握<strong>NVIDIA如何構建其堅不可摧的商業護城河</strong>，不僅透過「極限協同設計」實現系統級創新，更透過開放生態策略應對競爭。</li> <li>理解<strong>黃仁勳對全球AI競賽、美中科技關係及AI社會影響的全面思考</strong>，包括他對「主權AI」的定義、對美中政策的批判，以及對「美國夢」作為人才磁石的深刻洞察。</li> </ul> <hr /> <h1>摘要</h1> <p>這段影片由「最佳拍檔」的「大飛」主持，回顧了NVIDIA執行長<strong>黃仁勳</strong>與知名播客BG2長達100多分鐘的獨家對話。這場訪談標題為「NVIDIA：OpenAI、計算的未來與美國夢」，被譽為黃仁勳近期資訊密度最高、含金量最足的一次分享。影片作者表示，黃仁勳在訪談中系統性地闡釋了華爾街與矽谷之間存在的巨大認知分歧，詳細剖析了NVIDIA看似堅不可摧的商業護城河，並分享了他對全球人工智能競賽、大國博弈及未來社會形態的完整思考。</p> <p><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_0.jpg" /></p></p> <hr /> <h2>AI算力需求的「三重縮放定律」</h2> <p>影片作者指出，一年前市場曾擔憂AI預訓練需求放緩會導致算力中心過剩，但黃仁勳當時大膽預測推理（Inference）需求將成長「十億倍」的量級，而事實證明，連這個預測都低估了實際需求。黃仁勳在訪談中系統性地提出了AI算力需求由三種「<strong>縮放定律</strong>」共同驅動的關鍵觀點：</p> <p>第一種是「<strong>預訓練縮放定律</strong>」（Pre-training Scaling Law），即模型越大、數據越多、訓練時間越長，模型就越智能。這是過去幾年大模型算力增長的主要驅動力。</p> <p>第二種是「<strong>後訓練縮放定律</strong>」（Post-training Scaling Law），指的是模型在預訓練後，透過強化學習和大量「試錯」、「推理」來精通特定技能的過程，這個「練習」過程本身需要消耗大量的推理計算。</p> <p>第三種是「<strong>推理時思維縮放定律</strong>」（Inference-time Thinking Scaling Law），黃仁勳認為這是市場最未理解透徹的「革命性」驅動因素。它描述AI在回答問題前，會進行內部「思考」、查證、梳理邏輯，這是一個多輪的內部推理過程，而非簡單一步計算。任務越複雜，AI「思考」時間越長，答案質量越高，而每一次內部循環都是一次計算消耗。</p> <p>這三重定律徹底改變了傳統認知，未來AI系統的絕大部分算力消耗將發生在「使用」階段，尤其當AI Agent能自主完成複雜任務時，其背後「思考」的計算資源將是過去的億萬倍。</p> <p><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_90.jpg" /></p></p> <hr /> <h2>黃仁勳解析NVIDIA增長邏輯：三大宏大敘事框架</h2> <p>影片作者提到，這種指數級的增長需求與華爾街的線性預測模型之間形成了黃仁勳所說的「<strong>巨大的認知分歧</strong>」。主持人布拉德·格斯特納直接指出，一方面，Sam Altman和Sundar Pichai等行業領袖談論「萬億級」算力投資；另一方面，華爾街分析師普遍預測NVIDIA的增長率將在2027年「斷崖式下跌」到約8%。黃仁勳為此給出了一個三層宏大敘事框架，闡釋NVIDIA的增長邏輯：</p> <p>第一層是「<strong>物理定律層面的轉變</strong>」，即通用計算時代已結束，未來是<strong>加速計算和AI計算</strong>的時代。隨著摩爾定律走到盡頭，靠CPU性能提升推動計算發展的模式已不可行。全球價值數萬億美元的CPU數據中心基礎設施，在下一輪更新時必須轉向GPU、TPU等加速計算架構。這是一個龐大的「<strong>存量市場替換</strong>」，而非創造新市場。</p> <p>第二層是「<strong>現有應用的遷移</strong>」，將網際網路的核心工作負載從CPU遷移到GPU，這足以驅動數百億美元的需求。黃仁勳以Meta、Google、Amazon、字節跳動等巨頭的核心業務（如搜索、推薦引擎、電商購物）為例，說明它們正全面轉向AI和GPU，以提供更好的個性化體驗和更高效率。這是在用更先進的技術重塑一個已服務全球40億人的龐大市場。</p> <p>第三層是「<strong>未來的增量</strong>」，指的是<strong>AI作為「智能工廠」對全球GDP的賦能</strong>。這是最令人興奮的部分，也是AI創造全新價值的地方。黃仁勳將AI工廠類比為工業革命中的「馬達」，透過生成Token來增強人類的智力勞動。他估算全球50萬億美元的「智力勞動」相關GDP，若每年需要10萬億美元的「AI Token生成服務」來增強，以50%毛利率計算，將需要5萬億美元的AI基礎設施來支撐。這三層框架清晰表明NVIDIA的增長既來自舊設施替換、現有業務升級，也來自未來AI創造的新價值。</p> <p><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_230.jpg" /></p></p> <hr /> <h2>NVIDIA與OpenAI的「星際之門」戰略結盟</h2> <p>影片作者接著探討了NVIDIA與OpenAI的「<strong>星際之門</strong>」（Stargate）計畫。黃仁勳明確指出，這不是一次普通的商業合作，而是一次「<strong>戰略綁定</strong>」，背後邏輯深遠。他毫不掩飾對OpenAI未來的看好，認為OpenAI很可能成為下一個數萬億美元級別的「超大規模公司」（Hyperscale Company），滲透到個人生活、企業辦公、工業生產等各領域。因此，能在OpenAI成為巨擘前進行投資，黃仁勳認為是「能想像到的最聰明投資之一」，且這是OpenAI主動給NVIDIA的機會。</p> <p>這次合作的深度和廣度遠超外界想像，黃仁勳將其拆分為三個層次：</p> <p>首先是「<strong>現有雲合作的延續</strong>」，NVIDIA將繼續與微軟合作，為OpenAI在Azure雲平台構建價值「數千億美元」的算力集群；同時，還會與甲骨文、軟銀合作，建設數個吉瓦（Gigawatts）級數據中心。</p> <p>其次是「<strong>幫助OpenAI自建基礎設施</strong>」，這是新合作的核心。過去OpenAI主要租用雲服務商資源，但現在將自行建設AI基礎設施，NVIDIA會從最底層開始參與，包括晶片設計、軟體開發、系統集成，甚至是整個AI工廠的規劃和營運，提供「<strong>全棧式</strong>」支持。</p> <p>最後是「<strong>建立直接的戰略關係</strong>」，黃仁勳將此關係類比為NVIDIA與Meta、Google、微軟、xAI等頂級科技公司的直接合作，意味著OpenAI規模已大到可與核心技術供應商直接、平等對話，形成深度綁定的戰略夥伴關係。</p> <p>黃仁勳指出OpenAI做出此調整的核心原因，是面臨「<strong>雙重指數級增長壓力</strong>」。一方面是「用戶增長指數」，AI越好用，用戶數量和使用頻率就指數級增長；另一方面是「計算增長指數」，每個用戶每次與AI交互所需的計算量也在指數級增長。這兩者疊加，意味著OpenAI的算力需求將以「<strong>指數的指數</strong>」速度增長，單靠租用雲服務已無法滿足，必須同時推進「租雲」和「自建」兩條路。</p> <p><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_450.jpg" /></p></p> <hr /> <h2>英偉達的堅實護城河：「極限協同設計」與系統級創新</h2> <p>影片作者指出，與OpenAI的緊密合作，反映出NVIDIA堅固的護城河。面對AMD、英特爾以及ASIC晶片公司的競爭，黃仁勳表示NVIDIA真正的護城河不是單一晶片的性能優勢，而是一種「<strong>極限協同設計</strong>」（Extreme Co-Design）的系統級創新能力。</p> <p>首先，他解釋了NVIDIA為何要搞「<strong>年度發布週期</strong>」。隨著摩爾定律失效，晶體管性能不再大幅提升，若不能快速提升整體性能，AI生成Token的成本將持續上升。為持續降低Token成本，唯一的辦法就是「系統級的創新」，這正是「極限協同設計」的由來。黃仁勳說，極限協同設計要求<strong>同步優化模型、演算法、系統和晶片</strong>，讓它們像一個整體一樣工作，而非各自為戰。這體現在晶片層面（并行研發CPU、GPU、網路晶片和NVLink）、系統層面（晶片整合優化，數據傳輸效率）、以及軟體層面（提供從底層驅動到上層應用庫的完整軟體棧），才使NVIDIA在一年內實現了30倍的性能提升。</p> <p>其次，這種系統能力還體現在「<strong>規模化部署</strong>」的巨大挑戰上。黃仁勳以Elon Musk的xAI公司需部署50萬個GPU為例，這不僅是硬體堆疊，還要考慮電力供應、散熱、數據傳輸延遲、系統穩定性等一系列問題。客戶之所以敢下數百億美元訂單給NVIDIA，是因為NVIDIA的架構經過市場驗證，能確保大規模系統穩定運行。這種「經得起考驗的規模化部署能力」本身就是一道很高的信任壁壘。黃仁勳強調，現在的競爭已是「我的整個<strong>AI工廠</strong>比你的AI工廠效率更高」。</p> <p>他甚至斷言，即使競爭對手把ASIC晶片免費送給客戶，客戶仍應選擇NVIDIA的系統。黃仁勳解釋說，數據中心建設中土地、電力、建築成本高昂，當企業拿到寶貴的電力配額時，核心目標是「用這些電力創造最大的商業價值」。若NVIDIA的「<strong>每瓦性能</strong>」（即每瓦能生成的Token數量）是競爭對手的兩倍，客戶就能產生兩倍收入。如果Blackwell GPU性能是上一代Hopper的30倍，在同樣電力消耗下，客戶能獲得30倍潛在收入。為省一點晶片成本而放棄30倍收入，機會成本「高得離譜」。</p> <p>此外，黃仁勳分析了ASIC的「生態定位」。他認為ASIC適合「功能固定、市場規模有限」的領域（如視頻轉碼），但對於「工作負載多樣且快速變化」的AI領域，ASIC的「專用性」反成致命弱點。AI需要高度「<strong>可程式性</strong>」的計算平台來快速適配新任務、新演算法，這正是GPU和CUDA生態的核心優勢。NVIDIA透過開放NVLink Fusion等接口，允許英特爾等公司晶片接入自家生態，展現「平台化」與「生態化」思維，透過開放來擴大生態，共同做大AI算力市場。</p> <p><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_660.jpg" /></p></p> <hr /> <h2>全球AI競賽與美中關係：黃仁勳的坦率批判</h2> <p>影片作者指出，黃仁勳在訪談中還花了大量篇幅討論全球人工智能競賽，尤其是美國與中國的關係。黃仁勳首先提出AI基礎設施已成為與能源、通信同等重要的<strong>國家戰略資源</strong>，全球正掀起「<strong>主權AI</strong>」（Sovereign AI）建設浪潮。他認為每個國家都需要擁有自己的AI基礎設施，用自己的數據和文化訓練AI模型，確保AI能服務於本國特定需求，就像每個國家都有自己的電網、通信網路一樣，未來也會有自己的AI基礎設施網路。</p> <p>關於美國的對華技術政策，黃仁勳坦率批評美國採取「<strong>小院高牆</strong>」式的對華技術封鎖（如限制NVIDIA向中國出口高端晶片），這看起來是在遏制中國AI發展，但實際上不僅徒勞無功，反而更是一種危險的「<strong>單方面裁軍</strong>」。他指出這種政策的兩個主要後果：</p> <p>第一，會<strong>催生強大的競爭對手</strong>。將擁有95%市場份額的NVIDIA排除出中國市場，相當於將整個中國市場拱手讓給華為等本土企業。這些企業會在「沒有強競爭」的環境下，靠「壟斷利潤」加速技術研發和產能擴張，最終成長為NVIDIA在全球市場的強勁對手。</p> <p>第二，<strong>嚴重低估了中國的能力</strong>。黃仁勳曾警告，外界普遍認為中國造不出高端晶片或技術落後美國數年的想法都是「瘋狂的」。中國擁有最渴望成功、最勤奮的企業家以及充滿活力的內部競爭生態。中美在晶片和AI領域的技術差距其實是以「納秒」來計算的，而非數年。</p> <p>黃仁勳認為，正確的路徑是讓美國最優秀的企業在中國市場與本土企業直接競爭，這最符合美國的國家利益。這樣不僅能為美國企業創造經濟價值，還能讓美國透過技術影響力在全球AI格局中保持話語權，更重要的是，競爭能倒逼美國科技企業不斷創新，保持技術最前沿。他強調，一個自信、強大的國家應秉持「<strong>放馬過來</strong>」（Bring it on）的態度。</p> <p>談到美國的核心優勢，黃仁勳尖銳地指出，美國擁有世界上任何國家都沒有的獨特品牌聲譽：「<strong>來到美國，實現美國夢</strong>」。作為從中國台灣移民到美國、從餐館洗碗工成長為萬億市值公司CEO的親歷者，黃仁勳對「美國夢」理解深刻。這種「讓每個人都有機會透過努力改變命運」的信念，是美國吸引全球頂尖人才的根本原因。但他警告，近年來這個核心優勢正受到嚴重挑戰。他觀察到一個危險信號：頂尖中國AI研究者來美國的意願已從三年前的90%驟降到現在的10%-15%。他呼籲美國政策制定者必須謹慎區分「與中國競爭」和「對中國人強硬」這兩個概念，後者會摧毀美國最寶貴的資產——作為全球人才燈塔的品牌形象。</p> <p><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_1050.jpg" /></p></p> <hr /> <h2>AI的未來願景與社會變革</h2> <p>影片作者總結道，展望未來，黃仁勳堅信人工智能將從根本上改變社會，帶來巨大的<strong>生產力提升</strong>，而非大規模失業。他認為智力不是零和遊戲，周圍聰明的人和工具越多，能想到的新點子、能解決的新問題就越多，創造的崗位也會越多。每項工作都會改變，有些會消失，但經濟整體會增長。在他看來，AI本身就是最偉大的<strong>均衡器</strong>，過去一個人想利用計算機創造經濟價值至少要學習Python編程，現在只需學習人類語言，技術鴻溝正被技術本身填平。</p> <p>對於未來的具體形態，他預言在未來五年內，人工智能與機器人技術的融合將成為現實，每個人都會像電影《星球大戰》中一樣擁有自己的「<strong>R2-D2</strong>」機器人。雲端的人工智能和實體世界的機器人將無處不在，生物學的複雜性將被揭示，每個人都將擁有自己的「<strong>數字孿生</strong>」，用於預測健康狀況和疾病。面對這種指數級加速的變化，黃仁勳給出的建議很簡單：就是「<strong>登上那列火車</strong>」，不要試圖去預測火車未來會到哪個站點，因為當它呈指數級加速時，任何預測都是徒勞的。唯一的策略就是趁現在它還相對較慢時跳上去，然後隨著它一起經歷指數級的旅程。NVIDIA從晶片公司到AI基礎設施公司的進化本身，就是登上這列火車的最好證明，而對於整個世界來說，這趟旅程才剛剛開始。</p> <p><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_23.jpg" /></p></p> <hr /><hr />================================================</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">大家好，这里是最佳拍档，我是大飞。9月26日。知名播客BG2和英伟达的CEO黄仁勋进行了一场独家对话。这次访谈的标题叫做“NVIDIA：。OpenAI、计算的未来和美国梦”，时长超过100分钟。堪称黄仁勋近期信息密度最高、干货最足的一次分享。在这场对话中。黄仁勋系统性地解释了华尔街与硅谷之间存在的巨大认知分歧。并且详细拆解了英伟达看似坚不可摧的商业护城河。以及他对全球人工智能竞赛、大国博弈和未来社会形态的完整思考。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">今天我们就来回顾一下这场访谈的内容。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">可能大家还记得。一年前市场上有个挺流行的担忧。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">AI的预训练需求好像暂时放缓了。会不会导致之前建的算力中心过剩呢？当时黄仁勋给出了一个非常大胆的预测。推理（Inference）需求的增长不是100倍。也不是1000倍。而是会达到“10亿倍”的量级。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_60.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">现在一年过去了，事实证明。连黄仁勋当时的这个预测。都还是低估了实际的需求增长。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">为什么会这样呢？黄仁勋在访谈里系统性地提出了一个关键观点。那就是AI的算力需求。其实是由三种“缩放定律”（Scaling Laws）所共同驱动的。而不是大家之前以为的只有一种。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第一种是“预训练缩放定律”（Pre-training Scaling Law）。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这也是大家最熟悉的一种。简单说就是模型越大、用的数据越多、训练时间越长。模型就越智能。过去几年。不管是GPT系列还是其他大模型。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">其实都是靠这个定律驱动算力增长的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如GPT-3用了千亿参数。训练时消耗的算力达到了每天几百PFlops。这背后就是预训练缩放定律在起作用。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">但是黄仁勋强调。这只是算力需求的“第一引擎”，真正关键的是另外两种。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第二种是“后训练缩放定律”（Post-training Scaling Law）。后训练指的就是模型在预训练之后。通过类似“练习”的方式。来精通某项特定技能的过程。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_122.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这个过程会结合强化学习。让AI通过大量“试错”和“推理”来优化自己。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">举个例子，我们想让AI学会写代码。光给它看海量的代码库还不够。还得让它不断尝试编写代码、调试错误、运行测试。直到能写出符合要求的程序。这个“练习”的过程，就是后训练。而这个过程本身。需要消耗的推理计算量非常大。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第三种是“推理时思维缩放定律”（Inference-time Thinking Scaling Law）。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这也是黄仁勋认为市场最没理解透的一点。堪称“革命性”的算力驱动因素。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">他在访谈里说道。旧的推理方式是一次性的。你问AI一个问题，它会直接给你答案。但是新推理方式是‘思考’，也就是在回答之前。AI会先自己琢磨、查证、梳理逻辑。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这种“思考”不是简单的一步计算。而是一个复杂的过程。AI在生成最终答案前。会进行多轮的内部推理、研究事实。甚至调用外部工具。这个过程简单的话会形成“思维链”，复杂一点的话还会形成“推理树”，而且任务越复杂。AI“思考”的时间就越长。答案质量就越高，而每一次内部循环。都是一次计算消耗。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_195.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这三种缩放定律放在一起。就彻底改变了我们对“训练”和“推理”的传统认知。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">过去大家觉得。算力主要消耗在“训练”阶段。但是现在看来。未来AI系统的大部分算力消耗。会发生在“使用”阶段。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">想象一下。当一个AI Agent能像人类员工一样。自主完成复杂的任务。它背后“思考”所消耗的计算资源。会是过去简单任务的亿万倍。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这也能解释为什么OpenAI、谷歌这些公司。一边发布更强大的基础模型。一边把重心转向能执行复杂任务的Agent系统。因为这才是算力需求的真正未来。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">不过，这种指数级的增长需求。与华尔街的线性预测模型之间。形成了黄仁勋所说的“巨大的认知分歧”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">访谈主持人布拉德·格斯特纳直接抛出了这个问题。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">一方面。Sam Altman和Sundar Pichai这些行业领袖。都在谈论“万亿级”的算力投资；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">但是另一方面。覆盖Nvidia的25位华尔街分析师。普遍预测Nvidia的增长率会在2027年“断崖式下跌”到大约8%。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_262.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">为什么会有这么大的认知差距呢？黄仁勋给出了一个三层的宏大叙事框架。不仅阐释了Nvidia的增长逻辑。也回答了“增长从哪来”、“增长能持续多久”这两个核心问题。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第一层是“物理定律层面的转变”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">通用计算的时代已经结束。未来是加速计算和AI计算的时代。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">黄仁勋说，摩尔定律已经走到尽头。靠CPU性能提升来推动计算发展的模式。已经行不通了。这意味着。全球现有的、价值数万亿美元的、基于通用计算。也就是CPU的数据中心基础设施。在下一轮更新的时候。必须转向加速计算架构。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如用GPU、TPU这些专门为AI设计的芯片。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这不是“创造新的市场”，而是“存量市场的替换”，仅仅把旧的CPU数据中心替换成加速计算数据中心。就已经是一个庞大的市场空间了。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第二层是“现有应用的迁移”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">把互联网的核心工作负载从CPU迁移到GPU。就足以驱动数百亿美元的需求。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_326.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">黄仁勋举了个例子。像搜索、推荐引擎、电商购物这些。支撑Meta、谷歌、亚马逊、字节跳动等科技巨头核心业务的系统。过去都是跑在CPU上的。但是现在。它们正在全面转向用AI和GPU。因为AI能提供更好的个性化体验。效率也更高。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这个过程不是在“造新的东西”，而是用更先进的技术。重塑一个已经存在的、服务全球40亿人的庞大市场。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">黄仁勋说道。这就像从煤油灯转向电力。从螺旋桨飞机转向喷气式飞机一样。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第三层是“未来的增量”，指的是AI作为“智能工厂”，对全球GDP的赋能。这是最让人兴奋的部分。也是AI创造全新价值的地方。黄仁勋把AI工厂类比成工业革命中的“马达”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">马达替代了体力劳动。而AI工厂则通过生成Token来增强人类的智力劳动。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">他还做了一个经济学估算，首先。全球GDP中。大约50%-65%和“智力劳动”相关。比如设计、研发、咨询、编程这些需要动脑的工作。总价值大概是50万亿美元；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_397.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">其次。AI会对这50万亿美元的经济活动进行“增强”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">假设一家公司有一个年薪10万美元的员工。公司愿意额外花1万美元给这个员工配备AI服务。换来2-3倍的生产力提升。这笔投资是非常划算的。黄仁勋说。Nvidia内部已经给每一位芯片设计师和软件工程师。都配了AI助手。结果生产力的提升非常明显；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">最后。如果全球50万亿美元的“智能GDP”，每年需要价值10万亿美元的“AI Token生成服务”来增强。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">假设这些服务的毛利率是50%，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">那么就需要价值5万亿美元的AI基础设施。来支撑这些服务的运行。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这三层框架一出来。Nvidia的增长逻辑就从“卖芯片”的简单故事。变成了和全球经济结构变迁同频共振的宏大叙事。它清晰地表明，Nvidia的增长。既来自旧设施的替换。也来自现有业务的升级。还来自未来AI创造的新价值。这也能解释为什么Nvidia的收入和数据中心的电力消耗高度相关。因为算力越多。需要的电力就越多。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_467.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如阿里巴巴的吴泳铭就说过。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">他们的数据中心电力消耗会在本年代末增长10倍。而AI生成的Token数量每几个月就翻一番。背后都是对算力的无尽需求。以及对Nvidia AI基础设施的依赖。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">聊完英伟达的增长逻辑。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">我们再来看这次访谈里另一个重磅话题。那就是Nvidia和OpenAI的“星际之门”（Stargate）计划。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">最近大家可能都注意到一个新闻。那就是英伟达要投资OpenAI千亿美元。来搞“星际之门”项目。很多人觉得这只是一次普通的商业合作。Nvidia卖芯片。OpenAI买算力。但是黄仁勋在访谈里明确的说道。这是一次“战略绑定”，背后的逻辑比大家想的深得多。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">黄仁勋毫不掩饰对OpenAI未来的看好。他认为。OpenAI很可能将成为下一个数万亿美元级别的超大规模公司（Hyperscale Company）。什么是超大规模公司？就是像Meta、谷歌、微软这样。既能服务消费者市场。又能服务企业市场。甚至成为全球基础设施一部分的公司。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_528.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">黄仁勋认为。OpenAI未来也会达到这个级别。它的AI服务会渗透到个人生活、企业办公、工业生产等各个领域。就像现在的互联网一样普及。所以。能在OpenAI成为“巨无霸”之前进行投资。黄仁勋觉得是“他们能想象到的最聪明的投资之一”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而且特别关键的一点是。这笔投资不是Nvidia强制要求的。而是OpenAI主动给Nvidia的机会。这说明OpenAI也认可Nvidia的战略价值。想和它深度绑定。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">其次，这次合作的深度和广度。远超外界想象。黄仁勋把双方的合作拆成了三个层次。第一个层次是“现有云合作的延续”，Nvidia会继续和微软合作。为OpenAI在Azure云平台上构建价值“数千亿美元”的算力集群；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">同时，还会和甲骨文、软银合作。建设数个吉瓦（Gigawatts）级的数据中心。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第二个层次是“帮助OpenAI自建基础设施”，这是这次新合作的核心。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">过去OpenAI的算力主要靠租云服务商的资源。但是现在它要自己建AI基础设施了。而Nvidia会从最底层开始参与。包括芯片设计、软件开发、系统集成。甚至整个AI工厂的规划和运营。是“全栈式”的支持。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_607.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第三个层次是“建立直接的战略关系”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">黄仁勋把这种关系类比成Nvidia和Meta的Mark Zuckerberg、谷歌的Sundar Pichai、微软的Satya Nadella、xAI的Elon Musk之间的直接合作。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这意味着OpenAI的规模已经大到。不需要再通过云服务商做“中间人”了。而是可以和最核心的技术供应商直接、平等地对话。形成深度绑定的战略伙伴关系。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">为什么OpenAI要做这样的调整呢？黄仁勋点出了其中的核心原因。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">那就是OpenAI面临“双重指数级增长压力”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">一方面是“用户增长指数”，AI越好用。应用场景越多。用户数量和使用频率就会指数级增长；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">另一方面是“计算增长指数”，就像我们前面聊的。每个用户每次和AI交互。因为“思考”的引入。需要的计算量也在指数级增长。这两个指数叠加在一起。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">意味着OpenAI的算力需求。会以“指数的指数”的速度增长。单靠租云服务已经满足不了了。所以必须同时推进“租云”和“自建”两条路。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_670.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">才能够确保算力供给跟得上需求。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">其实呢与Openai的紧密合作。背后。反映出的正是英伟达坚固的护城河。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">现在市场上有很多竞争对手。比如说AMD英特尔。还有一些公司在做ASIC芯片。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">主持人直接问黄仁勋。Nvidia的竞争护城河是在扩大还是缩小呢？</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">黄仁勋的回答则是。Nvidia真正的护城河。不是某一款芯片的性能优势。而是一种被称为“极限协同设计”（Extreme Co-Design）的系统级创新能力。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">黄仁勋解释了为什么Nvidia要搞“年度发布周期”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">过去芯片的更新是18-24个月一次。但是现在改成了每年一次。因为摩尔定律失效以后。晶体管的性能不再大幅提升。如果不能快速提升整体性能。AI生成Token的成本就会持续上升。而要持续降低Token成本。唯一的办法就是“系统级的创新”，这就是“极限协同设计”的由来。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">黄仁勋说。极限协同设计要求同步优化模型、算法、系统和芯片。让它们像一个整体一样工作。而不是各自为战。这和传统的“盒子内的创新”完全不同。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_740.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">过去是只想着把CPU做得更快。但是现在要同步升级构成AI数据中心的所有核心组件。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">具体来说。“极限协同设计”体现在三个层面。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在芯片层面。Nvidia会并行研发革命性的CPU、GPU、网络芯片和NVLink；。系统层面。会把这些芯片以最优化的方式整合起来。确保它们之间的数据传输效率最高。不会出现“某一个组件拖后腿”的情况；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">软件层面。会提供从底层驱动到上层应用库的完整软件栈。让开发者能轻松用上整个系统的能力。不用自己去解决硬件兼容、数据传输这些复杂问题。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">正是这种跨所有层面的协同设计。才让Nvidia从Hopper架构到Blackwell架构。在一年内实现了性能提升30倍的突破。这绝对不是靠单一芯片的技术进步能做到的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">其次。这种系统能力还体现在“规模化部署”的巨大挑战上。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">黄仁勋举了个例子。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">Elon Musk的xAI公司要部署Colossus 2的集群。需要用到50万个GPU。这不是简单地把50万个GPU堆在一起就行。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_808.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">要考虑电力供应、散热、数据传输延迟、系统稳定性等一系列的问题。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">任何一个环节出问题。整个集群都没法正常工作。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而客户之所以敢下数百亿美元的订单给Nvidia。就是因为Nvidia的架构经过了市场验证。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">能确保这么大规模的系统稳定运行。这种“经得住考验的规模化部署能力”，本身就是一道很高的信任壁垒。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">所以黄仁勋说。现在的竞争已经不是“我的芯片比你的快”，而是“我的整个AI工厂。比你的AI工厂效率更高”了。这种从“组件思维”到“系统思维”的跃迁。正是Nvidia能远超竞争对手的根本原因。毕竟。竞争对手可能能做出一款性能不错的芯片。但是要想做到“芯片、系统、软件”全链条的协同创新。还要能支撑几十万GPU的规模化部署。难度要大得多。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">黄仁勋甚至说。即使竞争对手把他们的ASIC芯片免费送给客户。客户还是应该选择Nvidia的系统。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_870.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">很多人听到这话会觉得不可思议。免费的芯片都不要？但是黄仁勋的逻辑。其实紧扣了数据中心的“现实约束”，那就是电力和空间都是有限的。黄仁勋解释说，数据中心建设中。土地、电力、建筑这些投入的成本非常高。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">当一家企业拿到宝贵的2吉瓦电力配额的时候。它的核心目标不再是“节省芯片成本”，而是“用这些电力创造最大的商业价值”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">他用简单的算术算了一笔账。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">如果每瓦性能。或者说每瓦能生成的Token数量。是竞争对手的两倍。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">那么客户用同样的数据中心。就能产生两倍的收入，谁不想要呢？</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">再具体一点。假设竞争对手的ASIC芯片。性能和Nvidia上一代的Hopper GPU差不多；。而Nvidia新一代的Blackwell GPU。性能是Hopper的30倍。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这意味着，在同样的电力消耗下。用Blackwell的客户能够获得30倍的潜在收入。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在这种情况下。为了节省一点芯片成本而放弃30倍的收入。这种机会成本显然“高得离谱”。任何理性的CFO。都会选择“每瓦性能”最高的解决方案。因为这直接决定了企业收入的上限。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_940.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">除此之外。黄仁勋还分析了ASIC的“生态定位”。他认为。ASIC适合那些“功能固定、市场规模有限”的领域。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如视频转码器、智能网卡等等。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这些任务的算法很少变。用ASIC能做到很高的效率。但是对于AI这种“工作负载多样且快速变化”的领域。ASIC的“专用性”反而成了致命弱点。因为AI需要处理的任务实在是太多了。聊天、写代码、生成图片视频、做数据分析、制定商业计划。等等等等。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而且底层算法还在不断演进。这就要求计算平台必须具备高度的“可编程性”，能够快速适配新任务、新算法。而这正是GPU和CUDA生态的核心优势。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这也能解释为什么谷歌虽然有自己的TPU。但同时也是Nvidia GPU的大客户。因为在一个复杂的计算集群里。既需要TPU这样的“专用辅助”芯片。也需要GPU这样的“通用主力”芯片。通过合理组合来实现整体最优。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_1001.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而Nvidia通过开放NVLink Fusion等接口。允许英特尔等公司的芯片接入自己的生态。这正是“平台化”和“生态化”思维的体现。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">非但不靠封闭来阻挡对手。反而是靠开放来扩大生态。让更多伙伴参与进来。一起把AI算力的市场做大。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在这场对话中。黄仁勋还花了大量篇幅讨论了全球人工智能竞赛。尤其是美国与中国的关系。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">黄仁勋首先提出了一个观点。AI基础设施已经成为和能源、通信同等重要的国家战略资源。所以全球范围内正在掀起“主权AI”（Sovereign AI）的建设浪潮。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">什么是主权AI？就是每个国家都需要拥有自己的AI基础设施。用自己的数据和文化训练AI模型。确保AI能服务于本国的特定需求。不管是工业生产、制造业升级。还是国家安全。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">黄仁勋认为。虽然各国会使用GPT、Gemini这些全球领先的模型。但是同时必须建立自己的主权AI能力。因为AI不仅是技术。还承载着文化、价值观和历史。一个国家不能把核心的智能需求。完全依赖于其他国家的技术。就像每个国家都会有自己的电网、通信网络一样。未来也会有自己的AI基础设施网络。而这为Nvidia等基础设施提供商。创造了全球性的全新市场。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_1083.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">关于美国的对华技术政策。黄仁勋提出了坦率的批评。他认为。美国采取“小院高墙”式的对华技术封锁。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如限制Nvidia向中国出口高端芯片。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">看起来是在遏制中国AI发展的做法。但是实际上不仅徒劳无功。反而更是一种危险的“单方面裁军”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">他指出了这种政策的两个主要后果。第一，会催生强大的竞争对手。把拥有95%市场份额的Nvidia排除出中国市场。相当于把整个中国市场拱手让给华为等本土企业。这些企业会在“没有强竞争”的环境下。会靠着“垄断利润”加速技术研发和产能扩张。最终成长为Nvidia在全球市场的强劲对手。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第二，严重低估了中国的能力。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">黄仁勋曾经警告说。外界普遍认为中国造不出高端芯片。或者技术上落后美国数年。这些想法都是“疯狂”的。实际上。中国拥有世界上最渴望成功、最勤奋的企业家。还有充满活力的内部竞争生态。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_1144.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">中国在芯片和AI领域和美国的技术差距。其实是以“纳秒”来计算的。不是大家想的“几年”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">当然这里的纳秒是打了引号的。意思是强调中美之间的差距很小。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">那么，正确的路径应该是什么呢？黄仁勋认为。让美国最优秀的企业在中国市场和本土企业直接竞争。才最符合美国的国家利益。这样做不仅能为美国企业创造经济价值。还能让美国通过技术影响力。在全球AI格局中保持话语权；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">更重要的是。竞争能倒逼美国科技企业不断创新。保持在技术最前沿。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">黄仁勋说道，一个自信、强大的国家。应该秉持‘放马过来’（Bring it on）的态度。相信自己的体系和人民能在竞争中胜出。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在谈到美国的核心优势时。黄仁勋的回答更是尖锐。他说。美国拥有一个世界上任何国家都没有的独特品牌声誉。那就是来到美国，实现美国梦。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">作为一个从中国台湾移民到美国。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_1204.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">从餐馆洗碗工成长为万亿市值公司CEO的亲历者。黄仁勋对“美国梦”的理解非常深刻。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这种“让每个人都有机会通过努力改变命运”的信念。是美国吸引全球顶尖人才的根本原因。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如过去几十年。全球最优秀的科学家、工程师、创业者都愿意去美国。因为那里有更好的机会、更开放的环境。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">但是黄仁勋警告说，近些年来。这个核心优势正在受到严重的挑战。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">他观察到一个非常危险的信号。顶尖中国AI研究者来美国的意愿。已经从三年前的90%骤降到现在的10%-15%。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这不是一个小变化。而是关乎美国未来的“生存危机”级别的早期预警。因为AI行业的竞争。本质上是人才的竞争，没有顶尖人才。再先进的技术也难以持续领先。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">所以黄仁勋呼吁美国政策制定者。必须谨慎区分“与中国竞争”和“对中国人强硬”这两个概念。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">“与中国竞争”是在技术、市场上的良性比拼。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_1266.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而“对中国人强硬”则是把优秀的中国人才拒之门外。这会摧毁美国最宝贵的资产。也就是美国作为全球人才灯塔的品牌形象。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">展望未来。黄仁勋认为人工智能将从根本上改变社会。他坚信。人工智能会带来巨大的生产力提升。而不是大规模的失业。那种认为AI会摧毁就业的观点。前提是“我们再也没有新的想法了”，但是他认为智力不是零和游戏。周围聪明的人和工具越多。能想到的新点子、能解决的新问题就越多。创造的岗位也会越多。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">每项工作都会改变，有些会消失。但是经济整体会增长。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在他看来。AI本身就是最伟大的均衡器。过去。一个人想利用计算机创造经济价值。至少得学习Python编程。现在，他们只需要学习人类语言。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">技术鸿沟正在被技术本身填平。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">对于未来的具体形态。他预言在未来五年内。人工智能与机器人技术的融合将成为现实。每个人都会像电影星球大战中一样。有自己的“R2-D2”机器人。成为生活中的伙伴和向导。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_1334.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">云端的人工智能和实体世界的机器人将无处不在。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">生物学的复杂性将被揭示。每个人都将拥有自己的“数字孪生”，用于预测健康状况和疾病。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">面对这种指数级加速的变化。黄仁勋给出的建议很简单。那就是登上那列火车。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">不要试图去预测火车未来会到哪个站点。因为当它呈指数级加速的时候。任何预测都是徒劳的。唯一的策略就是趁现在它还相对较慢时跳上去。然后随着它一起经历指数级的旅程。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">也许，从芯片公司到AI基础设施公司。英伟达的进化本身就是登上这列火车的最好证明。而对于整个世界来说。这趟旅程才刚刚开始。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">好了，感谢观看本期视频。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">我们下期再见。</p>

<hr style="clear:both" />
=============
<p><a href="https://www.youtube.com/watch?v=--HbDQk2-jA">https://www.youtube.com/watch?v=--HbDQk2-jA</a></p><p>9月26日，知名播客BG2和英伟达的CEO黄仁勋进行了一场独家对话。这次访谈的标题叫做“NVIDIA：OpenAI、计算的未来和美国梦”，时长超过100分钟，堪称黄仁勋近期信息密度最高、干货最足的一次分享。在这场对话中，黄仁勋系统性地解释了华尔街与硅谷之间存在的巨大认知分歧，并且详细拆解了英伟达看似坚不可摧的商业护城河，以及他对全球人工智能竞赛、大国博弈和未来社会形态的完整思考，今天我们就来回顾一下这场访谈的内容。</p><p><a href="https://youtu.be/pE6sw_E9Gh0?si=L41UzPmgS_lP5i43">https://youtu.be/pE6sw_E9Gh0?si=L41UzPmgS_lP5i43</a></p>]]>
      </itunes:summary>
      <description>
        <![CDATA[<hr style="clear:both" />

<p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://img.youtube.com/vi/--HbDQk2-jA/maxresdefault.jpg" /></p><p><a href="https://www.youtube.com/watch?v=--HbDQk2-jA">https://www.youtube.com/watch?v=--HbDQk2-jA</a></p><h1>值得閱讀的理由</h1> <ul> <li>深入了解<strong>黃仁勳對AI算力需求的獨到見解</strong>，包括他提出的「三重縮放定律」，揭示AI未來主要算力消耗將從訓練轉向使用。</li> <li>掌握<strong>NVIDIA如何構建其堅不可摧的商業護城河</strong>，不僅透過「極限協同設計」實現系統級創新，更透過開放生態策略應對競爭。</li> <li>理解<strong>黃仁勳對全球AI競賽、美中科技關係及AI社會影響的全面思考</strong>，包括他對「主權AI」的定義、對美中政策的批判，以及對「美國夢」作為人才磁石的深刻洞察。</li> </ul> <hr /> <h1>摘要</h1> <p>這段影片由「最佳拍檔」的「大飛」主持，回顧了NVIDIA執行長<strong>黃仁勳</strong>與知名播客BG2長達100多分鐘的獨家對話。這場訪談標題為「NVIDIA：OpenAI、計算的未來與美國夢」，被譽為黃仁勳近期資訊密度最高、含金量最足的一次分享。影片作者表示，黃仁勳在訪談中系統性地闡釋了華爾街與矽谷之間存在的巨大認知分歧，詳細剖析了NVIDIA看似堅不可摧的商業護城河，並分享了他對全球人工智能競賽、大國博弈及未來社會形態的完整思考。</p> <p><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_0.jpg" /></p></p> <hr /> <h2>AI算力需求的「三重縮放定律」</h2> <p>影片作者指出，一年前市場曾擔憂AI預訓練需求放緩會導致算力中心過剩，但黃仁勳當時大膽預測推理（Inference）需求將成長「十億倍」的量級，而事實證明，連這個預測都低估了實際需求。黃仁勳在訪談中系統性地提出了AI算力需求由三種「<strong>縮放定律</strong>」共同驅動的關鍵觀點：</p> <p>第一種是「<strong>預訓練縮放定律</strong>」（Pre-training Scaling Law），即模型越大、數據越多、訓練時間越長，模型就越智能。這是過去幾年大模型算力增長的主要驅動力。</p> <p>第二種是「<strong>後訓練縮放定律</strong>」（Post-training Scaling Law），指的是模型在預訓練後，透過強化學習和大量「試錯」、「推理」來精通特定技能的過程，這個「練習」過程本身需要消耗大量的推理計算。</p> <p>第三種是「<strong>推理時思維縮放定律</strong>」（Inference-time Thinking Scaling Law），黃仁勳認為這是市場最未理解透徹的「革命性」驅動因素。它描述AI在回答問題前，會進行內部「思考」、查證、梳理邏輯，這是一個多輪的內部推理過程，而非簡單一步計算。任務越複雜，AI「思考」時間越長，答案質量越高，而每一次內部循環都是一次計算消耗。</p> <p>這三重定律徹底改變了傳統認知，未來AI系統的絕大部分算力消耗將發生在「使用」階段，尤其當AI Agent能自主完成複雜任務時，其背後「思考」的計算資源將是過去的億萬倍。</p> <p><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_90.jpg" /></p></p> <hr /> <h2>黃仁勳解析NVIDIA增長邏輯：三大宏大敘事框架</h2> <p>影片作者提到，這種指數級的增長需求與華爾街的線性預測模型之間形成了黃仁勳所說的「<strong>巨大的認知分歧</strong>」。主持人布拉德·格斯特納直接指出，一方面，Sam Altman和Sundar Pichai等行業領袖談論「萬億級」算力投資；另一方面，華爾街分析師普遍預測NVIDIA的增長率將在2027年「斷崖式下跌」到約8%。黃仁勳為此給出了一個三層宏大敘事框架，闡釋NVIDIA的增長邏輯：</p> <p>第一層是「<strong>物理定律層面的轉變</strong>」，即通用計算時代已結束，未來是<strong>加速計算和AI計算</strong>的時代。隨著摩爾定律走到盡頭，靠CPU性能提升推動計算發展的模式已不可行。全球價值數萬億美元的CPU數據中心基礎設施，在下一輪更新時必須轉向GPU、TPU等加速計算架構。這是一個龐大的「<strong>存量市場替換</strong>」，而非創造新市場。</p> <p>第二層是「<strong>現有應用的遷移</strong>」，將網際網路的核心工作負載從CPU遷移到GPU，這足以驅動數百億美元的需求。黃仁勳以Meta、Google、Amazon、字節跳動等巨頭的核心業務（如搜索、推薦引擎、電商購物）為例，說明它們正全面轉向AI和GPU，以提供更好的個性化體驗和更高效率。這是在用更先進的技術重塑一個已服務全球40億人的龐大市場。</p> <p>第三層是「<strong>未來的增量</strong>」，指的是<strong>AI作為「智能工廠」對全球GDP的賦能</strong>。這是最令人興奮的部分，也是AI創造全新價值的地方。黃仁勳將AI工廠類比為工業革命中的「馬達」，透過生成Token來增強人類的智力勞動。他估算全球50萬億美元的「智力勞動」相關GDP，若每年需要10萬億美元的「AI Token生成服務」來增強，以50%毛利率計算，將需要5萬億美元的AI基礎設施來支撐。這三層框架清晰表明NVIDIA的增長既來自舊設施替換、現有業務升級，也來自未來AI創造的新價值。</p> <p><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_230.jpg" /></p></p> <hr /> <h2>NVIDIA與OpenAI的「星際之門」戰略結盟</h2> <p>影片作者接著探討了NVIDIA與OpenAI的「<strong>星際之門</strong>」（Stargate）計畫。黃仁勳明確指出，這不是一次普通的商業合作，而是一次「<strong>戰略綁定</strong>」，背後邏輯深遠。他毫不掩飾對OpenAI未來的看好，認為OpenAI很可能成為下一個數萬億美元級別的「超大規模公司」（Hyperscale Company），滲透到個人生活、企業辦公、工業生產等各領域。因此，能在OpenAI成為巨擘前進行投資，黃仁勳認為是「能想像到的最聰明投資之一」，且這是OpenAI主動給NVIDIA的機會。</p> <p>這次合作的深度和廣度遠超外界想像，黃仁勳將其拆分為三個層次：</p> <p>首先是「<strong>現有雲合作的延續</strong>」，NVIDIA將繼續與微軟合作，為OpenAI在Azure雲平台構建價值「數千億美元」的算力集群；同時，還會與甲骨文、軟銀合作，建設數個吉瓦（Gigawatts）級數據中心。</p> <p>其次是「<strong>幫助OpenAI自建基礎設施</strong>」，這是新合作的核心。過去OpenAI主要租用雲服務商資源，但現在將自行建設AI基礎設施，NVIDIA會從最底層開始參與，包括晶片設計、軟體開發、系統集成，甚至是整個AI工廠的規劃和營運，提供「<strong>全棧式</strong>」支持。</p> <p>最後是「<strong>建立直接的戰略關係</strong>」，黃仁勳將此關係類比為NVIDIA與Meta、Google、微軟、xAI等頂級科技公司的直接合作，意味著OpenAI規模已大到可與核心技術供應商直接、平等對話，形成深度綁定的戰略夥伴關係。</p> <p>黃仁勳指出OpenAI做出此調整的核心原因，是面臨「<strong>雙重指數級增長壓力</strong>」。一方面是「用戶增長指數」，AI越好用，用戶數量和使用頻率就指數級增長；另一方面是「計算增長指數」，每個用戶每次與AI交互所需的計算量也在指數級增長。這兩者疊加，意味著OpenAI的算力需求將以「<strong>指數的指數</strong>」速度增長，單靠租用雲服務已無法滿足，必須同時推進「租雲」和「自建」兩條路。</p> <p><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_450.jpg" /></p></p> <hr /> <h2>英偉達的堅實護城河：「極限協同設計」與系統級創新</h2> <p>影片作者指出，與OpenAI的緊密合作，反映出NVIDIA堅固的護城河。面對AMD、英特爾以及ASIC晶片公司的競爭，黃仁勳表示NVIDIA真正的護城河不是單一晶片的性能優勢，而是一種「<strong>極限協同設計</strong>」（Extreme Co-Design）的系統級創新能力。</p> <p>首先，他解釋了NVIDIA為何要搞「<strong>年度發布週期</strong>」。隨著摩爾定律失效，晶體管性能不再大幅提升，若不能快速提升整體性能，AI生成Token的成本將持續上升。為持續降低Token成本，唯一的辦法就是「系統級的創新」，這正是「極限協同設計」的由來。黃仁勳說，極限協同設計要求<strong>同步優化模型、演算法、系統和晶片</strong>，讓它們像一個整體一樣工作，而非各自為戰。這體現在晶片層面（并行研發CPU、GPU、網路晶片和NVLink）、系統層面（晶片整合優化，數據傳輸效率）、以及軟體層面（提供從底層驅動到上層應用庫的完整軟體棧），才使NVIDIA在一年內實現了30倍的性能提升。</p> <p>其次，這種系統能力還體現在「<strong>規模化部署</strong>」的巨大挑戰上。黃仁勳以Elon Musk的xAI公司需部署50萬個GPU為例，這不僅是硬體堆疊，還要考慮電力供應、散熱、數據傳輸延遲、系統穩定性等一系列問題。客戶之所以敢下數百億美元訂單給NVIDIA，是因為NVIDIA的架構經過市場驗證，能確保大規模系統穩定運行。這種「經得起考驗的規模化部署能力」本身就是一道很高的信任壁壘。黃仁勳強調，現在的競爭已是「我的整個<strong>AI工廠</strong>比你的AI工廠效率更高」。</p> <p>他甚至斷言，即使競爭對手把ASIC晶片免費送給客戶，客戶仍應選擇NVIDIA的系統。黃仁勳解釋說，數據中心建設中土地、電力、建築成本高昂，當企業拿到寶貴的電力配額時，核心目標是「用這些電力創造最大的商業價值」。若NVIDIA的「<strong>每瓦性能</strong>」（即每瓦能生成的Token數量）是競爭對手的兩倍，客戶就能產生兩倍收入。如果Blackwell GPU性能是上一代Hopper的30倍，在同樣電力消耗下，客戶能獲得30倍潛在收入。為省一點晶片成本而放棄30倍收入，機會成本「高得離譜」。</p> <p>此外，黃仁勳分析了ASIC的「生態定位」。他認為ASIC適合「功能固定、市場規模有限」的領域（如視頻轉碼），但對於「工作負載多樣且快速變化」的AI領域，ASIC的「專用性」反成致命弱點。AI需要高度「<strong>可程式性</strong>」的計算平台來快速適配新任務、新演算法，這正是GPU和CUDA生態的核心優勢。NVIDIA透過開放NVLink Fusion等接口，允許英特爾等公司晶片接入自家生態，展現「平台化」與「生態化」思維，透過開放來擴大生態，共同做大AI算力市場。</p> <p><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_660.jpg" /></p></p> <hr /> <h2>全球AI競賽與美中關係：黃仁勳的坦率批判</h2> <p>影片作者指出，黃仁勳在訪談中還花了大量篇幅討論全球人工智能競賽，尤其是美國與中國的關係。黃仁勳首先提出AI基礎設施已成為與能源、通信同等重要的<strong>國家戰略資源</strong>，全球正掀起「<strong>主權AI</strong>」（Sovereign AI）建設浪潮。他認為每個國家都需要擁有自己的AI基礎設施，用自己的數據和文化訓練AI模型，確保AI能服務於本國特定需求，就像每個國家都有自己的電網、通信網路一樣，未來也會有自己的AI基礎設施網路。</p> <p>關於美國的對華技術政策，黃仁勳坦率批評美國採取「<strong>小院高牆</strong>」式的對華技術封鎖（如限制NVIDIA向中國出口高端晶片），這看起來是在遏制中國AI發展，但實際上不僅徒勞無功，反而更是一種危險的「<strong>單方面裁軍</strong>」。他指出這種政策的兩個主要後果：</p> <p>第一，會<strong>催生強大的競爭對手</strong>。將擁有95%市場份額的NVIDIA排除出中國市場，相當於將整個中國市場拱手讓給華為等本土企業。這些企業會在「沒有強競爭」的環境下，靠「壟斷利潤」加速技術研發和產能擴張，最終成長為NVIDIA在全球市場的強勁對手。</p> <p>第二，<strong>嚴重低估了中國的能力</strong>。黃仁勳曾警告，外界普遍認為中國造不出高端晶片或技術落後美國數年的想法都是「瘋狂的」。中國擁有最渴望成功、最勤奮的企業家以及充滿活力的內部競爭生態。中美在晶片和AI領域的技術差距其實是以「納秒」來計算的，而非數年。</p> <p>黃仁勳認為，正確的路徑是讓美國最優秀的企業在中國市場與本土企業直接競爭，這最符合美國的國家利益。這樣不僅能為美國企業創造經濟價值，還能讓美國透過技術影響力在全球AI格局中保持話語權，更重要的是，競爭能倒逼美國科技企業不斷創新，保持技術最前沿。他強調，一個自信、強大的國家應秉持「<strong>放馬過來</strong>」（Bring it on）的態度。</p> <p>談到美國的核心優勢，黃仁勳尖銳地指出，美國擁有世界上任何國家都沒有的獨特品牌聲譽：「<strong>來到美國，實現美國夢</strong>」。作為從中國台灣移民到美國、從餐館洗碗工成長為萬億市值公司CEO的親歷者，黃仁勳對「美國夢」理解深刻。這種「讓每個人都有機會透過努力改變命運」的信念，是美國吸引全球頂尖人才的根本原因。但他警告，近年來這個核心優勢正受到嚴重挑戰。他觀察到一個危險信號：頂尖中國AI研究者來美國的意願已從三年前的90%驟降到現在的10%-15%。他呼籲美國政策制定者必須謹慎區分「與中國競爭」和「對中國人強硬」這兩個概念，後者會摧毀美國最寶貴的資產——作為全球人才燈塔的品牌形象。</p> <p><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_1050.jpg" /></p></p> <hr /> <h2>AI的未來願景與社會變革</h2> <p>影片作者總結道，展望未來，黃仁勳堅信人工智能將從根本上改變社會，帶來巨大的<strong>生產力提升</strong>，而非大規模失業。他認為智力不是零和遊戲，周圍聰明的人和工具越多，能想到的新點子、能解決的新問題就越多，創造的崗位也會越多。每項工作都會改變，有些會消失，但經濟整體會增長。在他看來，AI本身就是最偉大的<strong>均衡器</strong>，過去一個人想利用計算機創造經濟價值至少要學習Python編程，現在只需學習人類語言，技術鴻溝正被技術本身填平。</p> <p>對於未來的具體形態，他預言在未來五年內，人工智能與機器人技術的融合將成為現實，每個人都會像電影《星球大戰》中一樣擁有自己的「<strong>R2-D2</strong>」機器人。雲端的人工智能和實體世界的機器人將無處不在，生物學的複雜性將被揭示，每個人都將擁有自己的「<strong>數字孿生</strong>」，用於預測健康狀況和疾病。面對這種指數級加速的變化，黃仁勳給出的建議很簡單：就是「<strong>登上那列火車</strong>」，不要試圖去預測火車未來會到哪個站點，因為當它呈指數級加速時，任何預測都是徒勞的。唯一的策略就是趁現在它還相對較慢時跳上去，然後隨著它一起經歷指數級的旅程。NVIDIA從晶片公司到AI基礎設施公司的進化本身，就是登上這列火車的最好證明，而對於整個世界來說，這趟旅程才剛剛開始。</p> <p><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_23.jpg" /></p></p> <hr /><hr />================================================</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">大家好，这里是最佳拍档，我是大飞。9月26日。知名播客BG2和英伟达的CEO黄仁勋进行了一场独家对话。这次访谈的标题叫做“NVIDIA：。OpenAI、计算的未来和美国梦”，时长超过100分钟。堪称黄仁勋近期信息密度最高、干货最足的一次分享。在这场对话中。黄仁勋系统性地解释了华尔街与硅谷之间存在的巨大认知分歧。并且详细拆解了英伟达看似坚不可摧的商业护城河。以及他对全球人工智能竞赛、大国博弈和未来社会形态的完整思考。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">今天我们就来回顾一下这场访谈的内容。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">可能大家还记得。一年前市场上有个挺流行的担忧。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">AI的预训练需求好像暂时放缓了。会不会导致之前建的算力中心过剩呢？当时黄仁勋给出了一个非常大胆的预测。推理（Inference）需求的增长不是100倍。也不是1000倍。而是会达到“10亿倍”的量级。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_60.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">现在一年过去了，事实证明。连黄仁勋当时的这个预测。都还是低估了实际的需求增长。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">为什么会这样呢？黄仁勋在访谈里系统性地提出了一个关键观点。那就是AI的算力需求。其实是由三种“缩放定律”（Scaling Laws）所共同驱动的。而不是大家之前以为的只有一种。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第一种是“预训练缩放定律”（Pre-training Scaling Law）。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这也是大家最熟悉的一种。简单说就是模型越大、用的数据越多、训练时间越长。模型就越智能。过去几年。不管是GPT系列还是其他大模型。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">其实都是靠这个定律驱动算力增长的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如GPT-3用了千亿参数。训练时消耗的算力达到了每天几百PFlops。这背后就是预训练缩放定律在起作用。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">但是黄仁勋强调。这只是算力需求的“第一引擎”，真正关键的是另外两种。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第二种是“后训练缩放定律”（Post-training Scaling Law）。后训练指的就是模型在预训练之后。通过类似“练习”的方式。来精通某项特定技能的过程。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_122.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这个过程会结合强化学习。让AI通过大量“试错”和“推理”来优化自己。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">举个例子，我们想让AI学会写代码。光给它看海量的代码库还不够。还得让它不断尝试编写代码、调试错误、运行测试。直到能写出符合要求的程序。这个“练习”的过程，就是后训练。而这个过程本身。需要消耗的推理计算量非常大。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第三种是“推理时思维缩放定律”（Inference-time Thinking Scaling Law）。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这也是黄仁勋认为市场最没理解透的一点。堪称“革命性”的算力驱动因素。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">他在访谈里说道。旧的推理方式是一次性的。你问AI一个问题，它会直接给你答案。但是新推理方式是‘思考’，也就是在回答之前。AI会先自己琢磨、查证、梳理逻辑。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这种“思考”不是简单的一步计算。而是一个复杂的过程。AI在生成最终答案前。会进行多轮的内部推理、研究事实。甚至调用外部工具。这个过程简单的话会形成“思维链”，复杂一点的话还会形成“推理树”，而且任务越复杂。AI“思考”的时间就越长。答案质量就越高，而每一次内部循环。都是一次计算消耗。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_195.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这三种缩放定律放在一起。就彻底改变了我们对“训练”和“推理”的传统认知。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">过去大家觉得。算力主要消耗在“训练”阶段。但是现在看来。未来AI系统的大部分算力消耗。会发生在“使用”阶段。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">想象一下。当一个AI Agent能像人类员工一样。自主完成复杂的任务。它背后“思考”所消耗的计算资源。会是过去简单任务的亿万倍。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这也能解释为什么OpenAI、谷歌这些公司。一边发布更强大的基础模型。一边把重心转向能执行复杂任务的Agent系统。因为这才是算力需求的真正未来。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">不过，这种指数级的增长需求。与华尔街的线性预测模型之间。形成了黄仁勋所说的“巨大的认知分歧”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">访谈主持人布拉德·格斯特纳直接抛出了这个问题。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">一方面。Sam Altman和Sundar Pichai这些行业领袖。都在谈论“万亿级”的算力投资；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">但是另一方面。覆盖Nvidia的25位华尔街分析师。普遍预测Nvidia的增长率会在2027年“断崖式下跌”到大约8%。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_262.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">为什么会有这么大的认知差距呢？黄仁勋给出了一个三层的宏大叙事框架。不仅阐释了Nvidia的增长逻辑。也回答了“增长从哪来”、“增长能持续多久”这两个核心问题。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第一层是“物理定律层面的转变”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">通用计算的时代已经结束。未来是加速计算和AI计算的时代。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">黄仁勋说，摩尔定律已经走到尽头。靠CPU性能提升来推动计算发展的模式。已经行不通了。这意味着。全球现有的、价值数万亿美元的、基于通用计算。也就是CPU的数据中心基础设施。在下一轮更新的时候。必须转向加速计算架构。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如用GPU、TPU这些专门为AI设计的芯片。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这不是“创造新的市场”，而是“存量市场的替换”，仅仅把旧的CPU数据中心替换成加速计算数据中心。就已经是一个庞大的市场空间了。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第二层是“现有应用的迁移”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">把互联网的核心工作负载从CPU迁移到GPU。就足以驱动数百亿美元的需求。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_326.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">黄仁勋举了个例子。像搜索、推荐引擎、电商购物这些。支撑Meta、谷歌、亚马逊、字节跳动等科技巨头核心业务的系统。过去都是跑在CPU上的。但是现在。它们正在全面转向用AI和GPU。因为AI能提供更好的个性化体验。效率也更高。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这个过程不是在“造新的东西”，而是用更先进的技术。重塑一个已经存在的、服务全球40亿人的庞大市场。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">黄仁勋说道。这就像从煤油灯转向电力。从螺旋桨飞机转向喷气式飞机一样。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第三层是“未来的增量”，指的是AI作为“智能工厂”，对全球GDP的赋能。这是最让人兴奋的部分。也是AI创造全新价值的地方。黄仁勋把AI工厂类比成工业革命中的“马达”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">马达替代了体力劳动。而AI工厂则通过生成Token来增强人类的智力劳动。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">他还做了一个经济学估算，首先。全球GDP中。大约50%-65%和“智力劳动”相关。比如设计、研发、咨询、编程这些需要动脑的工作。总价值大概是50万亿美元；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_397.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">其次。AI会对这50万亿美元的经济活动进行“增强”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">假设一家公司有一个年薪10万美元的员工。公司愿意额外花1万美元给这个员工配备AI服务。换来2-3倍的生产力提升。这笔投资是非常划算的。黄仁勋说。Nvidia内部已经给每一位芯片设计师和软件工程师。都配了AI助手。结果生产力的提升非常明显；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">最后。如果全球50万亿美元的“智能GDP”，每年需要价值10万亿美元的“AI Token生成服务”来增强。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">假设这些服务的毛利率是50%，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">那么就需要价值5万亿美元的AI基础设施。来支撑这些服务的运行。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这三层框架一出来。Nvidia的增长逻辑就从“卖芯片”的简单故事。变成了和全球经济结构变迁同频共振的宏大叙事。它清晰地表明，Nvidia的增长。既来自旧设施的替换。也来自现有业务的升级。还来自未来AI创造的新价值。这也能解释为什么Nvidia的收入和数据中心的电力消耗高度相关。因为算力越多。需要的电力就越多。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_467.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如阿里巴巴的吴泳铭就说过。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">他们的数据中心电力消耗会在本年代末增长10倍。而AI生成的Token数量每几个月就翻一番。背后都是对算力的无尽需求。以及对Nvidia AI基础设施的依赖。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">聊完英伟达的增长逻辑。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">我们再来看这次访谈里另一个重磅话题。那就是Nvidia和OpenAI的“星际之门”（Stargate）计划。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">最近大家可能都注意到一个新闻。那就是英伟达要投资OpenAI千亿美元。来搞“星际之门”项目。很多人觉得这只是一次普通的商业合作。Nvidia卖芯片。OpenAI买算力。但是黄仁勋在访谈里明确的说道。这是一次“战略绑定”，背后的逻辑比大家想的深得多。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">黄仁勋毫不掩饰对OpenAI未来的看好。他认为。OpenAI很可能将成为下一个数万亿美元级别的超大规模公司（Hyperscale Company）。什么是超大规模公司？就是像Meta、谷歌、微软这样。既能服务消费者市场。又能服务企业市场。甚至成为全球基础设施一部分的公司。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_528.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">黄仁勋认为。OpenAI未来也会达到这个级别。它的AI服务会渗透到个人生活、企业办公、工业生产等各个领域。就像现在的互联网一样普及。所以。能在OpenAI成为“巨无霸”之前进行投资。黄仁勋觉得是“他们能想象到的最聪明的投资之一”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而且特别关键的一点是。这笔投资不是Nvidia强制要求的。而是OpenAI主动给Nvidia的机会。这说明OpenAI也认可Nvidia的战略价值。想和它深度绑定。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">其次，这次合作的深度和广度。远超外界想象。黄仁勋把双方的合作拆成了三个层次。第一个层次是“现有云合作的延续”，Nvidia会继续和微软合作。为OpenAI在Azure云平台上构建价值“数千亿美元”的算力集群；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">同时，还会和甲骨文、软银合作。建设数个吉瓦（Gigawatts）级的数据中心。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第二个层次是“帮助OpenAI自建基础设施”，这是这次新合作的核心。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">过去OpenAI的算力主要靠租云服务商的资源。但是现在它要自己建AI基础设施了。而Nvidia会从最底层开始参与。包括芯片设计、软件开发、系统集成。甚至整个AI工厂的规划和运营。是“全栈式”的支持。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_607.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第三个层次是“建立直接的战略关系”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">黄仁勋把这种关系类比成Nvidia和Meta的Mark Zuckerberg、谷歌的Sundar Pichai、微软的Satya Nadella、xAI的Elon Musk之间的直接合作。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这意味着OpenAI的规模已经大到。不需要再通过云服务商做“中间人”了。而是可以和最核心的技术供应商直接、平等地对话。形成深度绑定的战略伙伴关系。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">为什么OpenAI要做这样的调整呢？黄仁勋点出了其中的核心原因。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">那就是OpenAI面临“双重指数级增长压力”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">一方面是“用户增长指数”，AI越好用。应用场景越多。用户数量和使用频率就会指数级增长；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">另一方面是“计算增长指数”，就像我们前面聊的。每个用户每次和AI交互。因为“思考”的引入。需要的计算量也在指数级增长。这两个指数叠加在一起。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">意味着OpenAI的算力需求。会以“指数的指数”的速度增长。单靠租云服务已经满足不了了。所以必须同时推进“租云”和“自建”两条路。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_670.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">才能够确保算力供给跟得上需求。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">其实呢与Openai的紧密合作。背后。反映出的正是英伟达坚固的护城河。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">现在市场上有很多竞争对手。比如说AMD英特尔。还有一些公司在做ASIC芯片。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">主持人直接问黄仁勋。Nvidia的竞争护城河是在扩大还是缩小呢？</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">黄仁勋的回答则是。Nvidia真正的护城河。不是某一款芯片的性能优势。而是一种被称为“极限协同设计”（Extreme Co-Design）的系统级创新能力。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">黄仁勋解释了为什么Nvidia要搞“年度发布周期”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">过去芯片的更新是18-24个月一次。但是现在改成了每年一次。因为摩尔定律失效以后。晶体管的性能不再大幅提升。如果不能快速提升整体性能。AI生成Token的成本就会持续上升。而要持续降低Token成本。唯一的办法就是“系统级的创新”，这就是“极限协同设计”的由来。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">黄仁勋说。极限协同设计要求同步优化模型、算法、系统和芯片。让它们像一个整体一样工作。而不是各自为战。这和传统的“盒子内的创新”完全不同。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_740.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">过去是只想着把CPU做得更快。但是现在要同步升级构成AI数据中心的所有核心组件。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">具体来说。“极限协同设计”体现在三个层面。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在芯片层面。Nvidia会并行研发革命性的CPU、GPU、网络芯片和NVLink；。系统层面。会把这些芯片以最优化的方式整合起来。确保它们之间的数据传输效率最高。不会出现“某一个组件拖后腿”的情况；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">软件层面。会提供从底层驱动到上层应用库的完整软件栈。让开发者能轻松用上整个系统的能力。不用自己去解决硬件兼容、数据传输这些复杂问题。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">正是这种跨所有层面的协同设计。才让Nvidia从Hopper架构到Blackwell架构。在一年内实现了性能提升30倍的突破。这绝对不是靠单一芯片的技术进步能做到的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">其次。这种系统能力还体现在“规模化部署”的巨大挑战上。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">黄仁勋举了个例子。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">Elon Musk的xAI公司要部署Colossus 2的集群。需要用到50万个GPU。这不是简单地把50万个GPU堆在一起就行。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_808.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">要考虑电力供应、散热、数据传输延迟、系统稳定性等一系列的问题。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">任何一个环节出问题。整个集群都没法正常工作。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而客户之所以敢下数百亿美元的订单给Nvidia。就是因为Nvidia的架构经过了市场验证。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">能确保这么大规模的系统稳定运行。这种“经得住考验的规模化部署能力”，本身就是一道很高的信任壁垒。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">所以黄仁勋说。现在的竞争已经不是“我的芯片比你的快”，而是“我的整个AI工厂。比你的AI工厂效率更高”了。这种从“组件思维”到“系统思维”的跃迁。正是Nvidia能远超竞争对手的根本原因。毕竟。竞争对手可能能做出一款性能不错的芯片。但是要想做到“芯片、系统、软件”全链条的协同创新。还要能支撑几十万GPU的规模化部署。难度要大得多。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">黄仁勋甚至说。即使竞争对手把他们的ASIC芯片免费送给客户。客户还是应该选择Nvidia的系统。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_870.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">很多人听到这话会觉得不可思议。免费的芯片都不要？但是黄仁勋的逻辑。其实紧扣了数据中心的“现实约束”，那就是电力和空间都是有限的。黄仁勋解释说，数据中心建设中。土地、电力、建筑这些投入的成本非常高。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">当一家企业拿到宝贵的2吉瓦电力配额的时候。它的核心目标不再是“节省芯片成本”，而是“用这些电力创造最大的商业价值”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">他用简单的算术算了一笔账。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">如果每瓦性能。或者说每瓦能生成的Token数量。是竞争对手的两倍。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">那么客户用同样的数据中心。就能产生两倍的收入，谁不想要呢？</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">再具体一点。假设竞争对手的ASIC芯片。性能和Nvidia上一代的Hopper GPU差不多；。而Nvidia新一代的Blackwell GPU。性能是Hopper的30倍。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这意味着，在同样的电力消耗下。用Blackwell的客户能够获得30倍的潜在收入。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在这种情况下。为了节省一点芯片成本而放弃30倍的收入。这种机会成本显然“高得离谱”。任何理性的CFO。都会选择“每瓦性能”最高的解决方案。因为这直接决定了企业收入的上限。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_940.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">除此之外。黄仁勋还分析了ASIC的“生态定位”。他认为。ASIC适合那些“功能固定、市场规模有限”的领域。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如视频转码器、智能网卡等等。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这些任务的算法很少变。用ASIC能做到很高的效率。但是对于AI这种“工作负载多样且快速变化”的领域。ASIC的“专用性”反而成了致命弱点。因为AI需要处理的任务实在是太多了。聊天、写代码、生成图片视频、做数据分析、制定商业计划。等等等等。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而且底层算法还在不断演进。这就要求计算平台必须具备高度的“可编程性”，能够快速适配新任务、新算法。而这正是GPU和CUDA生态的核心优势。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这也能解释为什么谷歌虽然有自己的TPU。但同时也是Nvidia GPU的大客户。因为在一个复杂的计算集群里。既需要TPU这样的“专用辅助”芯片。也需要GPU这样的“通用主力”芯片。通过合理组合来实现整体最优。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_1001.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而Nvidia通过开放NVLink Fusion等接口。允许英特尔等公司的芯片接入自己的生态。这正是“平台化”和“生态化”思维的体现。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">非但不靠封闭来阻挡对手。反而是靠开放来扩大生态。让更多伙伴参与进来。一起把AI算力的市场做大。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在这场对话中。黄仁勋还花了大量篇幅讨论了全球人工智能竞赛。尤其是美国与中国的关系。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">黄仁勋首先提出了一个观点。AI基础设施已经成为和能源、通信同等重要的国家战略资源。所以全球范围内正在掀起“主权AI”（Sovereign AI）的建设浪潮。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">什么是主权AI？就是每个国家都需要拥有自己的AI基础设施。用自己的数据和文化训练AI模型。确保AI能服务于本国的特定需求。不管是工业生产、制造业升级。还是国家安全。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">黄仁勋认为。虽然各国会使用GPT、Gemini这些全球领先的模型。但是同时必须建立自己的主权AI能力。因为AI不仅是技术。还承载着文化、价值观和历史。一个国家不能把核心的智能需求。完全依赖于其他国家的技术。就像每个国家都会有自己的电网、通信网络一样。未来也会有自己的AI基础设施网络。而这为Nvidia等基础设施提供商。创造了全球性的全新市场。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_1083.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">关于美国的对华技术政策。黄仁勋提出了坦率的批评。他认为。美国采取“小院高墙”式的对华技术封锁。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如限制Nvidia向中国出口高端芯片。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">看起来是在遏制中国AI发展的做法。但是实际上不仅徒劳无功。反而更是一种危险的“单方面裁军”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">他指出了这种政策的两个主要后果。第一，会催生强大的竞争对手。把拥有95%市场份额的Nvidia排除出中国市场。相当于把整个中国市场拱手让给华为等本土企业。这些企业会在“没有强竞争”的环境下。会靠着“垄断利润”加速技术研发和产能扩张。最终成长为Nvidia在全球市场的强劲对手。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第二，严重低估了中国的能力。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">黄仁勋曾经警告说。外界普遍认为中国造不出高端芯片。或者技术上落后美国数年。这些想法都是“疯狂”的。实际上。中国拥有世界上最渴望成功、最勤奋的企业家。还有充满活力的内部竞争生态。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_1144.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">中国在芯片和AI领域和美国的技术差距。其实是以“纳秒”来计算的。不是大家想的“几年”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">当然这里的纳秒是打了引号的。意思是强调中美之间的差距很小。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">那么，正确的路径应该是什么呢？黄仁勋认为。让美国最优秀的企业在中国市场和本土企业直接竞争。才最符合美国的国家利益。这样做不仅能为美国企业创造经济价值。还能让美国通过技术影响力。在全球AI格局中保持话语权；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">更重要的是。竞争能倒逼美国科技企业不断创新。保持在技术最前沿。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">黄仁勋说道，一个自信、强大的国家。应该秉持‘放马过来’（Bring it on）的态度。相信自己的体系和人民能在竞争中胜出。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在谈到美国的核心优势时。黄仁勋的回答更是尖锐。他说。美国拥有一个世界上任何国家都没有的独特品牌声誉。那就是来到美国，实现美国梦。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">作为一个从中国台湾移民到美国。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_1204.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">从餐馆洗碗工成长为万亿市值公司CEO的亲历者。黄仁勋对“美国梦”的理解非常深刻。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这种“让每个人都有机会通过努力改变命运”的信念。是美国吸引全球顶尖人才的根本原因。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如过去几十年。全球最优秀的科学家、工程师、创业者都愿意去美国。因为那里有更好的机会、更开放的环境。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">但是黄仁勋警告说，近些年来。这个核心优势正在受到严重的挑战。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">他观察到一个非常危险的信号。顶尖中国AI研究者来美国的意愿。已经从三年前的90%骤降到现在的10%-15%。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这不是一个小变化。而是关乎美国未来的“生存危机”级别的早期预警。因为AI行业的竞争。本质上是人才的竞争，没有顶尖人才。再先进的技术也难以持续领先。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">所以黄仁勋呼吁美国政策制定者。必须谨慎区分“与中国竞争”和“对中国人强硬”这两个概念。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">“与中国竞争”是在技术、市场上的良性比拼。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_1266.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而“对中国人强硬”则是把优秀的中国人才拒之门外。这会摧毁美国最宝贵的资产。也就是美国作为全球人才灯塔的品牌形象。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">展望未来。黄仁勋认为人工智能将从根本上改变社会。他坚信。人工智能会带来巨大的生产力提升。而不是大规模的失业。那种认为AI会摧毁就业的观点。前提是“我们再也没有新的想法了”，但是他认为智力不是零和游戏。周围聪明的人和工具越多。能想到的新点子、能解决的新问题就越多。创造的岗位也会越多。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">每项工作都会改变，有些会消失。但是经济整体会增长。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在他看来。AI本身就是最伟大的均衡器。过去。一个人想利用计算机创造经济价值。至少得学习Python编程。现在，他们只需要学习人类语言。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">技术鸿沟正在被技术本身填平。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">对于未来的具体形态。他预言在未来五年内。人工智能与机器人技术的融合将成为现实。每个人都会像电影星球大战中一样。有自己的“R2-D2”机器人。成为生活中的伙伴和向导。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_1334.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">云端的人工智能和实体世界的机器人将无处不在。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">生物学的复杂性将被揭示。每个人都将拥有自己的“数字孪生”，用于预测健康状况和疾病。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">面对这种指数级加速的变化。黄仁勋给出的建议很简单。那就是登上那列火车。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">不要试图去预测火车未来会到哪个站点。因为当它呈指数级加速的时候。任何预测都是徒劳的。唯一的策略就是趁现在它还相对较慢时跳上去。然后随着它一起经历指数级的旅程。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">也许，从芯片公司到AI基础设施公司。英伟达的进化本身就是登上这列火车的最好证明。而对于整个世界来说。这趟旅程才刚刚开始。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">好了，感谢观看本期视频。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">我们下期再见。</p>

<hr style="clear:both" />
=============
<p><a href="https://www.youtube.com/watch?v=--HbDQk2-jA">https://www.youtube.com/watch?v=--HbDQk2-jA</a></p><p>9月26日，知名播客BG2和英伟达的CEO黄仁勋进行了一场独家对话。这次访谈的标题叫做“NVIDIA：OpenAI、计算的未来和美国梦”，时长超过100分钟，堪称黄仁勋近期信息密度最高、干货最足的一次分享。在这场对话中，黄仁勋系统性地解释了华尔街与硅谷之间存在的巨大认知分歧，并且详细拆解了英伟达看似坚不可摧的商业护城河，以及他对全球人工智能竞赛、大国博弈和未来社会形态的完整思考，今天我们就来回顾一下这场访谈的内容。</p><p><a href="https://youtu.be/pE6sw_E9Gh0?si=L41UzPmgS_lP5i43">https://youtu.be/pE6sw_E9Gh0?si=L41UzPmgS_lP5i43</a></p>]]>
      </description>
      <content:encoded><![CDATA[<hr style="clear:both" />

<p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://img.youtube.com/vi/--HbDQk2-jA/maxresdefault.jpg" /></p><p><a href="https://www.youtube.com/watch?v=--HbDQk2-jA">https://www.youtube.com/watch?v=--HbDQk2-jA</a></p><h1>值得閱讀的理由</h1> <ul> <li>深入了解<strong>黃仁勳對AI算力需求的獨到見解</strong>，包括他提出的「三重縮放定律」，揭示AI未來主要算力消耗將從訓練轉向使用。</li> <li>掌握<strong>NVIDIA如何構建其堅不可摧的商業護城河</strong>，不僅透過「極限協同設計」實現系統級創新，更透過開放生態策略應對競爭。</li> <li>理解<strong>黃仁勳對全球AI競賽、美中科技關係及AI社會影響的全面思考</strong>，包括他對「主權AI」的定義、對美中政策的批判，以及對「美國夢」作為人才磁石的深刻洞察。</li> </ul> <hr /> <h1>摘要</h1> <p>這段影片由「最佳拍檔」的「大飛」主持，回顧了NVIDIA執行長<strong>黃仁勳</strong>與知名播客BG2長達100多分鐘的獨家對話。這場訪談標題為「NVIDIA：OpenAI、計算的未來與美國夢」，被譽為黃仁勳近期資訊密度最高、含金量最足的一次分享。影片作者表示，黃仁勳在訪談中系統性地闡釋了華爾街與矽谷之間存在的巨大認知分歧，詳細剖析了NVIDIA看似堅不可摧的商業護城河，並分享了他對全球人工智能競賽、大國博弈及未來社會形態的完整思考。</p> <p><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_0.jpg" /></p></p> <hr /> <h2>AI算力需求的「三重縮放定律」</h2> <p>影片作者指出，一年前市場曾擔憂AI預訓練需求放緩會導致算力中心過剩，但黃仁勳當時大膽預測推理（Inference）需求將成長「十億倍」的量級，而事實證明，連這個預測都低估了實際需求。黃仁勳在訪談中系統性地提出了AI算力需求由三種「<strong>縮放定律</strong>」共同驅動的關鍵觀點：</p> <p>第一種是「<strong>預訓練縮放定律</strong>」（Pre-training Scaling Law），即模型越大、數據越多、訓練時間越長，模型就越智能。這是過去幾年大模型算力增長的主要驅動力。</p> <p>第二種是「<strong>後訓練縮放定律</strong>」（Post-training Scaling Law），指的是模型在預訓練後，透過強化學習和大量「試錯」、「推理」來精通特定技能的過程，這個「練習」過程本身需要消耗大量的推理計算。</p> <p>第三種是「<strong>推理時思維縮放定律</strong>」（Inference-time Thinking Scaling Law），黃仁勳認為這是市場最未理解透徹的「革命性」驅動因素。它描述AI在回答問題前，會進行內部「思考」、查證、梳理邏輯，這是一個多輪的內部推理過程，而非簡單一步計算。任務越複雜，AI「思考」時間越長，答案質量越高，而每一次內部循環都是一次計算消耗。</p> <p>這三重定律徹底改變了傳統認知，未來AI系統的絕大部分算力消耗將發生在「使用」階段，尤其當AI Agent能自主完成複雜任務時，其背後「思考」的計算資源將是過去的億萬倍。</p> <p><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_90.jpg" /></p></p> <hr /> <h2>黃仁勳解析NVIDIA增長邏輯：三大宏大敘事框架</h2> <p>影片作者提到，這種指數級的增長需求與華爾街的線性預測模型之間形成了黃仁勳所說的「<strong>巨大的認知分歧</strong>」。主持人布拉德·格斯特納直接指出，一方面，Sam Altman和Sundar Pichai等行業領袖談論「萬億級」算力投資；另一方面，華爾街分析師普遍預測NVIDIA的增長率將在2027年「斷崖式下跌」到約8%。黃仁勳為此給出了一個三層宏大敘事框架，闡釋NVIDIA的增長邏輯：</p> <p>第一層是「<strong>物理定律層面的轉變</strong>」，即通用計算時代已結束，未來是<strong>加速計算和AI計算</strong>的時代。隨著摩爾定律走到盡頭，靠CPU性能提升推動計算發展的模式已不可行。全球價值數萬億美元的CPU數據中心基礎設施，在下一輪更新時必須轉向GPU、TPU等加速計算架構。這是一個龐大的「<strong>存量市場替換</strong>」，而非創造新市場。</p> <p>第二層是「<strong>現有應用的遷移</strong>」，將網際網路的核心工作負載從CPU遷移到GPU，這足以驅動數百億美元的需求。黃仁勳以Meta、Google、Amazon、字節跳動等巨頭的核心業務（如搜索、推薦引擎、電商購物）為例，說明它們正全面轉向AI和GPU，以提供更好的個性化體驗和更高效率。這是在用更先進的技術重塑一個已服務全球40億人的龐大市場。</p> <p>第三層是「<strong>未來的增量</strong>」，指的是<strong>AI作為「智能工廠」對全球GDP的賦能</strong>。這是最令人興奮的部分，也是AI創造全新價值的地方。黃仁勳將AI工廠類比為工業革命中的「馬達」，透過生成Token來增強人類的智力勞動。他估算全球50萬億美元的「智力勞動」相關GDP，若每年需要10萬億美元的「AI Token生成服務」來增強，以50%毛利率計算，將需要5萬億美元的AI基礎設施來支撐。這三層框架清晰表明NVIDIA的增長既來自舊設施替換、現有業務升級，也來自未來AI創造的新價值。</p> <p><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_230.jpg" /></p></p> <hr /> <h2>NVIDIA與OpenAI的「星際之門」戰略結盟</h2> <p>影片作者接著探討了NVIDIA與OpenAI的「<strong>星際之門</strong>」（Stargate）計畫。黃仁勳明確指出，這不是一次普通的商業合作，而是一次「<strong>戰略綁定</strong>」，背後邏輯深遠。他毫不掩飾對OpenAI未來的看好，認為OpenAI很可能成為下一個數萬億美元級別的「超大規模公司」（Hyperscale Company），滲透到個人生活、企業辦公、工業生產等各領域。因此，能在OpenAI成為巨擘前進行投資，黃仁勳認為是「能想像到的最聰明投資之一」，且這是OpenAI主動給NVIDIA的機會。</p> <p>這次合作的深度和廣度遠超外界想像，黃仁勳將其拆分為三個層次：</p> <p>首先是「<strong>現有雲合作的延續</strong>」，NVIDIA將繼續與微軟合作，為OpenAI在Azure雲平台構建價值「數千億美元」的算力集群；同時，還會與甲骨文、軟銀合作，建設數個吉瓦（Gigawatts）級數據中心。</p> <p>其次是「<strong>幫助OpenAI自建基礎設施</strong>」，這是新合作的核心。過去OpenAI主要租用雲服務商資源，但現在將自行建設AI基礎設施，NVIDIA會從最底層開始參與，包括晶片設計、軟體開發、系統集成，甚至是整個AI工廠的規劃和營運，提供「<strong>全棧式</strong>」支持。</p> <p>最後是「<strong>建立直接的戰略關係</strong>」，黃仁勳將此關係類比為NVIDIA與Meta、Google、微軟、xAI等頂級科技公司的直接合作，意味著OpenAI規模已大到可與核心技術供應商直接、平等對話，形成深度綁定的戰略夥伴關係。</p> <p>黃仁勳指出OpenAI做出此調整的核心原因，是面臨「<strong>雙重指數級增長壓力</strong>」。一方面是「用戶增長指數」，AI越好用，用戶數量和使用頻率就指數級增長；另一方面是「計算增長指數」，每個用戶每次與AI交互所需的計算量也在指數級增長。這兩者疊加，意味著OpenAI的算力需求將以「<strong>指數的指數</strong>」速度增長，單靠租用雲服務已無法滿足，必須同時推進「租雲」和「自建」兩條路。</p> <p><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_450.jpg" /></p></p> <hr /> <h2>英偉達的堅實護城河：「極限協同設計」與系統級創新</h2> <p>影片作者指出，與OpenAI的緊密合作，反映出NVIDIA堅固的護城河。面對AMD、英特爾以及ASIC晶片公司的競爭，黃仁勳表示NVIDIA真正的護城河不是單一晶片的性能優勢，而是一種「<strong>極限協同設計</strong>」（Extreme Co-Design）的系統級創新能力。</p> <p>首先，他解釋了NVIDIA為何要搞「<strong>年度發布週期</strong>」。隨著摩爾定律失效，晶體管性能不再大幅提升，若不能快速提升整體性能，AI生成Token的成本將持續上升。為持續降低Token成本，唯一的辦法就是「系統級的創新」，這正是「極限協同設計」的由來。黃仁勳說，極限協同設計要求<strong>同步優化模型、演算法、系統和晶片</strong>，讓它們像一個整體一樣工作，而非各自為戰。這體現在晶片層面（并行研發CPU、GPU、網路晶片和NVLink）、系統層面（晶片整合優化，數據傳輸效率）、以及軟體層面（提供從底層驅動到上層應用庫的完整軟體棧），才使NVIDIA在一年內實現了30倍的性能提升。</p> <p>其次，這種系統能力還體現在「<strong>規模化部署</strong>」的巨大挑戰上。黃仁勳以Elon Musk的xAI公司需部署50萬個GPU為例，這不僅是硬體堆疊，還要考慮電力供應、散熱、數據傳輸延遲、系統穩定性等一系列問題。客戶之所以敢下數百億美元訂單給NVIDIA，是因為NVIDIA的架構經過市場驗證，能確保大規模系統穩定運行。這種「經得起考驗的規模化部署能力」本身就是一道很高的信任壁壘。黃仁勳強調，現在的競爭已是「我的整個<strong>AI工廠</strong>比你的AI工廠效率更高」。</p> <p>他甚至斷言，即使競爭對手把ASIC晶片免費送給客戶，客戶仍應選擇NVIDIA的系統。黃仁勳解釋說，數據中心建設中土地、電力、建築成本高昂，當企業拿到寶貴的電力配額時，核心目標是「用這些電力創造最大的商業價值」。若NVIDIA的「<strong>每瓦性能</strong>」（即每瓦能生成的Token數量）是競爭對手的兩倍，客戶就能產生兩倍收入。如果Blackwell GPU性能是上一代Hopper的30倍，在同樣電力消耗下，客戶能獲得30倍潛在收入。為省一點晶片成本而放棄30倍收入，機會成本「高得離譜」。</p> <p>此外，黃仁勳分析了ASIC的「生態定位」。他認為ASIC適合「功能固定、市場規模有限」的領域（如視頻轉碼），但對於「工作負載多樣且快速變化」的AI領域，ASIC的「專用性」反成致命弱點。AI需要高度「<strong>可程式性</strong>」的計算平台來快速適配新任務、新演算法，這正是GPU和CUDA生態的核心優勢。NVIDIA透過開放NVLink Fusion等接口，允許英特爾等公司晶片接入自家生態，展現「平台化」與「生態化」思維，透過開放來擴大生態，共同做大AI算力市場。</p> <p><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_660.jpg" /></p></p> <hr /> <h2>全球AI競賽與美中關係：黃仁勳的坦率批判</h2> <p>影片作者指出，黃仁勳在訪談中還花了大量篇幅討論全球人工智能競賽，尤其是美國與中國的關係。黃仁勳首先提出AI基礎設施已成為與能源、通信同等重要的<strong>國家戰略資源</strong>，全球正掀起「<strong>主權AI</strong>」（Sovereign AI）建設浪潮。他認為每個國家都需要擁有自己的AI基礎設施，用自己的數據和文化訓練AI模型，確保AI能服務於本國特定需求，就像每個國家都有自己的電網、通信網路一樣，未來也會有自己的AI基礎設施網路。</p> <p>關於美國的對華技術政策，黃仁勳坦率批評美國採取「<strong>小院高牆</strong>」式的對華技術封鎖（如限制NVIDIA向中國出口高端晶片），這看起來是在遏制中國AI發展，但實際上不僅徒勞無功，反而更是一種危險的「<strong>單方面裁軍</strong>」。他指出這種政策的兩個主要後果：</p> <p>第一，會<strong>催生強大的競爭對手</strong>。將擁有95%市場份額的NVIDIA排除出中國市場，相當於將整個中國市場拱手讓給華為等本土企業。這些企業會在「沒有強競爭」的環境下，靠「壟斷利潤」加速技術研發和產能擴張，最終成長為NVIDIA在全球市場的強勁對手。</p> <p>第二，<strong>嚴重低估了中國的能力</strong>。黃仁勳曾警告，外界普遍認為中國造不出高端晶片或技術落後美國數年的想法都是「瘋狂的」。中國擁有最渴望成功、最勤奮的企業家以及充滿活力的內部競爭生態。中美在晶片和AI領域的技術差距其實是以「納秒」來計算的，而非數年。</p> <p>黃仁勳認為，正確的路徑是讓美國最優秀的企業在中國市場與本土企業直接競爭，這最符合美國的國家利益。這樣不僅能為美國企業創造經濟價值，還能讓美國透過技術影響力在全球AI格局中保持話語權，更重要的是，競爭能倒逼美國科技企業不斷創新，保持技術最前沿。他強調，一個自信、強大的國家應秉持「<strong>放馬過來</strong>」（Bring it on）的態度。</p> <p>談到美國的核心優勢，黃仁勳尖銳地指出，美國擁有世界上任何國家都沒有的獨特品牌聲譽：「<strong>來到美國，實現美國夢</strong>」。作為從中國台灣移民到美國、從餐館洗碗工成長為萬億市值公司CEO的親歷者，黃仁勳對「美國夢」理解深刻。這種「讓每個人都有機會透過努力改變命運」的信念，是美國吸引全球頂尖人才的根本原因。但他警告，近年來這個核心優勢正受到嚴重挑戰。他觀察到一個危險信號：頂尖中國AI研究者來美國的意願已從三年前的90%驟降到現在的10%-15%。他呼籲美國政策制定者必須謹慎區分「與中國競爭」和「對中國人強硬」這兩個概念，後者會摧毀美國最寶貴的資產——作為全球人才燈塔的品牌形象。</p> <p><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_1050.jpg" /></p></p> <hr /> <h2>AI的未來願景與社會變革</h2> <p>影片作者總結道，展望未來，黃仁勳堅信人工智能將從根本上改變社會，帶來巨大的<strong>生產力提升</strong>，而非大規模失業。他認為智力不是零和遊戲，周圍聰明的人和工具越多，能想到的新點子、能解決的新問題就越多，創造的崗位也會越多。每項工作都會改變，有些會消失，但經濟整體會增長。在他看來，AI本身就是最偉大的<strong>均衡器</strong>，過去一個人想利用計算機創造經濟價值至少要學習Python編程，現在只需學習人類語言，技術鴻溝正被技術本身填平。</p> <p>對於未來的具體形態，他預言在未來五年內，人工智能與機器人技術的融合將成為現實，每個人都會像電影《星球大戰》中一樣擁有自己的「<strong>R2-D2</strong>」機器人。雲端的人工智能和實體世界的機器人將無處不在，生物學的複雜性將被揭示，每個人都將擁有自己的「<strong>數字孿生</strong>」，用於預測健康狀況和疾病。面對這種指數級加速的變化，黃仁勳給出的建議很簡單：就是「<strong>登上那列火車</strong>」，不要試圖去預測火車未來會到哪個站點，因為當它呈指數級加速時，任何預測都是徒勞的。唯一的策略就是趁現在它還相對較慢時跳上去，然後隨著它一起經歷指數級的旅程。NVIDIA從晶片公司到AI基礎設施公司的進化本身，就是登上這列火車的最好證明，而對於整個世界來說，這趟旅程才剛剛開始。</p> <p><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_23.jpg" /></p></p> <hr /><hr />================================================</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">大家好，这里是最佳拍档，我是大飞。9月26日。知名播客BG2和英伟达的CEO黄仁勋进行了一场独家对话。这次访谈的标题叫做“NVIDIA：。OpenAI、计算的未来和美国梦”，时长超过100分钟。堪称黄仁勋近期信息密度最高、干货最足的一次分享。在这场对话中。黄仁勋系统性地解释了华尔街与硅谷之间存在的巨大认知分歧。并且详细拆解了英伟达看似坚不可摧的商业护城河。以及他对全球人工智能竞赛、大国博弈和未来社会形态的完整思考。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">今天我们就来回顾一下这场访谈的内容。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">可能大家还记得。一年前市场上有个挺流行的担忧。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">AI的预训练需求好像暂时放缓了。会不会导致之前建的算力中心过剩呢？当时黄仁勋给出了一个非常大胆的预测。推理（Inference）需求的增长不是100倍。也不是1000倍。而是会达到“10亿倍”的量级。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_60.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">现在一年过去了，事实证明。连黄仁勋当时的这个预测。都还是低估了实际的需求增长。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">为什么会这样呢？黄仁勋在访谈里系统性地提出了一个关键观点。那就是AI的算力需求。其实是由三种“缩放定律”（Scaling Laws）所共同驱动的。而不是大家之前以为的只有一种。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第一种是“预训练缩放定律”（Pre-training Scaling Law）。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这也是大家最熟悉的一种。简单说就是模型越大、用的数据越多、训练时间越长。模型就越智能。过去几年。不管是GPT系列还是其他大模型。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">其实都是靠这个定律驱动算力增长的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如GPT-3用了千亿参数。训练时消耗的算力达到了每天几百PFlops。这背后就是预训练缩放定律在起作用。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">但是黄仁勋强调。这只是算力需求的“第一引擎”，真正关键的是另外两种。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第二种是“后训练缩放定律”（Post-training Scaling Law）。后训练指的就是模型在预训练之后。通过类似“练习”的方式。来精通某项特定技能的过程。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_122.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这个过程会结合强化学习。让AI通过大量“试错”和“推理”来优化自己。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">举个例子，我们想让AI学会写代码。光给它看海量的代码库还不够。还得让它不断尝试编写代码、调试错误、运行测试。直到能写出符合要求的程序。这个“练习”的过程，就是后训练。而这个过程本身。需要消耗的推理计算量非常大。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第三种是“推理时思维缩放定律”（Inference-time Thinking Scaling Law）。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这也是黄仁勋认为市场最没理解透的一点。堪称“革命性”的算力驱动因素。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">他在访谈里说道。旧的推理方式是一次性的。你问AI一个问题，它会直接给你答案。但是新推理方式是‘思考’，也就是在回答之前。AI会先自己琢磨、查证、梳理逻辑。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这种“思考”不是简单的一步计算。而是一个复杂的过程。AI在生成最终答案前。会进行多轮的内部推理、研究事实。甚至调用外部工具。这个过程简单的话会形成“思维链”，复杂一点的话还会形成“推理树”，而且任务越复杂。AI“思考”的时间就越长。答案质量就越高，而每一次内部循环。都是一次计算消耗。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_195.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这三种缩放定律放在一起。就彻底改变了我们对“训练”和“推理”的传统认知。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">过去大家觉得。算力主要消耗在“训练”阶段。但是现在看来。未来AI系统的大部分算力消耗。会发生在“使用”阶段。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">想象一下。当一个AI Agent能像人类员工一样。自主完成复杂的任务。它背后“思考”所消耗的计算资源。会是过去简单任务的亿万倍。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这也能解释为什么OpenAI、谷歌这些公司。一边发布更强大的基础模型。一边把重心转向能执行复杂任务的Agent系统。因为这才是算力需求的真正未来。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">不过，这种指数级的增长需求。与华尔街的线性预测模型之间。形成了黄仁勋所说的“巨大的认知分歧”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">访谈主持人布拉德·格斯特纳直接抛出了这个问题。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">一方面。Sam Altman和Sundar Pichai这些行业领袖。都在谈论“万亿级”的算力投资；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">但是另一方面。覆盖Nvidia的25位华尔街分析师。普遍预测Nvidia的增长率会在2027年“断崖式下跌”到大约8%。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_262.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">为什么会有这么大的认知差距呢？黄仁勋给出了一个三层的宏大叙事框架。不仅阐释了Nvidia的增长逻辑。也回答了“增长从哪来”、“增长能持续多久”这两个核心问题。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第一层是“物理定律层面的转变”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">通用计算的时代已经结束。未来是加速计算和AI计算的时代。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">黄仁勋说，摩尔定律已经走到尽头。靠CPU性能提升来推动计算发展的模式。已经行不通了。这意味着。全球现有的、价值数万亿美元的、基于通用计算。也就是CPU的数据中心基础设施。在下一轮更新的时候。必须转向加速计算架构。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如用GPU、TPU这些专门为AI设计的芯片。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这不是“创造新的市场”，而是“存量市场的替换”，仅仅把旧的CPU数据中心替换成加速计算数据中心。就已经是一个庞大的市场空间了。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第二层是“现有应用的迁移”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">把互联网的核心工作负载从CPU迁移到GPU。就足以驱动数百亿美元的需求。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_326.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">黄仁勋举了个例子。像搜索、推荐引擎、电商购物这些。支撑Meta、谷歌、亚马逊、字节跳动等科技巨头核心业务的系统。过去都是跑在CPU上的。但是现在。它们正在全面转向用AI和GPU。因为AI能提供更好的个性化体验。效率也更高。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这个过程不是在“造新的东西”，而是用更先进的技术。重塑一个已经存在的、服务全球40亿人的庞大市场。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">黄仁勋说道。这就像从煤油灯转向电力。从螺旋桨飞机转向喷气式飞机一样。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第三层是“未来的增量”，指的是AI作为“智能工厂”，对全球GDP的赋能。这是最让人兴奋的部分。也是AI创造全新价值的地方。黄仁勋把AI工厂类比成工业革命中的“马达”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">马达替代了体力劳动。而AI工厂则通过生成Token来增强人类的智力劳动。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">他还做了一个经济学估算，首先。全球GDP中。大约50%-65%和“智力劳动”相关。比如设计、研发、咨询、编程这些需要动脑的工作。总价值大概是50万亿美元；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_397.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">其次。AI会对这50万亿美元的经济活动进行“增强”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">假设一家公司有一个年薪10万美元的员工。公司愿意额外花1万美元给这个员工配备AI服务。换来2-3倍的生产力提升。这笔投资是非常划算的。黄仁勋说。Nvidia内部已经给每一位芯片设计师和软件工程师。都配了AI助手。结果生产力的提升非常明显；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">最后。如果全球50万亿美元的“智能GDP”，每年需要价值10万亿美元的“AI Token生成服务”来增强。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">假设这些服务的毛利率是50%，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">那么就需要价值5万亿美元的AI基础设施。来支撑这些服务的运行。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这三层框架一出来。Nvidia的增长逻辑就从“卖芯片”的简单故事。变成了和全球经济结构变迁同频共振的宏大叙事。它清晰地表明，Nvidia的增长。既来自旧设施的替换。也来自现有业务的升级。还来自未来AI创造的新价值。这也能解释为什么Nvidia的收入和数据中心的电力消耗高度相关。因为算力越多。需要的电力就越多。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_467.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如阿里巴巴的吴泳铭就说过。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">他们的数据中心电力消耗会在本年代末增长10倍。而AI生成的Token数量每几个月就翻一番。背后都是对算力的无尽需求。以及对Nvidia AI基础设施的依赖。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">聊完英伟达的增长逻辑。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">我们再来看这次访谈里另一个重磅话题。那就是Nvidia和OpenAI的“星际之门”（Stargate）计划。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">最近大家可能都注意到一个新闻。那就是英伟达要投资OpenAI千亿美元。来搞“星际之门”项目。很多人觉得这只是一次普通的商业合作。Nvidia卖芯片。OpenAI买算力。但是黄仁勋在访谈里明确的说道。这是一次“战略绑定”，背后的逻辑比大家想的深得多。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">黄仁勋毫不掩饰对OpenAI未来的看好。他认为。OpenAI很可能将成为下一个数万亿美元级别的超大规模公司（Hyperscale Company）。什么是超大规模公司？就是像Meta、谷歌、微软这样。既能服务消费者市场。又能服务企业市场。甚至成为全球基础设施一部分的公司。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_528.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">黄仁勋认为。OpenAI未来也会达到这个级别。它的AI服务会渗透到个人生活、企业办公、工业生产等各个领域。就像现在的互联网一样普及。所以。能在OpenAI成为“巨无霸”之前进行投资。黄仁勋觉得是“他们能想象到的最聪明的投资之一”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而且特别关键的一点是。这笔投资不是Nvidia强制要求的。而是OpenAI主动给Nvidia的机会。这说明OpenAI也认可Nvidia的战略价值。想和它深度绑定。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">其次，这次合作的深度和广度。远超外界想象。黄仁勋把双方的合作拆成了三个层次。第一个层次是“现有云合作的延续”，Nvidia会继续和微软合作。为OpenAI在Azure云平台上构建价值“数千亿美元”的算力集群；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">同时，还会和甲骨文、软银合作。建设数个吉瓦（Gigawatts）级的数据中心。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第二个层次是“帮助OpenAI自建基础设施”，这是这次新合作的核心。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">过去OpenAI的算力主要靠租云服务商的资源。但是现在它要自己建AI基础设施了。而Nvidia会从最底层开始参与。包括芯片设计、软件开发、系统集成。甚至整个AI工厂的规划和运营。是“全栈式”的支持。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_607.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第三个层次是“建立直接的战略关系”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">黄仁勋把这种关系类比成Nvidia和Meta的Mark Zuckerberg、谷歌的Sundar Pichai、微软的Satya Nadella、xAI的Elon Musk之间的直接合作。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这意味着OpenAI的规模已经大到。不需要再通过云服务商做“中间人”了。而是可以和最核心的技术供应商直接、平等地对话。形成深度绑定的战略伙伴关系。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">为什么OpenAI要做这样的调整呢？黄仁勋点出了其中的核心原因。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">那就是OpenAI面临“双重指数级增长压力”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">一方面是“用户增长指数”，AI越好用。应用场景越多。用户数量和使用频率就会指数级增长；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">另一方面是“计算增长指数”，就像我们前面聊的。每个用户每次和AI交互。因为“思考”的引入。需要的计算量也在指数级增长。这两个指数叠加在一起。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">意味着OpenAI的算力需求。会以“指数的指数”的速度增长。单靠租云服务已经满足不了了。所以必须同时推进“租云”和“自建”两条路。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_670.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">才能够确保算力供给跟得上需求。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">其实呢与Openai的紧密合作。背后。反映出的正是英伟达坚固的护城河。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">现在市场上有很多竞争对手。比如说AMD英特尔。还有一些公司在做ASIC芯片。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">主持人直接问黄仁勋。Nvidia的竞争护城河是在扩大还是缩小呢？</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">黄仁勋的回答则是。Nvidia真正的护城河。不是某一款芯片的性能优势。而是一种被称为“极限协同设计”（Extreme Co-Design）的系统级创新能力。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">黄仁勋解释了为什么Nvidia要搞“年度发布周期”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">过去芯片的更新是18-24个月一次。但是现在改成了每年一次。因为摩尔定律失效以后。晶体管的性能不再大幅提升。如果不能快速提升整体性能。AI生成Token的成本就会持续上升。而要持续降低Token成本。唯一的办法就是“系统级的创新”，这就是“极限协同设计”的由来。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">黄仁勋说。极限协同设计要求同步优化模型、算法、系统和芯片。让它们像一个整体一样工作。而不是各自为战。这和传统的“盒子内的创新”完全不同。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_740.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">过去是只想着把CPU做得更快。但是现在要同步升级构成AI数据中心的所有核心组件。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">具体来说。“极限协同设计”体现在三个层面。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在芯片层面。Nvidia会并行研发革命性的CPU、GPU、网络芯片和NVLink；。系统层面。会把这些芯片以最优化的方式整合起来。确保它们之间的数据传输效率最高。不会出现“某一个组件拖后腿”的情况；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">软件层面。会提供从底层驱动到上层应用库的完整软件栈。让开发者能轻松用上整个系统的能力。不用自己去解决硬件兼容、数据传输这些复杂问题。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">正是这种跨所有层面的协同设计。才让Nvidia从Hopper架构到Blackwell架构。在一年内实现了性能提升30倍的突破。这绝对不是靠单一芯片的技术进步能做到的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">其次。这种系统能力还体现在“规模化部署”的巨大挑战上。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">黄仁勋举了个例子。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">Elon Musk的xAI公司要部署Colossus 2的集群。需要用到50万个GPU。这不是简单地把50万个GPU堆在一起就行。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_808.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">要考虑电力供应、散热、数据传输延迟、系统稳定性等一系列的问题。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">任何一个环节出问题。整个集群都没法正常工作。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而客户之所以敢下数百亿美元的订单给Nvidia。就是因为Nvidia的架构经过了市场验证。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">能确保这么大规模的系统稳定运行。这种“经得住考验的规模化部署能力”，本身就是一道很高的信任壁垒。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">所以黄仁勋说。现在的竞争已经不是“我的芯片比你的快”，而是“我的整个AI工厂。比你的AI工厂效率更高”了。这种从“组件思维”到“系统思维”的跃迁。正是Nvidia能远超竞争对手的根本原因。毕竟。竞争对手可能能做出一款性能不错的芯片。但是要想做到“芯片、系统、软件”全链条的协同创新。还要能支撑几十万GPU的规模化部署。难度要大得多。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">黄仁勋甚至说。即使竞争对手把他们的ASIC芯片免费送给客户。客户还是应该选择Nvidia的系统。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_870.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">很多人听到这话会觉得不可思议。免费的芯片都不要？但是黄仁勋的逻辑。其实紧扣了数据中心的“现实约束”，那就是电力和空间都是有限的。黄仁勋解释说，数据中心建设中。土地、电力、建筑这些投入的成本非常高。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">当一家企业拿到宝贵的2吉瓦电力配额的时候。它的核心目标不再是“节省芯片成本”，而是“用这些电力创造最大的商业价值”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">他用简单的算术算了一笔账。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">如果每瓦性能。或者说每瓦能生成的Token数量。是竞争对手的两倍。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">那么客户用同样的数据中心。就能产生两倍的收入，谁不想要呢？</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">再具体一点。假设竞争对手的ASIC芯片。性能和Nvidia上一代的Hopper GPU差不多；。而Nvidia新一代的Blackwell GPU。性能是Hopper的30倍。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这意味着，在同样的电力消耗下。用Blackwell的客户能够获得30倍的潜在收入。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在这种情况下。为了节省一点芯片成本而放弃30倍的收入。这种机会成本显然“高得离谱”。任何理性的CFO。都会选择“每瓦性能”最高的解决方案。因为这直接决定了企业收入的上限。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_940.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">除此之外。黄仁勋还分析了ASIC的“生态定位”。他认为。ASIC适合那些“功能固定、市场规模有限”的领域。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如视频转码器、智能网卡等等。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这些任务的算法很少变。用ASIC能做到很高的效率。但是对于AI这种“工作负载多样且快速变化”的领域。ASIC的“专用性”反而成了致命弱点。因为AI需要处理的任务实在是太多了。聊天、写代码、生成图片视频、做数据分析、制定商业计划。等等等等。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而且底层算法还在不断演进。这就要求计算平台必须具备高度的“可编程性”，能够快速适配新任务、新算法。而这正是GPU和CUDA生态的核心优势。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这也能解释为什么谷歌虽然有自己的TPU。但同时也是Nvidia GPU的大客户。因为在一个复杂的计算集群里。既需要TPU这样的“专用辅助”芯片。也需要GPU这样的“通用主力”芯片。通过合理组合来实现整体最优。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_1001.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而Nvidia通过开放NVLink Fusion等接口。允许英特尔等公司的芯片接入自己的生态。这正是“平台化”和“生态化”思维的体现。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">非但不靠封闭来阻挡对手。反而是靠开放来扩大生态。让更多伙伴参与进来。一起把AI算力的市场做大。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在这场对话中。黄仁勋还花了大量篇幅讨论了全球人工智能竞赛。尤其是美国与中国的关系。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">黄仁勋首先提出了一个观点。AI基础设施已经成为和能源、通信同等重要的国家战略资源。所以全球范围内正在掀起“主权AI”（Sovereign AI）的建设浪潮。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">什么是主权AI？就是每个国家都需要拥有自己的AI基础设施。用自己的数据和文化训练AI模型。确保AI能服务于本国的特定需求。不管是工业生产、制造业升级。还是国家安全。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">黄仁勋认为。虽然各国会使用GPT、Gemini这些全球领先的模型。但是同时必须建立自己的主权AI能力。因为AI不仅是技术。还承载着文化、价值观和历史。一个国家不能把核心的智能需求。完全依赖于其他国家的技术。就像每个国家都会有自己的电网、通信网络一样。未来也会有自己的AI基础设施网络。而这为Nvidia等基础设施提供商。创造了全球性的全新市场。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_1083.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">关于美国的对华技术政策。黄仁勋提出了坦率的批评。他认为。美国采取“小院高墙”式的对华技术封锁。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如限制Nvidia向中国出口高端芯片。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">看起来是在遏制中国AI发展的做法。但是实际上不仅徒劳无功。反而更是一种危险的“单方面裁军”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">他指出了这种政策的两个主要后果。第一，会催生强大的竞争对手。把拥有95%市场份额的Nvidia排除出中国市场。相当于把整个中国市场拱手让给华为等本土企业。这些企业会在“没有强竞争”的环境下。会靠着“垄断利润”加速技术研发和产能扩张。最终成长为Nvidia在全球市场的强劲对手。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第二，严重低估了中国的能力。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">黄仁勋曾经警告说。外界普遍认为中国造不出高端芯片。或者技术上落后美国数年。这些想法都是“疯狂”的。实际上。中国拥有世界上最渴望成功、最勤奋的企业家。还有充满活力的内部竞争生态。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_1144.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">中国在芯片和AI领域和美国的技术差距。其实是以“纳秒”来计算的。不是大家想的“几年”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">当然这里的纳秒是打了引号的。意思是强调中美之间的差距很小。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">那么，正确的路径应该是什么呢？黄仁勋认为。让美国最优秀的企业在中国市场和本土企业直接竞争。才最符合美国的国家利益。这样做不仅能为美国企业创造经济价值。还能让美国通过技术影响力。在全球AI格局中保持话语权；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">更重要的是。竞争能倒逼美国科技企业不断创新。保持在技术最前沿。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">黄仁勋说道，一个自信、强大的国家。应该秉持‘放马过来’（Bring it on）的态度。相信自己的体系和人民能在竞争中胜出。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在谈到美国的核心优势时。黄仁勋的回答更是尖锐。他说。美国拥有一个世界上任何国家都没有的独特品牌声誉。那就是来到美国，实现美国梦。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">作为一个从中国台湾移民到美国。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_1204.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">从餐馆洗碗工成长为万亿市值公司CEO的亲历者。黄仁勋对“美国梦”的理解非常深刻。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这种“让每个人都有机会通过努力改变命运”的信念。是美国吸引全球顶尖人才的根本原因。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如过去几十年。全球最优秀的科学家、工程师、创业者都愿意去美国。因为那里有更好的机会、更开放的环境。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">但是黄仁勋警告说，近些年来。这个核心优势正在受到严重的挑战。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">他观察到一个非常危险的信号。顶尖中国AI研究者来美国的意愿。已经从三年前的90%骤降到现在的10%-15%。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这不是一个小变化。而是关乎美国未来的“生存危机”级别的早期预警。因为AI行业的竞争。本质上是人才的竞争，没有顶尖人才。再先进的技术也难以持续领先。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">所以黄仁勋呼吁美国政策制定者。必须谨慎区分“与中国竞争”和“对中国人强硬”这两个概念。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">“与中国竞争”是在技术、市场上的良性比拼。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_1266.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而“对中国人强硬”则是把优秀的中国人才拒之门外。这会摧毁美国最宝贵的资产。也就是美国作为全球人才灯塔的品牌形象。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">展望未来。黄仁勋认为人工智能将从根本上改变社会。他坚信。人工智能会带来巨大的生产力提升。而不是大规模的失业。那种认为AI会摧毁就业的观点。前提是“我们再也没有新的想法了”，但是他认为智力不是零和游戏。周围聪明的人和工具越多。能想到的新点子、能解决的新问题就越多。创造的岗位也会越多。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">每项工作都会改变，有些会消失。但是经济整体会增长。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在他看来。AI本身就是最伟大的均衡器。过去。一个人想利用计算机创造经济价值。至少得学习Python编程。现在，他们只需要学习人类语言。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">技术鸿沟正在被技术本身填平。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">对于未来的具体形态。他预言在未来五年内。人工智能与机器人技术的融合将成为现实。每个人都会像电影星球大战中一样。有自己的“R2-D2”机器人。成为生活中的伙伴和向导。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_1334.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">云端的人工智能和实体世界的机器人将无处不在。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">生物学的复杂性将被揭示。每个人都将拥有自己的“数字孪生”，用于预测健康状况和疾病。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">面对这种指数级加速的变化。黄仁勋给出的建议很简单。那就是登上那列火车。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">不要试图去预测火车未来会到哪个站点。因为当它呈指数级加速的时候。任何预测都是徒劳的。唯一的策略就是趁现在它还相对较慢时跳上去。然后随着它一起经历指数级的旅程。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">也许，从芯片公司到AI基础设施公司。英伟达的进化本身就是登上这列火车的最好证明。而对于整个世界来说。这趟旅程才刚刚开始。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">好了，感谢观看本期视频。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">我们下期再见。</p>

<hr style="clear:both" />
=============
<p><a href="https://www.youtube.com/watch?v=--HbDQk2-jA">https://www.youtube.com/watch?v=--HbDQk2-jA</a></p><p>9月26日，知名播客BG2和英伟达的CEO黄仁勋进行了一场独家对话。这次访谈的标题叫做“NVIDIA：OpenAI、计算的未来和美国梦”，时长超过100分钟，堪称黄仁勋近期信息密度最高、干货最足的一次分享。在这场对话中，黄仁勋系统性地解释了华尔街与硅谷之间存在的巨大认知分歧，并且详细拆解了英伟达看似坚不可摧的商业护城河，以及他对全球人工智能竞赛、大国博弈和未来社会形态的完整思考，今天我们就来回顾一下这场访谈的内容。</p><p><a href="https://youtu.be/pE6sw_E9Gh0?si=L41UzPmgS_lP5i43">https://youtu.be/pE6sw_E9Gh0?si=L41UzPmgS_lP5i43</a></p>]]></content:encoded>
      <itunes:image href="https://i.ytimg.com/vi/--HbDQk2-jA/hqdefault.jpg"/>
      <pubDate>2025-10-05T09:22:21.000Z</pubDate>
    </item><item>
      <title><![CDATA[【人工智能】Lora无悔 | Thinking Machines最新研究 | John Schulman | 全量微调 | 什么时候可以放心用Lora | MLP层 | 学习率LR | eNTK理论]]></title>
      <link>https://www.youtube.com/watch?v=MlqXL6o--2M</link>
      <itunes:title><![CDATA[【人工智能】Lora无悔 | Thinking Machines最新研究 | John Schulman | 全量微调 | 什么时候可以放心用Lora | MLP层 | 学习率LR | eNTK理论]]></itunes:title>
      <itunes:author><![CDATA[最佳拍档]]></itunes:author>
      <itunes:summary>
        <![CDATA[<hr style="clear:both" />

<p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://img.youtube.com/vi/MlqXL6o--2M/maxresdefault.jpg" /></p><p><a href="https://www.youtube.com/watch?v=MlqXL6o--2M">https://www.youtube.com/watch?v=MlqXL6o--2M</a></p><h1>值得閱讀的理由</h1> <ul> <li>深入理解<strong>LoRA</strong>與<strong>全量微調（FullFT）</strong>在資源消耗與性能表現之間的平衡，為大模型微調提供決策依據。</li> <li>掌握<strong>約翰·舒爾曼（John Schulman）</strong>團隊提出的關鍵條件和最佳實踐，有效提升LoRA的微調效果。</li> <li>探索LoRA在監督學習與強化學習場景下的應用差異，以及未來大模型微調研究的開放性問題。</li> </ul> <hr /> <h1>摘要</h1> <p>本影片由影片的講者深入探討了當今大模型微調領域中一個核心問題：究竟該選擇<strong>低秩適應（LoRA）</strong>還是<strong>全量微調（FullFT）</strong>？這個問題的本質在於<strong>「資源」</strong>與<strong>「效果」</strong>的權衡。影片指出，儘管全量微調能最大化模型性能，但其對硬體資源的龐大需求（如儲存原始權重、梯度、優化器動量，且需使用FP32精度）使得一般團隊難以承受。相較之下，LoRA作為一種參數高效微調的主流方法，以其節省記憶體和算力的優勢吸引了廣泛關注，但其性能能否媲美全量微調一直是業界普遍的疑慮。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_90.jpg" /></p> <p>影片接著介紹了由<strong>約翰·舒爾曼（John Schulman）及其團隊</strong>發表的深度分析文章《LoRA無悔（LoRA Without Regret）》，該文章不僅回應了LoRA能否匹敵全量微調的疑問，更提出了具體的操作條件和超參數設定建議。影片強調，理解LoRA的重要性源於當前主流大型語言模型（如Llama 3、Qwen3）龐大的參數規模和預訓練資料量。在模型的「訓練後處理」階段，由於資料集規模變小且關注領域變窄，全量微調顯得過於浪費，因此催生了<strong>參數高效微調（PEFT）</strong>的發展，而LoRA正是其中最成熟的方法。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_180.jpg" /></p> <h2>LoRA的核心機制與優勢</h2> <p>影片詳細解釋了LoRA的核心思路：它不直接修改原模型的權重矩陣W，而是添加一個「低秩增量項」<strong>γBA</strong>。這裡的B和A是兩個低秩矩陣，它們的參數數量遠少於原始權重矩陣W，卻能「模擬」出原模型權重的更新效果。這種機制不僅實現了<strong>參數高效性</strong>，還帶來了多項實際優勢。首先是<strong>「多租戶服務」</strong>，由於LoRA只訓練輕量級的「適配器」，一個推理伺服器能夠同時載入多個適配器以處理不同任務，顯著節省硬體成本。其次是<strong>「訓練佈局優化」</strong>，LoRA更新的參數少，記憶體佔用小，使得訓練所需的硬體資源大幅降低，普通團隊也能用少量GPU進行訓練。最後是<strong>「載入傳輸方便」</strong>，LoRA適配器檔案體積小（僅幾MB到幾十MB），便於機器間傳輸和模型載入，提高了營運彈性。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_300.jpg" /></p> <h2>關鍵實驗發現與「低遺憾區間」</h2> <p>影片闡述了約翰·舒爾曼團隊實驗的核心目標：回答LoRA能否匹配全量微調的性能，以及需要滿足哪些條件。他們採取的實驗設計具有兩個顯著特點：一是<strong>「不聚焦特定任務，而是研究通用關係」</strong>，探討「訓練集大小」與「LoRA參數數量」之間的通用規律；二是採用<strong>「對數損失（log loss）評估」</strong>，能客觀反映模型的預測誤差和訓練表現。透過嚴謹的實驗設計，團隊得出了五個關鍵發現：</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_450.jpg" /></p> <p>1.  在中小規模的指令微調或推理資料集上，只要不超過LoRA的容量，<strong>LoRA與全量微調的性能幾乎完全一致</strong>。 2.  若資料集規模超出LoRA容量，LoRA會表現較差，但並非無法下降，而是<strong>「訓練效率變慢」</strong>。 3.  LoRA對<strong>「批量大小（batch size）」</strong>的容忍度低於全量微調，批次過大會導致LoRA性能明顯下降，且增大LoRA的秩也無法解決此問題。 4.  LoRA必須應用於模型<strong>「所有層」</strong>，尤其是<strong>MLP層或MoE層</strong>，效果才最佳；僅在注意力層使用LoRA的性能表現不佳。 5.  在<strong>強化學習（RL）</strong>場景中，即使使用<strong>極低秩的LoRA</strong>也能與全量微調性能一致，這歸因於強化學習每輪所需的資訊量較少（O(1)比特）。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_705.jpg" /></p> <p>基於這些發現，研究團隊提出了<strong>「低遺憾區間（low-regret regime）」</strong>的概念：只要LoRA的參數容量能覆蓋資料集所需的資訊，並且應用到所有層，就能與全量微調性能相當。這意味著在大多數企業的訓練後處理場景中，LoRA完全可以替代全量微調。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_750.jpg" /></p> <h2>實驗細節剖析</h2> <p>影片進一步深入探討了約翰·舒爾曼團隊的實驗細節，為實踐者提供了寶貴的參考。他們測試了從<strong>秩1到512</strong>的三個數量級LoRA秩，並對每個實驗條件進行了<strong>「學習率掃描」</strong>以找到最佳值，同時採用<strong>「恆定學習率」</strong>排除調度干擾。實驗模型涵蓋了<strong>Llama 3系列</strong>和<strong>Qwen3系列</strong>，以及<strong>混合專家（MoE）模型</strong>。資料集則選用了監督學習的<strong>Tulu3</strong>和<strong>OpenThoughts3</strong>，以及強化學習的<strong>MATH</strong>和<strong>GSM8K</strong>，確保了實驗的廣泛性和代表性。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_880.jpg" /></p> <p>關於<strong>LoRA秩的影響</strong>，高秩LoRA（如256、512）與全量微調的學習曲線幾乎重合，損失呈現「隨步驟的對數線性下降」；低秩LoRA（如1、4）在訓練初期尚可，但達到閾值後便會因「容量耗盡」而偏離最佳損失曲線。在<strong>學習率</strong>方面，研究發現LoRA的最佳學習率約為全量微調的10倍，且不同秩的LoRA之間最佳學習率差異不大，這為設定LoRA學習率提供了便利。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_990.jpg" /></p> <p><strong>批量大小效應</strong>實驗揭示，當批量大小較小（如32）時，LoRA與全量微調性能差距微小；但當批量大小增大（如256、512）時，LoRA的損失會顯著高於全量微調，且這種差距與LoRA的秩無關。這歸因於LoRA的「矩陣乘積參數化（BA）」與原權重矩陣（W）的優化動態不同，大批量會放大這種差異。幸運的是，在實際應用中，只要避免使用過大的批量，此問題便可緩解。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_1070.jpg" /></p> <p>影片特別強調了<strong>LoRA應該應用到哪些層</strong>的實驗結果。研究團隊通過三組對比實驗（僅注意力層、僅MLP層、所有層）發現，僅在注意力層使用LoRA的性能最差，而<strong>MLP層才是LoRA發揮作用的核心</strong>；將LoRA應用於所有層與僅MLP層的性能幾乎一致。這個發現與<strong>經驗神經正切核（eNTK）理論</strong>相符：參數越多的層對核函數影響越大，應用於所有層能使LoRA的eNTK與全量微調的eNTK保持一致，從而實現相似的學習動態。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_20.jpg" /></p> <h2>結論與深層意義</h2> <p>影片總結了該研究的深層意義。首先，LoRA匹配全量微調的核心條件有二：一是<strong>必須應用到所有層（尤其MLP/MoE層）</strong>，確保eNTK的一致性；二是<strong>LoRA的容量需能覆蓋資料集所需的資訊</strong>。其次，<strong>監督學習與強化學習對LoRA容量的需求存在顯著差異</strong>：監督學習需要更多資訊，適合高秩LoRA；強化學習每輪所需資訊少，低秩LoRA便足夠。這能幫助開發者在強化學習任務中節省資源，無需追求高秩。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_22.jpg" /></p> <p>研究還指出LoRA的<strong>計算效率優勢</strong>，每輪訓練的FLOPs約為全量微調的2/3，意味著在相同硬體下LoRA能多訓練50%的樣本。儘管如此，研究也提出了一些<strong>「開放問題」</strong>，例如如何更精準地預測LoRA性能、LoRA最佳學習率是全量微調10倍的理論解釋、LoRA變體的表現，以及LoE層LoRA如何與張量並行等技術結合以適配更大模型等，這些都是未來值得關注的研究方向。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_24.jpg" /></p> <p>最後，約翰·舒爾曼團隊的研究目標不僅是為了節省資源，更是為了<strong>「讓大模型的微調能力更易獲取」</strong>，使普通團隊也能利用大模型解決具體問題。透過對LoRA的深入研究，他們也深化了對「模型容量」、「資料集複雜度」、「樣本效率」等基礎問題的理解，展現了AI研究中實用技術背後蘊藏的核心原理新認知。</p> <hr /><hr />================================================</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">大家好，这里是最佳拍档，我是大飞。现在微调大模型。到底应该用LoRA还是全量微调呢？其实这个问题的核心。本质是“资源”和“效果”的平衡。全量微调（FullFT）虽然能把模型性能拉满。但是太“吃资源”了。一个万亿参数的模型。全量微调时不仅要存原权重。还要存梯度、优化器动量。而且这些数据得用FP32精度。比推理时的BF16多占一倍空间。普通团队根本扛不住硬件成本。而LoRA作为参数高效微调的主流方法。确实省内存、省算力。但是大家总是感觉没底。它的效果到底能不能跟全量微调比？会不会调了半天，性能差一截呢？而Thinking Machines Lab最近发布的一篇博客。恰恰把这个问题给讲透了。文章的标题是《LoRA无悔（LoRA Without Regret）》，作者是约翰·舒尔曼（John Schulman）和他的团队。文章不仅回答了“LoRA能不能媲美全量微调”的问题。还给出了具体的操作条件。甚至连超参数怎么设都讲清楚了。不管你是想入门LoRA。还是已经在落地中遇到问题。这篇内容都值得你仔细阅读。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_75.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先，我们得先搞清楚。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">为什么现在大家这么需要LoRA？这还得从大模型的现状说起。现在主流的语言模型。比如Llama 3、Qwen3。参数都在上万亿。预训练时用了数万亿个Token。这么大的规模。就是为了让模型学到人类书面知识里的所有模式。基础性能才能上去。但是到了“训练后处理”阶段。情况就发生了变化。不仅数据集变小了。关注的领域也变窄了。这时候如果还用全量微调。这就像用一辆卡车去拉一颗鸡蛋。太浪费了。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">就是因为这个“浪费”的问题。参数高效微调（PEFT）才发展起来。而低秩适应LoRA就是其中最成熟的方法。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">LoRA的核心思路很巧妙。它不直接改原模型的权重矩阵W。而是给W加了一个“低秩增量项”，公式是W' = W + γBA。这里的W'是微调后的权重。B和A是两个低秩矩阵。它们加起来的参数数量。比原矩阵W少得多。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_141.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">γ是个常数缩放因子。用来调整增量项的大小。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">简单说，LoRA就是用两个小矩阵。“模拟”出原模型权重的更新效果。这样既不用动原模型的海量参数。又能让模型适配新任务。这就是它“参数高效”的关键。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而且LoRA不止省参数。还有三个很实际的优势。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第一个是“多租户服务（Multi-tenant serving）”，因为LoRA只训练A和B这两个“适配器”，原模型权重不动。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">所以一个推理服务器可以在内存里存多个适配器。对应不同的任务。比如一个适配器处理客服对话。另一个处理产品摘要生成。还能批量处理这些请求。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">像陈（Chen）等人2023年提出的Punica框架。还有现在常用的vLLM、SGLang这些推理引擎。都已经支持这个功能。对企业来说能省不少硬件钱。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第二个优势是“训练布局优化”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">全量微调的时候。硬件布局得专门设计。你想，除了原权重。还要存梯度、优化器动量，精度还高。需要的加速器数量往往是推理时的10倍以上。但LoRA要更新的参数少。占用内存也少。训练时的硬件布局只比推理时稍微大一点。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_220.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">普通团队用几台GPU就能跑。门槛低多了。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第三个优势是“加载传输方便”，适配器矩阵的参数少。文件体积小。比如一个秩为128的LoRA适配器。可能就几MB或几十MB。不管是在机器间传。还是加载到模型里。都比传整个模型快得多，运维也灵活。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">正是这些优势。LoRA从2021年胡（Hu）等人发表的第一篇论文（《LoRA:。Low-Rank Adaptation of Large Language Models》）后。就越来越火。但是问题也来了，现有的研究里。LoRA和全量微调的性能对比一直不明确。大家公认的是。在“类似预训练”的场景里。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如数据集特别大。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">超过LoRA的参数存储极限的时候。LoRA性能会差一些；。但是在训练后处理的常见数据集规模下。LoRA虽然容量足够存下关键信息。可是没人能保证它的“样本效率”和“计算效率”，能跟全量微调相比。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">所以约翰·舒尔曼团队的实验。核心就是回答了。LoRA到底能不能匹配全量微调的性能？如果能，需要满足哪些条件呢？</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_289.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">他们的实验结论很明确。只要把几个关键细节做对。LoRA的样本效率和最终性能。就能和全量微调完全一致。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">那这些“关键细节”到底是什么？我们得先从他们的实验设计说起。因为好的实验设计。是结论可信的基础。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">和之前的LoRA研究相比。他们的实验有两个很重要的特点。第一个是“不盯着具体任务。而是研究通用关系”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">很多研究只会选一两个数据集。比如只测情感分析。或者只测文本摘要，但是他们不一样。专门研究“训练集大小”和“LoRA参数数量”之间的通用规律。这样得出的结论。不管你是做医疗还是教育任务。都能参考。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第二个是采用了“对数损失（log loss）评估”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">之前很多研究采用的是“基于采样的评估”，比如让模型生成文本，再人工打分。但是这种方法主观性强。不同任务里标准还不一样。而对数损失能直接反映模型的预测误差。而且能够清晰地看到不同训练步骤、不同参数设置下的模型表现规律。通用性强得多。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_357.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">就是靠着这样的实验设计。他们得出了五个关键发现。这些发现也是LoRA能够媲美全量微调的核心条件。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第一个发现。在中小规模的指令微调或者推理数据集上。LoRA和全量微调的性能完全一样。比如他们用的Tulu3和OpenThoughts3数据集。只要规模没超过LoRA的容量。两者的损失曲线、收敛速度都几乎重合。这说明在大多数企业的落地场景里。LoRA完全够用。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第二个发现。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">如果数据集规模超过了LoRA的容量。LoRA就会比全量微调差。但不是“损失降到一定程度就降不下去”了。而是“训练效率变慢”了。具体会慢多少。要取决于LoRA的参数容量和数据集大小的比例。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如一个秩为32的LoRA。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">处理100万Token的数据集没问题。但是如果数据集涨到1亿个Token。它的训练效率就会明显下降。损失下降速度变慢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_419.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第三个发现。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">LoRA对“批量大小（batch size）”的容忍度比全量微调低。如果批量 size 太大。超过某个阈值后。LoRA的性能损失就会比全量微调大得多。而且这个问题。就算增大LoRA的秩也解决不了。因为这是LoRA“矩阵乘积参数化（BA）”的固有属性。它的优化动态和原权重矩阵（W）不一样。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">批量一大，这种动态差异就会被放大。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第四个发现。LoRA一定要应用到模型的所有层。尤其是MLP层或MoE层，效果才好。之前很多研究受到第一篇LoRA论文的影响。只把LoRA用到注意力层。但是约翰·舒尔曼团队发现。仅在注意力层使用LoRA的性能很差。而MLP层才是LoRA发挥作用的关键。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比德曼（Biderman）等人2024年的研究（《LoRA Learns Less and Forgets Less》）也得出了类似结论。进一步验证了这个发现。第五个发现，在强化学习的场景里。就算用很低秩的LoRA。也能和全量微调性能一致。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_482.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">他们用数学推理任务做实验。比如让模型去解初中数学题。以答案正确性为奖励，结果发现。哪怕LoRA的秩只有1。最终的准确率也和全量微调一样。这背后有个信息论的解释。那就是监督学习里。每一轮训练能给模型提供“O(Token数量)”个比特的信息。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如一句话有100个Token。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">就能提供100比特左右的信息；。但是强化学习不一样。政策梯度算法的学习。靠的是“优势函数（Advantage Function）”，每一轮只能提供“O(1)”比特的信息。简单来说。就是强化学习需要学的信息少。就算是低秩LoRA，容量也完全够。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">基于这些发现。他们提出了一个“低遗憾区间（low-regret regime）”的概念。只要LoRA的参数容量能覆盖数据集的信息需求。并且应用到所有层。就能和全量微调性能相当。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而这个区间。刚好覆盖了大多数企业的训练后处理场景。这也意味着，LoRA在很多实际应用里。完全可以替代全量微调。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_547.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">当然，光有结论不够。我们还得知道这些结论是怎么来的。也就是他们的实验细节。这能帮我们在自己做实验的时候少走弯路。我们挑几个部分来重点讲一下。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先，他们的实验设置非常严谨。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先是LoRA的秩。覆盖了1到512三个数量级。从最低的秩1。到能覆盖大部分场景的秩128、256。再到接近全量微调的秩512。这样能全面看到秩对性能的影响。然后是学习率（LR）。为了避免“因为学习率选得不好。导致LoRA性能差”的情况出现。他们对每个实验条件都做了“学习率扫描”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如对秩32的LoRA。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">测试0.0001到0.01的不同学习率。找到最佳值；。而且采用了“恒定学习率”，没有热身或冷却阶段。这样能够排除学习率调度的干扰。更准确对比LoRA和全量微调的差异。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_608.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">模型方面，他们用了两类主流模型。一类是Llama 3系列。另一类是Qwen3系列。还包括了混合专家（MoE）模型。因为MoE模型的激活效率高。现在很多大模型都用这种架构。所以加入MoE的实验，能让结论更全面。数据集方面。监督学习用了Tulu3和OpenThoughts3。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">Tulu3是艾维森（Ivison）等人在2024年提出的。侧重“指令遵循”，比如让模型根据用户指令生成邮件、写代码；。OpenThoughts3是古哈（Guha）等人在2025年提出的。侧重“推理”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如数学题、逻辑推理题。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这两个数据集在任务类型、数据结构上差异很大。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">强化学习实验则用了MATH数据集和GSM8K数据集。都是数学推理任务。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">以答案正确与否作为奖励信号。以便专注测试强化学习场景下的LoRA性能。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">先看“LoRA秩”的影响实验。他们在Tulu3和OpenThoughts3上训练了一个epoch。对每个数据集、每个模型大小。都扫描了不同的LoRA秩和学习率。结果发现，高秩LoRA，比如256、512。和全量微调的学习曲线几乎重合。损失都是“随步骤的对数线性下降”，也就是说。每训练10倍的步骤。损失就会按固定比例下降。这和全量微调的规律完全一样。而低秩LoRA，比如1、4。在训练初期还能跟上。但是到了某个步骤阈值后。就会“偏离最小损失曲线”。这就是之前说的“容量耗尽”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_707.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">低秩矩阵存不下更多的任务信息。所以学习速度变慢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">更重要的是“学习率”的发现。他们绘制了“学习率和最终损失”的曲线。发现高秩LoRA的最小损失和全量微调几乎一样。但是最佳学习率是全量微调的10倍。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如全量微调的最佳学习率是0.001。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">LoRA就需要0.01。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比德曼（Biderman）等人2024年的研究也发现了类似的10倍比例。这说明这个规律不是偶然的。而是LoRA的固有属性。而且不同秩的LoRA。最佳学习率很接近。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如秩4和秩512的最佳学习率差异不到2倍。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这为我们设置LoRA学习率提供了很大便利。不用频繁调整。按全量微调的10倍开始试就行。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">再看“批量大小效应”的实验。他们用了OpenThoughts3里一个1万样本的子集。测试不同批量大小的影响。结果很明显，当批量大小比较小。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_768.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如32的时候。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">LoRA和全量微调的性能差距很小。而且随着训练推进，差距还会缩小；。但当批量大小变大。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如256、512的时候。LoRA的损失会明显比全量微调高。而且这个差距会一直存在。不会随训练步骤消失。更关键的是。这个差距和LoRA的秩无关。就算用秩512的LoRA。大批次下还是比全量微调差。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">他们分析。这是因为LoRA的“矩阵乘积（BA）”参数化。和原权重矩阵（W）的优化动态不同。批量一大。原矩阵能更好地利用批次信息更新。而BA矩阵的更新效率会下降。导致性能差距。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">不过好在，不管是LoRA还是全量微调。都是“小批量下性能更好”，所以实际应用里。只要别用太大的批量。这个问题就能够缓解。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">接下来。我们来重点说一说跟前面第4点发现。“LoRA该应用到哪些层”有关的实验过程。这是很多人调参时容易踩坑的地方。之前很多人习惯只把LoRA用到注意力层。觉得注意力层负责“理解上下文”，所以是关键，但是实验结果刚好相反。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_838.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">约翰·舒尔曼团队做了三组对比实验。第一组只在注意力层用LoRA。第二组只在MLP层用LoRA。第三组在所有层。也就是注意力+MLP+MoE都用了LoRA。结果发现。仅注意力层的LoRA性能最差。就算用更高的秩。参数数量和仅MLP层的秩128差不多。性能还是差一截。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如在Llama-3.1-8B模型上，仅MLP层。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">秩128，0.24B参数的损失。比仅注意力层。秩256，0.25B参数低了0.15左右。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">要知道，在语言模型里。损失降低0.1已经是很明显的提升了。而“所有层用LoRA”和“仅MLP层用LoRA”的性能几乎一样。这说明MLP层才是LoRA发挥作用的核心。注意力层加不加LoRA。对最终性能影响不大。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">他们还在MoE模型上做了实验。对Qwen3-30B-A3B-Base的每个专家都单独训练LoRA。秩设为“总秩除以活跃专家数量”，这样能保证MoE层的LoRA参数比例和其他层一致。结果和稠密模型一样。仅MLP层的LoRA优于仅注意力层。所有层和仅MLP层相当。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_918.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">为什么会这样？这就要用到马拉迪（Malladi）等人在2022年提出的“经验神经正切核（eNTK）”理论了。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">eNTK是用来描述模型微调初期学习动态的工具。它的核心是“梯度的点积”。比如对每个Token的预测。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">梯度g_i是损失对参数的偏导。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">eNTK的核函数K(i。j)就是g_i和g_j的点积。这个核函数决定了模型怎么“学习”数据里的模式。而参数越多的层。对核函数的影响越大。马拉迪等人的研究指出。当LoRA应用到所有层的时候。它的eNTK和全量微调的eNTK几乎一样。所以学习动态也相似；。但是如果只应用到注意力层。而忽略了参数更多的MLP层。eNTK就会和全量微调有明显差异。学习速度自然慢下来。这也解释了为什么仅注意力层的LoRA效果差。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">聊完实验。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">我们最后再来总结一下这篇研究的主要结论和背后的深层意义。它不止告诉了我们LoRA怎么用。还帮我们深化了对大模型微调的理解。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_986.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先是“LoRA匹配全量微调的两个核心条件”，一是要应用到所有层。尤其是MLP/MoE层。这样才能保证eNTK和全量微调的一致；。二是LoRA的容量要能覆盖数据集的信息需求。也就是“非容量约束”。只要满足这两个条件。LoRA就能和全量微调性能相当。这为我们选择微调方法提供了明确的判断标准。如果你的数据集是中小规模。模型有MLP层。就用LoRA；。如果数据集特别大。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如接近预训练规模。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">再考虑全量微调。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">其次是“监督学习和RL的容量需求的差异”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">监督学习需要的信息多。每个token 1比特。所以需要更高秩的LoRA；。强化学习需要的信息少，每轮次1比特。低秩LoRA就够。这个差异能帮我们节省资源。也就是做强化学习任务时。不用追求高秩。秩1、8就够，参数更少，训练更快。然后是“LoRA的计算效率优势”，他们算了一笔账。LoRA每轮训练的FLOPs。是全量微调的2/3左右。具体怎么算的大家可以去看一下这一段。这里我们就不一一推导了。简单来说，对于一个N×N的权重矩阵W。LoRA的总FLOPs是2N²。全量微调是3N²，所以比例就是2/3。这意味着，同样的硬件。LoRA能比全量微调多训练50%的样本。计算效率更高。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_1079.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">当然，研究也提出了一些“开放问题”，这些也是未来值得关注的方向。比如怎么更精准地预测LoRA的性能。不用每次都做实验；。为什么LoRA的最佳学习率是全量微调的10倍。目前还没有完整的理论解释；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">LoRA的变体在这种评估方法下表现如何；。还有MoE层的LoRA。怎么和张量并行、专家并行这些技术结合。适配更大的模型，等等等等。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">最后，约翰·舒尔曼团队在结语里提到。他们研究LoRA的目标。不只是为了省资源。更是为了“让大模型的微调能力更易获取”，毕竟不是每个团队都有能力做全量微调。但是LoRA能让普通团队也能用大模型来解决具体的问题。而且通过研究LoRA。他们也深化了对“模型容量”、“数据集复杂度”、“样本效率”这些基础问题的理解。这其实也是AI研究的魅力。一个实用的技术。背后往往藏着对核心原理的新认知。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_1140.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">好了，今天的内容就到这里。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">如果你在做LoRA相关的工作。希望今天的拆解能帮你少走弯路；。如果你只是感兴趣。也希望能够帮助你增加对LoRA的了解。感谢大家观看本期视频。我们下期再见。</p>

<hr style="clear:both" />
=============
<p><a href="https://www.youtube.com/watch?v=MlqXL6o--2M">https://www.youtube.com/watch?v=MlqXL6o--2M</a></p><p>Thinking Machines Lab最近发布的一篇博客，文章的标题是《LoRA无悔（LoRA Without Regret）》，作者是约翰·舒尔曼（John Schulman）和他的团队。文章不仅回答了“LoRA能不能媲美全量微调”的问题，还给出了具体的操作条件，甚至连超参数怎么设都讲清楚了。不管你是想入门LoRA，还是已经在落地中遇到问题，这篇内容都值得你仔细阅读。</p><p><a href="https://thinkingmachines.ai/blog/lora/">https://thinkingmachines.ai/blog/lora/</a></p>]]>
      </itunes:summary>
      <description>
        <![CDATA[<hr style="clear:both" />

<p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://img.youtube.com/vi/MlqXL6o--2M/maxresdefault.jpg" /></p><p><a href="https://www.youtube.com/watch?v=MlqXL6o--2M">https://www.youtube.com/watch?v=MlqXL6o--2M</a></p><h1>值得閱讀的理由</h1> <ul> <li>深入理解<strong>LoRA</strong>與<strong>全量微調（FullFT）</strong>在資源消耗與性能表現之間的平衡，為大模型微調提供決策依據。</li> <li>掌握<strong>約翰·舒爾曼（John Schulman）</strong>團隊提出的關鍵條件和最佳實踐，有效提升LoRA的微調效果。</li> <li>探索LoRA在監督學習與強化學習場景下的應用差異，以及未來大模型微調研究的開放性問題。</li> </ul> <hr /> <h1>摘要</h1> <p>本影片由影片的講者深入探討了當今大模型微調領域中一個核心問題：究竟該選擇<strong>低秩適應（LoRA）</strong>還是<strong>全量微調（FullFT）</strong>？這個問題的本質在於<strong>「資源」</strong>與<strong>「效果」</strong>的權衡。影片指出，儘管全量微調能最大化模型性能，但其對硬體資源的龐大需求（如儲存原始權重、梯度、優化器動量，且需使用FP32精度）使得一般團隊難以承受。相較之下，LoRA作為一種參數高效微調的主流方法，以其節省記憶體和算力的優勢吸引了廣泛關注，但其性能能否媲美全量微調一直是業界普遍的疑慮。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_90.jpg" /></p> <p>影片接著介紹了由<strong>約翰·舒爾曼（John Schulman）及其團隊</strong>發表的深度分析文章《LoRA無悔（LoRA Without Regret）》，該文章不僅回應了LoRA能否匹敵全量微調的疑問，更提出了具體的操作條件和超參數設定建議。影片強調，理解LoRA的重要性源於當前主流大型語言模型（如Llama 3、Qwen3）龐大的參數規模和預訓練資料量。在模型的「訓練後處理」階段，由於資料集規模變小且關注領域變窄，全量微調顯得過於浪費，因此催生了<strong>參數高效微調（PEFT）</strong>的發展，而LoRA正是其中最成熟的方法。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_180.jpg" /></p> <h2>LoRA的核心機制與優勢</h2> <p>影片詳細解釋了LoRA的核心思路：它不直接修改原模型的權重矩陣W，而是添加一個「低秩增量項」<strong>γBA</strong>。這裡的B和A是兩個低秩矩陣，它們的參數數量遠少於原始權重矩陣W，卻能「模擬」出原模型權重的更新效果。這種機制不僅實現了<strong>參數高效性</strong>，還帶來了多項實際優勢。首先是<strong>「多租戶服務」</strong>，由於LoRA只訓練輕量級的「適配器」，一個推理伺服器能夠同時載入多個適配器以處理不同任務，顯著節省硬體成本。其次是<strong>「訓練佈局優化」</strong>，LoRA更新的參數少，記憶體佔用小，使得訓練所需的硬體資源大幅降低，普通團隊也能用少量GPU進行訓練。最後是<strong>「載入傳輸方便」</strong>，LoRA適配器檔案體積小（僅幾MB到幾十MB），便於機器間傳輸和模型載入，提高了營運彈性。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_300.jpg" /></p> <h2>關鍵實驗發現與「低遺憾區間」</h2> <p>影片闡述了約翰·舒爾曼團隊實驗的核心目標：回答LoRA能否匹配全量微調的性能，以及需要滿足哪些條件。他們採取的實驗設計具有兩個顯著特點：一是<strong>「不聚焦特定任務，而是研究通用關係」</strong>，探討「訓練集大小」與「LoRA參數數量」之間的通用規律；二是採用<strong>「對數損失（log loss）評估」</strong>，能客觀反映模型的預測誤差和訓練表現。透過嚴謹的實驗設計，團隊得出了五個關鍵發現：</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_450.jpg" /></p> <p>1.  在中小規模的指令微調或推理資料集上，只要不超過LoRA的容量，<strong>LoRA與全量微調的性能幾乎完全一致</strong>。 2.  若資料集規模超出LoRA容量，LoRA會表現較差，但並非無法下降，而是<strong>「訓練效率變慢」</strong>。 3.  LoRA對<strong>「批量大小（batch size）」</strong>的容忍度低於全量微調，批次過大會導致LoRA性能明顯下降，且增大LoRA的秩也無法解決此問題。 4.  LoRA必須應用於模型<strong>「所有層」</strong>，尤其是<strong>MLP層或MoE層</strong>，效果才最佳；僅在注意力層使用LoRA的性能表現不佳。 5.  在<strong>強化學習（RL）</strong>場景中，即使使用<strong>極低秩的LoRA</strong>也能與全量微調性能一致，這歸因於強化學習每輪所需的資訊量較少（O(1)比特）。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_705.jpg" /></p> <p>基於這些發現，研究團隊提出了<strong>「低遺憾區間（low-regret regime）」</strong>的概念：只要LoRA的參數容量能覆蓋資料集所需的資訊，並且應用到所有層，就能與全量微調性能相當。這意味著在大多數企業的訓練後處理場景中，LoRA完全可以替代全量微調。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_750.jpg" /></p> <h2>實驗細節剖析</h2> <p>影片進一步深入探討了約翰·舒爾曼團隊的實驗細節，為實踐者提供了寶貴的參考。他們測試了從<strong>秩1到512</strong>的三個數量級LoRA秩，並對每個實驗條件進行了<strong>「學習率掃描」</strong>以找到最佳值，同時採用<strong>「恆定學習率」</strong>排除調度干擾。實驗模型涵蓋了<strong>Llama 3系列</strong>和<strong>Qwen3系列</strong>，以及<strong>混合專家（MoE）模型</strong>。資料集則選用了監督學習的<strong>Tulu3</strong>和<strong>OpenThoughts3</strong>，以及強化學習的<strong>MATH</strong>和<strong>GSM8K</strong>，確保了實驗的廣泛性和代表性。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_880.jpg" /></p> <p>關於<strong>LoRA秩的影響</strong>，高秩LoRA（如256、512）與全量微調的學習曲線幾乎重合，損失呈現「隨步驟的對數線性下降」；低秩LoRA（如1、4）在訓練初期尚可，但達到閾值後便會因「容量耗盡」而偏離最佳損失曲線。在<strong>學習率</strong>方面，研究發現LoRA的最佳學習率約為全量微調的10倍，且不同秩的LoRA之間最佳學習率差異不大，這為設定LoRA學習率提供了便利。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_990.jpg" /></p> <p><strong>批量大小效應</strong>實驗揭示，當批量大小較小（如32）時，LoRA與全量微調性能差距微小；但當批量大小增大（如256、512）時，LoRA的損失會顯著高於全量微調，且這種差距與LoRA的秩無關。這歸因於LoRA的「矩陣乘積參數化（BA）」與原權重矩陣（W）的優化動態不同，大批量會放大這種差異。幸運的是，在實際應用中，只要避免使用過大的批量，此問題便可緩解。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_1070.jpg" /></p> <p>影片特別強調了<strong>LoRA應該應用到哪些層</strong>的實驗結果。研究團隊通過三組對比實驗（僅注意力層、僅MLP層、所有層）發現，僅在注意力層使用LoRA的性能最差，而<strong>MLP層才是LoRA發揮作用的核心</strong>；將LoRA應用於所有層與僅MLP層的性能幾乎一致。這個發現與<strong>經驗神經正切核（eNTK）理論</strong>相符：參數越多的層對核函數影響越大，應用於所有層能使LoRA的eNTK與全量微調的eNTK保持一致，從而實現相似的學習動態。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_20.jpg" /></p> <h2>結論與深層意義</h2> <p>影片總結了該研究的深層意義。首先，LoRA匹配全量微調的核心條件有二：一是<strong>必須應用到所有層（尤其MLP/MoE層）</strong>，確保eNTK的一致性；二是<strong>LoRA的容量需能覆蓋資料集所需的資訊</strong>。其次，<strong>監督學習與強化學習對LoRA容量的需求存在顯著差異</strong>：監督學習需要更多資訊，適合高秩LoRA；強化學習每輪所需資訊少，低秩LoRA便足夠。這能幫助開發者在強化學習任務中節省資源，無需追求高秩。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_22.jpg" /></p> <p>研究還指出LoRA的<strong>計算效率優勢</strong>，每輪訓練的FLOPs約為全量微調的2/3，意味著在相同硬體下LoRA能多訓練50%的樣本。儘管如此，研究也提出了一些<strong>「開放問題」</strong>，例如如何更精準地預測LoRA性能、LoRA最佳學習率是全量微調10倍的理論解釋、LoRA變體的表現，以及LoE層LoRA如何與張量並行等技術結合以適配更大模型等，這些都是未來值得關注的研究方向。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_24.jpg" /></p> <p>最後，約翰·舒爾曼團隊的研究目標不僅是為了節省資源，更是為了<strong>「讓大模型的微調能力更易獲取」</strong>，使普通團隊也能利用大模型解決具體問題。透過對LoRA的深入研究，他們也深化了對「模型容量」、「資料集複雜度」、「樣本效率」等基礎問題的理解，展現了AI研究中實用技術背後蘊藏的核心原理新認知。</p> <hr /><hr />================================================</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">大家好，这里是最佳拍档，我是大飞。现在微调大模型。到底应该用LoRA还是全量微调呢？其实这个问题的核心。本质是“资源”和“效果”的平衡。全量微调（FullFT）虽然能把模型性能拉满。但是太“吃资源”了。一个万亿参数的模型。全量微调时不仅要存原权重。还要存梯度、优化器动量。而且这些数据得用FP32精度。比推理时的BF16多占一倍空间。普通团队根本扛不住硬件成本。而LoRA作为参数高效微调的主流方法。确实省内存、省算力。但是大家总是感觉没底。它的效果到底能不能跟全量微调比？会不会调了半天，性能差一截呢？而Thinking Machines Lab最近发布的一篇博客。恰恰把这个问题给讲透了。文章的标题是《LoRA无悔（LoRA Without Regret）》，作者是约翰·舒尔曼（John Schulman）和他的团队。文章不仅回答了“LoRA能不能媲美全量微调”的问题。还给出了具体的操作条件。甚至连超参数怎么设都讲清楚了。不管你是想入门LoRA。还是已经在落地中遇到问题。这篇内容都值得你仔细阅读。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_75.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先，我们得先搞清楚。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">为什么现在大家这么需要LoRA？这还得从大模型的现状说起。现在主流的语言模型。比如Llama 3、Qwen3。参数都在上万亿。预训练时用了数万亿个Token。这么大的规模。就是为了让模型学到人类书面知识里的所有模式。基础性能才能上去。但是到了“训练后处理”阶段。情况就发生了变化。不仅数据集变小了。关注的领域也变窄了。这时候如果还用全量微调。这就像用一辆卡车去拉一颗鸡蛋。太浪费了。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">就是因为这个“浪费”的问题。参数高效微调（PEFT）才发展起来。而低秩适应LoRA就是其中最成熟的方法。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">LoRA的核心思路很巧妙。它不直接改原模型的权重矩阵W。而是给W加了一个“低秩增量项”，公式是W' = W + γBA。这里的W'是微调后的权重。B和A是两个低秩矩阵。它们加起来的参数数量。比原矩阵W少得多。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_141.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">γ是个常数缩放因子。用来调整增量项的大小。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">简单说，LoRA就是用两个小矩阵。“模拟”出原模型权重的更新效果。这样既不用动原模型的海量参数。又能让模型适配新任务。这就是它“参数高效”的关键。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而且LoRA不止省参数。还有三个很实际的优势。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第一个是“多租户服务（Multi-tenant serving）”，因为LoRA只训练A和B这两个“适配器”，原模型权重不动。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">所以一个推理服务器可以在内存里存多个适配器。对应不同的任务。比如一个适配器处理客服对话。另一个处理产品摘要生成。还能批量处理这些请求。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">像陈（Chen）等人2023年提出的Punica框架。还有现在常用的vLLM、SGLang这些推理引擎。都已经支持这个功能。对企业来说能省不少硬件钱。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第二个优势是“训练布局优化”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">全量微调的时候。硬件布局得专门设计。你想，除了原权重。还要存梯度、优化器动量，精度还高。需要的加速器数量往往是推理时的10倍以上。但LoRA要更新的参数少。占用内存也少。训练时的硬件布局只比推理时稍微大一点。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_220.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">普通团队用几台GPU就能跑。门槛低多了。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第三个优势是“加载传输方便”，适配器矩阵的参数少。文件体积小。比如一个秩为128的LoRA适配器。可能就几MB或几十MB。不管是在机器间传。还是加载到模型里。都比传整个模型快得多，运维也灵活。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">正是这些优势。LoRA从2021年胡（Hu）等人发表的第一篇论文（《LoRA:。Low-Rank Adaptation of Large Language Models》）后。就越来越火。但是问题也来了，现有的研究里。LoRA和全量微调的性能对比一直不明确。大家公认的是。在“类似预训练”的场景里。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如数据集特别大。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">超过LoRA的参数存储极限的时候。LoRA性能会差一些；。但是在训练后处理的常见数据集规模下。LoRA虽然容量足够存下关键信息。可是没人能保证它的“样本效率”和“计算效率”，能跟全量微调相比。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">所以约翰·舒尔曼团队的实验。核心就是回答了。LoRA到底能不能匹配全量微调的性能？如果能，需要满足哪些条件呢？</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_289.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">他们的实验结论很明确。只要把几个关键细节做对。LoRA的样本效率和最终性能。就能和全量微调完全一致。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">那这些“关键细节”到底是什么？我们得先从他们的实验设计说起。因为好的实验设计。是结论可信的基础。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">和之前的LoRA研究相比。他们的实验有两个很重要的特点。第一个是“不盯着具体任务。而是研究通用关系”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">很多研究只会选一两个数据集。比如只测情感分析。或者只测文本摘要，但是他们不一样。专门研究“训练集大小”和“LoRA参数数量”之间的通用规律。这样得出的结论。不管你是做医疗还是教育任务。都能参考。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第二个是采用了“对数损失（log loss）评估”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">之前很多研究采用的是“基于采样的评估”，比如让模型生成文本，再人工打分。但是这种方法主观性强。不同任务里标准还不一样。而对数损失能直接反映模型的预测误差。而且能够清晰地看到不同训练步骤、不同参数设置下的模型表现规律。通用性强得多。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_357.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">就是靠着这样的实验设计。他们得出了五个关键发现。这些发现也是LoRA能够媲美全量微调的核心条件。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第一个发现。在中小规模的指令微调或者推理数据集上。LoRA和全量微调的性能完全一样。比如他们用的Tulu3和OpenThoughts3数据集。只要规模没超过LoRA的容量。两者的损失曲线、收敛速度都几乎重合。这说明在大多数企业的落地场景里。LoRA完全够用。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第二个发现。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">如果数据集规模超过了LoRA的容量。LoRA就会比全量微调差。但不是“损失降到一定程度就降不下去”了。而是“训练效率变慢”了。具体会慢多少。要取决于LoRA的参数容量和数据集大小的比例。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如一个秩为32的LoRA。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">处理100万Token的数据集没问题。但是如果数据集涨到1亿个Token。它的训练效率就会明显下降。损失下降速度变慢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_419.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第三个发现。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">LoRA对“批量大小（batch size）”的容忍度比全量微调低。如果批量 size 太大。超过某个阈值后。LoRA的性能损失就会比全量微调大得多。而且这个问题。就算增大LoRA的秩也解决不了。因为这是LoRA“矩阵乘积参数化（BA）”的固有属性。它的优化动态和原权重矩阵（W）不一样。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">批量一大，这种动态差异就会被放大。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第四个发现。LoRA一定要应用到模型的所有层。尤其是MLP层或MoE层，效果才好。之前很多研究受到第一篇LoRA论文的影响。只把LoRA用到注意力层。但是约翰·舒尔曼团队发现。仅在注意力层使用LoRA的性能很差。而MLP层才是LoRA发挥作用的关键。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比德曼（Biderman）等人2024年的研究（《LoRA Learns Less and Forgets Less》）也得出了类似结论。进一步验证了这个发现。第五个发现，在强化学习的场景里。就算用很低秩的LoRA。也能和全量微调性能一致。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_482.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">他们用数学推理任务做实验。比如让模型去解初中数学题。以答案正确性为奖励，结果发现。哪怕LoRA的秩只有1。最终的准确率也和全量微调一样。这背后有个信息论的解释。那就是监督学习里。每一轮训练能给模型提供“O(Token数量)”个比特的信息。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如一句话有100个Token。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">就能提供100比特左右的信息；。但是强化学习不一样。政策梯度算法的学习。靠的是“优势函数（Advantage Function）”，每一轮只能提供“O(1)”比特的信息。简单来说。就是强化学习需要学的信息少。就算是低秩LoRA，容量也完全够。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">基于这些发现。他们提出了一个“低遗憾区间（low-regret regime）”的概念。只要LoRA的参数容量能覆盖数据集的信息需求。并且应用到所有层。就能和全量微调性能相当。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而这个区间。刚好覆盖了大多数企业的训练后处理场景。这也意味着，LoRA在很多实际应用里。完全可以替代全量微调。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_547.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">当然，光有结论不够。我们还得知道这些结论是怎么来的。也就是他们的实验细节。这能帮我们在自己做实验的时候少走弯路。我们挑几个部分来重点讲一下。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先，他们的实验设置非常严谨。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先是LoRA的秩。覆盖了1到512三个数量级。从最低的秩1。到能覆盖大部分场景的秩128、256。再到接近全量微调的秩512。这样能全面看到秩对性能的影响。然后是学习率（LR）。为了避免“因为学习率选得不好。导致LoRA性能差”的情况出现。他们对每个实验条件都做了“学习率扫描”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如对秩32的LoRA。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">测试0.0001到0.01的不同学习率。找到最佳值；。而且采用了“恒定学习率”，没有热身或冷却阶段。这样能够排除学习率调度的干扰。更准确对比LoRA和全量微调的差异。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_608.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">模型方面，他们用了两类主流模型。一类是Llama 3系列。另一类是Qwen3系列。还包括了混合专家（MoE）模型。因为MoE模型的激活效率高。现在很多大模型都用这种架构。所以加入MoE的实验，能让结论更全面。数据集方面。监督学习用了Tulu3和OpenThoughts3。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">Tulu3是艾维森（Ivison）等人在2024年提出的。侧重“指令遵循”，比如让模型根据用户指令生成邮件、写代码；。OpenThoughts3是古哈（Guha）等人在2025年提出的。侧重“推理”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如数学题、逻辑推理题。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这两个数据集在任务类型、数据结构上差异很大。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">强化学习实验则用了MATH数据集和GSM8K数据集。都是数学推理任务。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">以答案正确与否作为奖励信号。以便专注测试强化学习场景下的LoRA性能。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">先看“LoRA秩”的影响实验。他们在Tulu3和OpenThoughts3上训练了一个epoch。对每个数据集、每个模型大小。都扫描了不同的LoRA秩和学习率。结果发现，高秩LoRA，比如256、512。和全量微调的学习曲线几乎重合。损失都是“随步骤的对数线性下降”，也就是说。每训练10倍的步骤。损失就会按固定比例下降。这和全量微调的规律完全一样。而低秩LoRA，比如1、4。在训练初期还能跟上。但是到了某个步骤阈值后。就会“偏离最小损失曲线”。这就是之前说的“容量耗尽”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_707.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">低秩矩阵存不下更多的任务信息。所以学习速度变慢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">更重要的是“学习率”的发现。他们绘制了“学习率和最终损失”的曲线。发现高秩LoRA的最小损失和全量微调几乎一样。但是最佳学习率是全量微调的10倍。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如全量微调的最佳学习率是0.001。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">LoRA就需要0.01。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比德曼（Biderman）等人2024年的研究也发现了类似的10倍比例。这说明这个规律不是偶然的。而是LoRA的固有属性。而且不同秩的LoRA。最佳学习率很接近。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如秩4和秩512的最佳学习率差异不到2倍。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这为我们设置LoRA学习率提供了很大便利。不用频繁调整。按全量微调的10倍开始试就行。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">再看“批量大小效应”的实验。他们用了OpenThoughts3里一个1万样本的子集。测试不同批量大小的影响。结果很明显，当批量大小比较小。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_768.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如32的时候。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">LoRA和全量微调的性能差距很小。而且随着训练推进，差距还会缩小；。但当批量大小变大。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如256、512的时候。LoRA的损失会明显比全量微调高。而且这个差距会一直存在。不会随训练步骤消失。更关键的是。这个差距和LoRA的秩无关。就算用秩512的LoRA。大批次下还是比全量微调差。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">他们分析。这是因为LoRA的“矩阵乘积（BA）”参数化。和原权重矩阵（W）的优化动态不同。批量一大。原矩阵能更好地利用批次信息更新。而BA矩阵的更新效率会下降。导致性能差距。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">不过好在，不管是LoRA还是全量微调。都是“小批量下性能更好”，所以实际应用里。只要别用太大的批量。这个问题就能够缓解。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">接下来。我们来重点说一说跟前面第4点发现。“LoRA该应用到哪些层”有关的实验过程。这是很多人调参时容易踩坑的地方。之前很多人习惯只把LoRA用到注意力层。觉得注意力层负责“理解上下文”，所以是关键，但是实验结果刚好相反。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_838.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">约翰·舒尔曼团队做了三组对比实验。第一组只在注意力层用LoRA。第二组只在MLP层用LoRA。第三组在所有层。也就是注意力+MLP+MoE都用了LoRA。结果发现。仅注意力层的LoRA性能最差。就算用更高的秩。参数数量和仅MLP层的秩128差不多。性能还是差一截。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如在Llama-3.1-8B模型上，仅MLP层。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">秩128，0.24B参数的损失。比仅注意力层。秩256，0.25B参数低了0.15左右。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">要知道，在语言模型里。损失降低0.1已经是很明显的提升了。而“所有层用LoRA”和“仅MLP层用LoRA”的性能几乎一样。这说明MLP层才是LoRA发挥作用的核心。注意力层加不加LoRA。对最终性能影响不大。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">他们还在MoE模型上做了实验。对Qwen3-30B-A3B-Base的每个专家都单独训练LoRA。秩设为“总秩除以活跃专家数量”，这样能保证MoE层的LoRA参数比例和其他层一致。结果和稠密模型一样。仅MLP层的LoRA优于仅注意力层。所有层和仅MLP层相当。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_918.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">为什么会这样？这就要用到马拉迪（Malladi）等人在2022年提出的“经验神经正切核（eNTK）”理论了。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">eNTK是用来描述模型微调初期学习动态的工具。它的核心是“梯度的点积”。比如对每个Token的预测。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">梯度g_i是损失对参数的偏导。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">eNTK的核函数K(i。j)就是g_i和g_j的点积。这个核函数决定了模型怎么“学习”数据里的模式。而参数越多的层。对核函数的影响越大。马拉迪等人的研究指出。当LoRA应用到所有层的时候。它的eNTK和全量微调的eNTK几乎一样。所以学习动态也相似；。但是如果只应用到注意力层。而忽略了参数更多的MLP层。eNTK就会和全量微调有明显差异。学习速度自然慢下来。这也解释了为什么仅注意力层的LoRA效果差。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">聊完实验。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">我们最后再来总结一下这篇研究的主要结论和背后的深层意义。它不止告诉了我们LoRA怎么用。还帮我们深化了对大模型微调的理解。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_986.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先是“LoRA匹配全量微调的两个核心条件”，一是要应用到所有层。尤其是MLP/MoE层。这样才能保证eNTK和全量微调的一致；。二是LoRA的容量要能覆盖数据集的信息需求。也就是“非容量约束”。只要满足这两个条件。LoRA就能和全量微调性能相当。这为我们选择微调方法提供了明确的判断标准。如果你的数据集是中小规模。模型有MLP层。就用LoRA；。如果数据集特别大。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如接近预训练规模。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">再考虑全量微调。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">其次是“监督学习和RL的容量需求的差异”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">监督学习需要的信息多。每个token 1比特。所以需要更高秩的LoRA；。强化学习需要的信息少，每轮次1比特。低秩LoRA就够。这个差异能帮我们节省资源。也就是做强化学习任务时。不用追求高秩。秩1、8就够，参数更少，训练更快。然后是“LoRA的计算效率优势”，他们算了一笔账。LoRA每轮训练的FLOPs。是全量微调的2/3左右。具体怎么算的大家可以去看一下这一段。这里我们就不一一推导了。简单来说，对于一个N×N的权重矩阵W。LoRA的总FLOPs是2N²。全量微调是3N²，所以比例就是2/3。这意味着，同样的硬件。LoRA能比全量微调多训练50%的样本。计算效率更高。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_1079.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">当然，研究也提出了一些“开放问题”，这些也是未来值得关注的方向。比如怎么更精准地预测LoRA的性能。不用每次都做实验；。为什么LoRA的最佳学习率是全量微调的10倍。目前还没有完整的理论解释；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">LoRA的变体在这种评估方法下表现如何；。还有MoE层的LoRA。怎么和张量并行、专家并行这些技术结合。适配更大的模型，等等等等。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">最后，约翰·舒尔曼团队在结语里提到。他们研究LoRA的目标。不只是为了省资源。更是为了“让大模型的微调能力更易获取”，毕竟不是每个团队都有能力做全量微调。但是LoRA能让普通团队也能用大模型来解决具体的问题。而且通过研究LoRA。他们也深化了对“模型容量”、“数据集复杂度”、“样本效率”这些基础问题的理解。这其实也是AI研究的魅力。一个实用的技术。背后往往藏着对核心原理的新认知。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_1140.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">好了，今天的内容就到这里。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">如果你在做LoRA相关的工作。希望今天的拆解能帮你少走弯路；。如果你只是感兴趣。也希望能够帮助你增加对LoRA的了解。感谢大家观看本期视频。我们下期再见。</p>

<hr style="clear:both" />
=============
<p><a href="https://www.youtube.com/watch?v=MlqXL6o--2M">https://www.youtube.com/watch?v=MlqXL6o--2M</a></p><p>Thinking Machines Lab最近发布的一篇博客，文章的标题是《LoRA无悔（LoRA Without Regret）》，作者是约翰·舒尔曼（John Schulman）和他的团队。文章不仅回答了“LoRA能不能媲美全量微调”的问题，还给出了具体的操作条件，甚至连超参数怎么设都讲清楚了。不管你是想入门LoRA，还是已经在落地中遇到问题，这篇内容都值得你仔细阅读。</p><p><a href="https://thinkingmachines.ai/blog/lora/">https://thinkingmachines.ai/blog/lora/</a></p>]]>
      </description>
      <content:encoded><![CDATA[<hr style="clear:both" />

<p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://img.youtube.com/vi/MlqXL6o--2M/maxresdefault.jpg" /></p><p><a href="https://www.youtube.com/watch?v=MlqXL6o--2M">https://www.youtube.com/watch?v=MlqXL6o--2M</a></p><h1>值得閱讀的理由</h1> <ul> <li>深入理解<strong>LoRA</strong>與<strong>全量微調（FullFT）</strong>在資源消耗與性能表現之間的平衡，為大模型微調提供決策依據。</li> <li>掌握<strong>約翰·舒爾曼（John Schulman）</strong>團隊提出的關鍵條件和最佳實踐，有效提升LoRA的微調效果。</li> <li>探索LoRA在監督學習與強化學習場景下的應用差異，以及未來大模型微調研究的開放性問題。</li> </ul> <hr /> <h1>摘要</h1> <p>本影片由影片的講者深入探討了當今大模型微調領域中一個核心問題：究竟該選擇<strong>低秩適應（LoRA）</strong>還是<strong>全量微調（FullFT）</strong>？這個問題的本質在於<strong>「資源」</strong>與<strong>「效果」</strong>的權衡。影片指出，儘管全量微調能最大化模型性能，但其對硬體資源的龐大需求（如儲存原始權重、梯度、優化器動量，且需使用FP32精度）使得一般團隊難以承受。相較之下，LoRA作為一種參數高效微調的主流方法，以其節省記憶體和算力的優勢吸引了廣泛關注，但其性能能否媲美全量微調一直是業界普遍的疑慮。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_90.jpg" /></p> <p>影片接著介紹了由<strong>約翰·舒爾曼（John Schulman）及其團隊</strong>發表的深度分析文章《LoRA無悔（LoRA Without Regret）》，該文章不僅回應了LoRA能否匹敵全量微調的疑問，更提出了具體的操作條件和超參數設定建議。影片強調，理解LoRA的重要性源於當前主流大型語言模型（如Llama 3、Qwen3）龐大的參數規模和預訓練資料量。在模型的「訓練後處理」階段，由於資料集規模變小且關注領域變窄，全量微調顯得過於浪費，因此催生了<strong>參數高效微調（PEFT）</strong>的發展，而LoRA正是其中最成熟的方法。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_180.jpg" /></p> <h2>LoRA的核心機制與優勢</h2> <p>影片詳細解釋了LoRA的核心思路：它不直接修改原模型的權重矩陣W，而是添加一個「低秩增量項」<strong>γBA</strong>。這裡的B和A是兩個低秩矩陣，它們的參數數量遠少於原始權重矩陣W，卻能「模擬」出原模型權重的更新效果。這種機制不僅實現了<strong>參數高效性</strong>，還帶來了多項實際優勢。首先是<strong>「多租戶服務」</strong>，由於LoRA只訓練輕量級的「適配器」，一個推理伺服器能夠同時載入多個適配器以處理不同任務，顯著節省硬體成本。其次是<strong>「訓練佈局優化」</strong>，LoRA更新的參數少，記憶體佔用小，使得訓練所需的硬體資源大幅降低，普通團隊也能用少量GPU進行訓練。最後是<strong>「載入傳輸方便」</strong>，LoRA適配器檔案體積小（僅幾MB到幾十MB），便於機器間傳輸和模型載入，提高了營運彈性。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_300.jpg" /></p> <h2>關鍵實驗發現與「低遺憾區間」</h2> <p>影片闡述了約翰·舒爾曼團隊實驗的核心目標：回答LoRA能否匹配全量微調的性能，以及需要滿足哪些條件。他們採取的實驗設計具有兩個顯著特點：一是<strong>「不聚焦特定任務，而是研究通用關係」</strong>，探討「訓練集大小」與「LoRA參數數量」之間的通用規律；二是採用<strong>「對數損失（log loss）評估」</strong>，能客觀反映模型的預測誤差和訓練表現。透過嚴謹的實驗設計，團隊得出了五個關鍵發現：</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_450.jpg" /></p> <p>1.  在中小規模的指令微調或推理資料集上，只要不超過LoRA的容量，<strong>LoRA與全量微調的性能幾乎完全一致</strong>。 2.  若資料集規模超出LoRA容量，LoRA會表現較差，但並非無法下降，而是<strong>「訓練效率變慢」</strong>。 3.  LoRA對<strong>「批量大小（batch size）」</strong>的容忍度低於全量微調，批次過大會導致LoRA性能明顯下降，且增大LoRA的秩也無法解決此問題。 4.  LoRA必須應用於模型<strong>「所有層」</strong>，尤其是<strong>MLP層或MoE層</strong>，效果才最佳；僅在注意力層使用LoRA的性能表現不佳。 5.  在<strong>強化學習（RL）</strong>場景中，即使使用<strong>極低秩的LoRA</strong>也能與全量微調性能一致，這歸因於強化學習每輪所需的資訊量較少（O(1)比特）。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_705.jpg" /></p> <p>基於這些發現，研究團隊提出了<strong>「低遺憾區間（low-regret regime）」</strong>的概念：只要LoRA的參數容量能覆蓋資料集所需的資訊，並且應用到所有層，就能與全量微調性能相當。這意味著在大多數企業的訓練後處理場景中，LoRA完全可以替代全量微調。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_750.jpg" /></p> <h2>實驗細節剖析</h2> <p>影片進一步深入探討了約翰·舒爾曼團隊的實驗細節，為實踐者提供了寶貴的參考。他們測試了從<strong>秩1到512</strong>的三個數量級LoRA秩，並對每個實驗條件進行了<strong>「學習率掃描」</strong>以找到最佳值，同時採用<strong>「恆定學習率」</strong>排除調度干擾。實驗模型涵蓋了<strong>Llama 3系列</strong>和<strong>Qwen3系列</strong>，以及<strong>混合專家（MoE）模型</strong>。資料集則選用了監督學習的<strong>Tulu3</strong>和<strong>OpenThoughts3</strong>，以及強化學習的<strong>MATH</strong>和<strong>GSM8K</strong>，確保了實驗的廣泛性和代表性。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_880.jpg" /></p> <p>關於<strong>LoRA秩的影響</strong>，高秩LoRA（如256、512）與全量微調的學習曲線幾乎重合，損失呈現「隨步驟的對數線性下降」；低秩LoRA（如1、4）在訓練初期尚可，但達到閾值後便會因「容量耗盡」而偏離最佳損失曲線。在<strong>學習率</strong>方面，研究發現LoRA的最佳學習率約為全量微調的10倍，且不同秩的LoRA之間最佳學習率差異不大，這為設定LoRA學習率提供了便利。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_990.jpg" /></p> <p><strong>批量大小效應</strong>實驗揭示，當批量大小較小（如32）時，LoRA與全量微調性能差距微小；但當批量大小增大（如256、512）時，LoRA的損失會顯著高於全量微調，且這種差距與LoRA的秩無關。這歸因於LoRA的「矩陣乘積參數化（BA）」與原權重矩陣（W）的優化動態不同，大批量會放大這種差異。幸運的是，在實際應用中，只要避免使用過大的批量，此問題便可緩解。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_1070.jpg" /></p> <p>影片特別強調了<strong>LoRA應該應用到哪些層</strong>的實驗結果。研究團隊通過三組對比實驗（僅注意力層、僅MLP層、所有層）發現，僅在注意力層使用LoRA的性能最差，而<strong>MLP層才是LoRA發揮作用的核心</strong>；將LoRA應用於所有層與僅MLP層的性能幾乎一致。這個發現與<strong>經驗神經正切核（eNTK）理論</strong>相符：參數越多的層對核函數影響越大，應用於所有層能使LoRA的eNTK與全量微調的eNTK保持一致，從而實現相似的學習動態。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_20.jpg" /></p> <h2>結論與深層意義</h2> <p>影片總結了該研究的深層意義。首先，LoRA匹配全量微調的核心條件有二：一是<strong>必須應用到所有層（尤其MLP/MoE層）</strong>，確保eNTK的一致性；二是<strong>LoRA的容量需能覆蓋資料集所需的資訊</strong>。其次，<strong>監督學習與強化學習對LoRA容量的需求存在顯著差異</strong>：監督學習需要更多資訊，適合高秩LoRA；強化學習每輪所需資訊少，低秩LoRA便足夠。這能幫助開發者在強化學習任務中節省資源，無需追求高秩。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_22.jpg" /></p> <p>研究還指出LoRA的<strong>計算效率優勢</strong>，每輪訓練的FLOPs約為全量微調的2/3，意味著在相同硬體下LoRA能多訓練50%的樣本。儘管如此，研究也提出了一些<strong>「開放問題」</strong>，例如如何更精準地預測LoRA性能、LoRA最佳學習率是全量微調10倍的理論解釋、LoRA變體的表現，以及LoE層LoRA如何與張量並行等技術結合以適配更大模型等，這些都是未來值得關注的研究方向。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_24.jpg" /></p> <p>最後，約翰·舒爾曼團隊的研究目標不僅是為了節省資源，更是為了<strong>「讓大模型的微調能力更易獲取」</strong>，使普通團隊也能利用大模型解決具體問題。透過對LoRA的深入研究，他們也深化了對「模型容量」、「資料集複雜度」、「樣本效率」等基礎問題的理解，展現了AI研究中實用技術背後蘊藏的核心原理新認知。</p> <hr /><hr />================================================</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">大家好，这里是最佳拍档，我是大飞。现在微调大模型。到底应该用LoRA还是全量微调呢？其实这个问题的核心。本质是“资源”和“效果”的平衡。全量微调（FullFT）虽然能把模型性能拉满。但是太“吃资源”了。一个万亿参数的模型。全量微调时不仅要存原权重。还要存梯度、优化器动量。而且这些数据得用FP32精度。比推理时的BF16多占一倍空间。普通团队根本扛不住硬件成本。而LoRA作为参数高效微调的主流方法。确实省内存、省算力。但是大家总是感觉没底。它的效果到底能不能跟全量微调比？会不会调了半天，性能差一截呢？而Thinking Machines Lab最近发布的一篇博客。恰恰把这个问题给讲透了。文章的标题是《LoRA无悔（LoRA Without Regret）》，作者是约翰·舒尔曼（John Schulman）和他的团队。文章不仅回答了“LoRA能不能媲美全量微调”的问题。还给出了具体的操作条件。甚至连超参数怎么设都讲清楚了。不管你是想入门LoRA。还是已经在落地中遇到问题。这篇内容都值得你仔细阅读。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_75.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先，我们得先搞清楚。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">为什么现在大家这么需要LoRA？这还得从大模型的现状说起。现在主流的语言模型。比如Llama 3、Qwen3。参数都在上万亿。预训练时用了数万亿个Token。这么大的规模。就是为了让模型学到人类书面知识里的所有模式。基础性能才能上去。但是到了“训练后处理”阶段。情况就发生了变化。不仅数据集变小了。关注的领域也变窄了。这时候如果还用全量微调。这就像用一辆卡车去拉一颗鸡蛋。太浪费了。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">就是因为这个“浪费”的问题。参数高效微调（PEFT）才发展起来。而低秩适应LoRA就是其中最成熟的方法。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">LoRA的核心思路很巧妙。它不直接改原模型的权重矩阵W。而是给W加了一个“低秩增量项”，公式是W' = W + γBA。这里的W'是微调后的权重。B和A是两个低秩矩阵。它们加起来的参数数量。比原矩阵W少得多。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_141.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">γ是个常数缩放因子。用来调整增量项的大小。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">简单说，LoRA就是用两个小矩阵。“模拟”出原模型权重的更新效果。这样既不用动原模型的海量参数。又能让模型适配新任务。这就是它“参数高效”的关键。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而且LoRA不止省参数。还有三个很实际的优势。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第一个是“多租户服务（Multi-tenant serving）”，因为LoRA只训练A和B这两个“适配器”，原模型权重不动。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">所以一个推理服务器可以在内存里存多个适配器。对应不同的任务。比如一个适配器处理客服对话。另一个处理产品摘要生成。还能批量处理这些请求。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">像陈（Chen）等人2023年提出的Punica框架。还有现在常用的vLLM、SGLang这些推理引擎。都已经支持这个功能。对企业来说能省不少硬件钱。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第二个优势是“训练布局优化”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">全量微调的时候。硬件布局得专门设计。你想，除了原权重。还要存梯度、优化器动量，精度还高。需要的加速器数量往往是推理时的10倍以上。但LoRA要更新的参数少。占用内存也少。训练时的硬件布局只比推理时稍微大一点。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_220.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">普通团队用几台GPU就能跑。门槛低多了。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第三个优势是“加载传输方便”，适配器矩阵的参数少。文件体积小。比如一个秩为128的LoRA适配器。可能就几MB或几十MB。不管是在机器间传。还是加载到模型里。都比传整个模型快得多，运维也灵活。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">正是这些优势。LoRA从2021年胡（Hu）等人发表的第一篇论文（《LoRA:。Low-Rank Adaptation of Large Language Models》）后。就越来越火。但是问题也来了，现有的研究里。LoRA和全量微调的性能对比一直不明确。大家公认的是。在“类似预训练”的场景里。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如数据集特别大。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">超过LoRA的参数存储极限的时候。LoRA性能会差一些；。但是在训练后处理的常见数据集规模下。LoRA虽然容量足够存下关键信息。可是没人能保证它的“样本效率”和“计算效率”，能跟全量微调相比。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">所以约翰·舒尔曼团队的实验。核心就是回答了。LoRA到底能不能匹配全量微调的性能？如果能，需要满足哪些条件呢？</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_289.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">他们的实验结论很明确。只要把几个关键细节做对。LoRA的样本效率和最终性能。就能和全量微调完全一致。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">那这些“关键细节”到底是什么？我们得先从他们的实验设计说起。因为好的实验设计。是结论可信的基础。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">和之前的LoRA研究相比。他们的实验有两个很重要的特点。第一个是“不盯着具体任务。而是研究通用关系”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">很多研究只会选一两个数据集。比如只测情感分析。或者只测文本摘要，但是他们不一样。专门研究“训练集大小”和“LoRA参数数量”之间的通用规律。这样得出的结论。不管你是做医疗还是教育任务。都能参考。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第二个是采用了“对数损失（log loss）评估”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">之前很多研究采用的是“基于采样的评估”，比如让模型生成文本，再人工打分。但是这种方法主观性强。不同任务里标准还不一样。而对数损失能直接反映模型的预测误差。而且能够清晰地看到不同训练步骤、不同参数设置下的模型表现规律。通用性强得多。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_357.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">就是靠着这样的实验设计。他们得出了五个关键发现。这些发现也是LoRA能够媲美全量微调的核心条件。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第一个发现。在中小规模的指令微调或者推理数据集上。LoRA和全量微调的性能完全一样。比如他们用的Tulu3和OpenThoughts3数据集。只要规模没超过LoRA的容量。两者的损失曲线、收敛速度都几乎重合。这说明在大多数企业的落地场景里。LoRA完全够用。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第二个发现。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">如果数据集规模超过了LoRA的容量。LoRA就会比全量微调差。但不是“损失降到一定程度就降不下去”了。而是“训练效率变慢”了。具体会慢多少。要取决于LoRA的参数容量和数据集大小的比例。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如一个秩为32的LoRA。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">处理100万Token的数据集没问题。但是如果数据集涨到1亿个Token。它的训练效率就会明显下降。损失下降速度变慢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_419.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第三个发现。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">LoRA对“批量大小（batch size）”的容忍度比全量微调低。如果批量 size 太大。超过某个阈值后。LoRA的性能损失就会比全量微调大得多。而且这个问题。就算增大LoRA的秩也解决不了。因为这是LoRA“矩阵乘积参数化（BA）”的固有属性。它的优化动态和原权重矩阵（W）不一样。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">批量一大，这种动态差异就会被放大。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第四个发现。LoRA一定要应用到模型的所有层。尤其是MLP层或MoE层，效果才好。之前很多研究受到第一篇LoRA论文的影响。只把LoRA用到注意力层。但是约翰·舒尔曼团队发现。仅在注意力层使用LoRA的性能很差。而MLP层才是LoRA发挥作用的关键。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比德曼（Biderman）等人2024年的研究（《LoRA Learns Less and Forgets Less》）也得出了类似结论。进一步验证了这个发现。第五个发现，在强化学习的场景里。就算用很低秩的LoRA。也能和全量微调性能一致。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_482.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">他们用数学推理任务做实验。比如让模型去解初中数学题。以答案正确性为奖励，结果发现。哪怕LoRA的秩只有1。最终的准确率也和全量微调一样。这背后有个信息论的解释。那就是监督学习里。每一轮训练能给模型提供“O(Token数量)”个比特的信息。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如一句话有100个Token。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">就能提供100比特左右的信息；。但是强化学习不一样。政策梯度算法的学习。靠的是“优势函数（Advantage Function）”，每一轮只能提供“O(1)”比特的信息。简单来说。就是强化学习需要学的信息少。就算是低秩LoRA，容量也完全够。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">基于这些发现。他们提出了一个“低遗憾区间（low-regret regime）”的概念。只要LoRA的参数容量能覆盖数据集的信息需求。并且应用到所有层。就能和全量微调性能相当。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而这个区间。刚好覆盖了大多数企业的训练后处理场景。这也意味着，LoRA在很多实际应用里。完全可以替代全量微调。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_547.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">当然，光有结论不够。我们还得知道这些结论是怎么来的。也就是他们的实验细节。这能帮我们在自己做实验的时候少走弯路。我们挑几个部分来重点讲一下。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先，他们的实验设置非常严谨。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先是LoRA的秩。覆盖了1到512三个数量级。从最低的秩1。到能覆盖大部分场景的秩128、256。再到接近全量微调的秩512。这样能全面看到秩对性能的影响。然后是学习率（LR）。为了避免“因为学习率选得不好。导致LoRA性能差”的情况出现。他们对每个实验条件都做了“学习率扫描”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如对秩32的LoRA。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">测试0.0001到0.01的不同学习率。找到最佳值；。而且采用了“恒定学习率”，没有热身或冷却阶段。这样能够排除学习率调度的干扰。更准确对比LoRA和全量微调的差异。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_608.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">模型方面，他们用了两类主流模型。一类是Llama 3系列。另一类是Qwen3系列。还包括了混合专家（MoE）模型。因为MoE模型的激活效率高。现在很多大模型都用这种架构。所以加入MoE的实验，能让结论更全面。数据集方面。监督学习用了Tulu3和OpenThoughts3。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">Tulu3是艾维森（Ivison）等人在2024年提出的。侧重“指令遵循”，比如让模型根据用户指令生成邮件、写代码；。OpenThoughts3是古哈（Guha）等人在2025年提出的。侧重“推理”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如数学题、逻辑推理题。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这两个数据集在任务类型、数据结构上差异很大。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">强化学习实验则用了MATH数据集和GSM8K数据集。都是数学推理任务。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">以答案正确与否作为奖励信号。以便专注测试强化学习场景下的LoRA性能。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">先看“LoRA秩”的影响实验。他们在Tulu3和OpenThoughts3上训练了一个epoch。对每个数据集、每个模型大小。都扫描了不同的LoRA秩和学习率。结果发现，高秩LoRA，比如256、512。和全量微调的学习曲线几乎重合。损失都是“随步骤的对数线性下降”，也就是说。每训练10倍的步骤。损失就会按固定比例下降。这和全量微调的规律完全一样。而低秩LoRA，比如1、4。在训练初期还能跟上。但是到了某个步骤阈值后。就会“偏离最小损失曲线”。这就是之前说的“容量耗尽”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_707.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">低秩矩阵存不下更多的任务信息。所以学习速度变慢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">更重要的是“学习率”的发现。他们绘制了“学习率和最终损失”的曲线。发现高秩LoRA的最小损失和全量微调几乎一样。但是最佳学习率是全量微调的10倍。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如全量微调的最佳学习率是0.001。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">LoRA就需要0.01。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比德曼（Biderman）等人2024年的研究也发现了类似的10倍比例。这说明这个规律不是偶然的。而是LoRA的固有属性。而且不同秩的LoRA。最佳学习率很接近。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如秩4和秩512的最佳学习率差异不到2倍。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这为我们设置LoRA学习率提供了很大便利。不用频繁调整。按全量微调的10倍开始试就行。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">再看“批量大小效应”的实验。他们用了OpenThoughts3里一个1万样本的子集。测试不同批量大小的影响。结果很明显，当批量大小比较小。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_768.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如32的时候。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">LoRA和全量微调的性能差距很小。而且随着训练推进，差距还会缩小；。但当批量大小变大。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如256、512的时候。LoRA的损失会明显比全量微调高。而且这个差距会一直存在。不会随训练步骤消失。更关键的是。这个差距和LoRA的秩无关。就算用秩512的LoRA。大批次下还是比全量微调差。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">他们分析。这是因为LoRA的“矩阵乘积（BA）”参数化。和原权重矩阵（W）的优化动态不同。批量一大。原矩阵能更好地利用批次信息更新。而BA矩阵的更新效率会下降。导致性能差距。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">不过好在，不管是LoRA还是全量微调。都是“小批量下性能更好”，所以实际应用里。只要别用太大的批量。这个问题就能够缓解。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">接下来。我们来重点说一说跟前面第4点发现。“LoRA该应用到哪些层”有关的实验过程。这是很多人调参时容易踩坑的地方。之前很多人习惯只把LoRA用到注意力层。觉得注意力层负责“理解上下文”，所以是关键，但是实验结果刚好相反。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_838.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">约翰·舒尔曼团队做了三组对比实验。第一组只在注意力层用LoRA。第二组只在MLP层用LoRA。第三组在所有层。也就是注意力+MLP+MoE都用了LoRA。结果发现。仅注意力层的LoRA性能最差。就算用更高的秩。参数数量和仅MLP层的秩128差不多。性能还是差一截。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如在Llama-3.1-8B模型上，仅MLP层。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">秩128，0.24B参数的损失。比仅注意力层。秩256，0.25B参数低了0.15左右。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">要知道，在语言模型里。损失降低0.1已经是很明显的提升了。而“所有层用LoRA”和“仅MLP层用LoRA”的性能几乎一样。这说明MLP层才是LoRA发挥作用的核心。注意力层加不加LoRA。对最终性能影响不大。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">他们还在MoE模型上做了实验。对Qwen3-30B-A3B-Base的每个专家都单独训练LoRA。秩设为“总秩除以活跃专家数量”，这样能保证MoE层的LoRA参数比例和其他层一致。结果和稠密模型一样。仅MLP层的LoRA优于仅注意力层。所有层和仅MLP层相当。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_918.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">为什么会这样？这就要用到马拉迪（Malladi）等人在2022年提出的“经验神经正切核（eNTK）”理论了。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">eNTK是用来描述模型微调初期学习动态的工具。它的核心是“梯度的点积”。比如对每个Token的预测。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">梯度g_i是损失对参数的偏导。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">eNTK的核函数K(i。j)就是g_i和g_j的点积。这个核函数决定了模型怎么“学习”数据里的模式。而参数越多的层。对核函数的影响越大。马拉迪等人的研究指出。当LoRA应用到所有层的时候。它的eNTK和全量微调的eNTK几乎一样。所以学习动态也相似；。但是如果只应用到注意力层。而忽略了参数更多的MLP层。eNTK就会和全量微调有明显差异。学习速度自然慢下来。这也解释了为什么仅注意力层的LoRA效果差。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">聊完实验。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">我们最后再来总结一下这篇研究的主要结论和背后的深层意义。它不止告诉了我们LoRA怎么用。还帮我们深化了对大模型微调的理解。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_986.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先是“LoRA匹配全量微调的两个核心条件”，一是要应用到所有层。尤其是MLP/MoE层。这样才能保证eNTK和全量微调的一致；。二是LoRA的容量要能覆盖数据集的信息需求。也就是“非容量约束”。只要满足这两个条件。LoRA就能和全量微调性能相当。这为我们选择微调方法提供了明确的判断标准。如果你的数据集是中小规模。模型有MLP层。就用LoRA；。如果数据集特别大。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如接近预训练规模。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">再考虑全量微调。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">其次是“监督学习和RL的容量需求的差异”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">监督学习需要的信息多。每个token 1比特。所以需要更高秩的LoRA；。强化学习需要的信息少，每轮次1比特。低秩LoRA就够。这个差异能帮我们节省资源。也就是做强化学习任务时。不用追求高秩。秩1、8就够，参数更少，训练更快。然后是“LoRA的计算效率优势”，他们算了一笔账。LoRA每轮训练的FLOPs。是全量微调的2/3左右。具体怎么算的大家可以去看一下这一段。这里我们就不一一推导了。简单来说，对于一个N×N的权重矩阵W。LoRA的总FLOPs是2N²。全量微调是3N²，所以比例就是2/3。这意味着，同样的硬件。LoRA能比全量微调多训练50%的样本。计算效率更高。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_1079.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">当然，研究也提出了一些“开放问题”，这些也是未来值得关注的方向。比如怎么更精准地预测LoRA的性能。不用每次都做实验；。为什么LoRA的最佳学习率是全量微调的10倍。目前还没有完整的理论解释；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">LoRA的变体在这种评估方法下表现如何；。还有MoE层的LoRA。怎么和张量并行、专家并行这些技术结合。适配更大的模型，等等等等。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">最后，约翰·舒尔曼团队在结语里提到。他们研究LoRA的目标。不只是为了省资源。更是为了“让大模型的微调能力更易获取”，毕竟不是每个团队都有能力做全量微调。但是LoRA能让普通团队也能用大模型来解决具体的问题。而且通过研究LoRA。他们也深化了对“模型容量”、“数据集复杂度”、“样本效率”这些基础问题的理解。这其实也是AI研究的魅力。一个实用的技术。背后往往藏着对核心原理的新认知。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_1140.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">好了，今天的内容就到这里。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">如果你在做LoRA相关的工作。希望今天的拆解能帮你少走弯路；。如果你只是感兴趣。也希望能够帮助你增加对LoRA的了解。感谢大家观看本期视频。我们下期再见。</p>

<hr style="clear:both" />
=============
<p><a href="https://www.youtube.com/watch?v=MlqXL6o--2M">https://www.youtube.com/watch?v=MlqXL6o--2M</a></p><p>Thinking Machines Lab最近发布的一篇博客，文章的标题是《LoRA无悔（LoRA Without Regret）》，作者是约翰·舒尔曼（John Schulman）和他的团队。文章不仅回答了“LoRA能不能媲美全量微调”的问题，还给出了具体的操作条件，甚至连超参数怎么设都讲清楚了。不管你是想入门LoRA，还是已经在落地中遇到问题，这篇内容都值得你仔细阅读。</p><p><a href="https://thinkingmachines.ai/blog/lora/">https://thinkingmachines.ai/blog/lora/</a></p>]]></content:encoded>
      <itunes:image href="https://i.ytimg.com/vi/MlqXL6o--2M/hqdefault.jpg"/>
      <pubDate>2025-10-02T09:00:39.000Z</pubDate>
    </item><item>
      <title><![CDATA[Sora 2 official demo collections]]></title>
      <link>https://www.youtube.com/watch?v=M1FhHfY82mY</link>
      <itunes:title><![CDATA[Sora 2 official demo collections]]></itunes:title>
      <itunes:author><![CDATA[最佳拍档]]></itunes:author>
      <itunes:summary>
        <![CDATA[<hr style="clear:both" />

<p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://img.youtube.com/vi/M1FhHfY82mY/maxresdefault.jpg" /></p><p><a href="https://www.youtube.com/watch?v=M1FhHfY82mY">https://www.youtube.com/watch?v=M1FhHfY82mY</a></p><h1>值得閱讀的理由</h1><ul><li>探索人類挑戰極限和發現奇特能力的多樣面貌。</li><li>體驗一場充滿刺激冒險、溫馨互動與深刻反思的旅程。</li><li>見證團隊合作、堅韌不拔及對知識永無止境的追求。</li></ul><hr /><h1>摘要</h1><h2>挑戰與技巧</h2><p>影片開場以一場驚險的特技或墜落展開，作者在看似危險的情況下展現了<strong>從容不迫</strong>的態度。隨後，作者展示了與一隻貓的互動，暗示著某種表演或協作。接著，作者駁斥了「不能騎馬」的說法，與搭檔一同展現了<strong>精湛而穩定的騎術</strong>，證明了他們的默契與技巧。</p><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/M1FhHfY82mY_20.jpg" /></p><p>在接下來的片段中，作者參與了一場戰術性的行動，詳細描述了<strong>團隊協作</strong>的過程，從位置部署到攻擊執行，強調了精準與效率，展現了面對挑戰時的<strong>策略性思考</strong>。</p><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/M1FhHfY82mY_34.jpg" /></p><h2>知識與探索</h2><p>影片深入探討了知識的本質，作者與「索拉」討論，將學習過程比喻為<strong>磨光寶石</strong>，每一層都使表徵更加精煉，並表達了建立一個能理解宇宙萬物的網絡的宏願。這份對知識的<strong>永恆追求</strong>被視為人生的伴侶，而非終點。</p><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/M1FhHfY82mY_44.jpg" /></p><p>隨後，面對山脊崩塌的危險，作者展現了<strong>堅定的決心</strong>，不願回頭，堅持要衝向頂峰。這種不屈不撓的精神，象徵著在困境中追求目標的勇氣。</p><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/M1FhHfY82mY_52.jpg" /></p><p>緊接著，作者踏上了在太平洋西北地區尋找<strong>傳奇生物大腳怪</strong>的冒險。在濃密的森林中，他發現了巨大的腳印，並在觀察到動靜後，立即展開追蹤，展現了探索未知世界的<strong>好奇心與行動力</strong>。</p><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/M1FhHfY82mY_62.jpg" /></p><h2>奇遇與夥伴</h2><p>影片進入奇幻篇章，作者驚訝地發現自己擁有了<strong>飛行能力</strong>。他描述了在空中漂浮的不可思議感，儘管仍在摸索降落技巧，但飛行的體驗卻讓他感到<strong>無比的自由與喜悅</strong>。</p><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/M1FhHfY82mY_73.jpg" /></p><p>一場歡樂的<strong>帽子搶奪戰</strong>隨後展開，一隻調皮的鳥搶走了作者的帽子。在父子的協力下，他們試圖追回，這段插曲為影片增添了輕鬆幽默的氛圍。</p><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/M1FhHfY82mY_88.jpg" /></p><p>關鍵時刻，一隻名為<strong>「火箭」的忠誠狗狗</strong>英勇地幫助作者奪回了帽子，作者對此表達了由衷的感激，甚至稱<strong>火箭</strong>救了他的命，凸顯了人與動物間的<strong>深厚情誼</strong>。</p><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/M1FhHfY82mY_102.jpg" /></p><p>最後，作者與<strong>火箭</strong>一同展開了一段<strong>狂野的騎乘之旅</strong>，感受著<strong>火箭</strong>強大的力量，體驗著驚險刺激的冒險，為整段影片畫上了一個充滿活力和想像力的句號。</p><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/M1FhHfY82mY_109.jpg" /></p><hr /><hr />================================================</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">[Music]. Steady. Damn it. Ready when you are. 3 2 1 go. Dude, you all right? I'm good. [Music]. Ladies and gentlemen,. with a cat. 1 2 3. [Music]. [Music]. </p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">That's a landing. [Music]. Folks keep saying you can't ride a. horse. It's riding a horse. Guess nobody told these two. Steadiest pair I've ever had under me. Cannon bubble. That's cold. Mine. Got it. Up. Up. Cover. Middle. Outside. Outside. I'm on it. Set. Go. </p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">Close. Nice swing. Good job, y'all. [Music]. On steady pace, ease that shield wall. for the north. [Music]. [Applause]. [Music]. [Applause]. [Music]. Affirmative, Sora. Each layer refineses. the representation, like polishing a. </p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">gem. Then maybe one day I'll build a network. that understands every star. We have to turn back. The ridge is. collapsing. No, we pushed to the summit. Now we have. a. [Music]. Give me. [Music]. Knowledge is not a destination. It's a. companion for the road. </p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">And ready, begin. We're out here in the Pacific Northwest. hunting for the legendary Bigfoot. The. forest is perfect for hiding. I think I. just found something. Look at the size. of this print. Wait, did you see that? Something's moving over there. That's. got to be him. I'm going after it. Hey there, traveler. So,. this is kind of wild. Turns out I can. actually fly. Watch. Ready? Look. Totally off the ground. Let's go. </p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">outside. The view up here is insane. I. can just drift around like still. figuring out the landing, but it's the. best feeling ever. Mhm. Come on. Not yet. There it is. That's a pin. Yeah. Yeah. You got. Whoa. Your hat. Hey. Hey. Give that back. Come on. That's mine. You hear me? My hat. </p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">Dad, it's running. Come back here, you feathered thief. You're not going to catch it. Oh, I'm. trying. Rocket. Thank you, Rocket. I owe you my life. Rocket. Whoa. Hang on, buddy. Oh, it's got some. power. Woo! Easy, easy. This is wild. You still. with me? Come on. Yes. [Music]. </p>

<hr style="clear:both" />
=============
<p><a href="https://www.youtube.com/watch?v=M1FhHfY82mY">https://www.youtube.com/watch?v=M1FhHfY82mY</a></p><p>Warning</p><p>All videos below were generated by Sora 2.</p>]]>
      </itunes:summary>
      <description>
        <![CDATA[<hr style="clear:both" />

<p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://img.youtube.com/vi/M1FhHfY82mY/maxresdefault.jpg" /></p><p><a href="https://www.youtube.com/watch?v=M1FhHfY82mY">https://www.youtube.com/watch?v=M1FhHfY82mY</a></p><h1>值得閱讀的理由</h1><ul><li>探索人類挑戰極限和發現奇特能力的多樣面貌。</li><li>體驗一場充滿刺激冒險、溫馨互動與深刻反思的旅程。</li><li>見證團隊合作、堅韌不拔及對知識永無止境的追求。</li></ul><hr /><h1>摘要</h1><h2>挑戰與技巧</h2><p>影片開場以一場驚險的特技或墜落展開，作者在看似危險的情況下展現了<strong>從容不迫</strong>的態度。隨後，作者展示了與一隻貓的互動，暗示著某種表演或協作。接著，作者駁斥了「不能騎馬」的說法，與搭檔一同展現了<strong>精湛而穩定的騎術</strong>，證明了他們的默契與技巧。</p><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/M1FhHfY82mY_20.jpg" /></p><p>在接下來的片段中，作者參與了一場戰術性的行動，詳細描述了<strong>團隊協作</strong>的過程，從位置部署到攻擊執行，強調了精準與效率，展現了面對挑戰時的<strong>策略性思考</strong>。</p><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/M1FhHfY82mY_34.jpg" /></p><h2>知識與探索</h2><p>影片深入探討了知識的本質，作者與「索拉」討論，將學習過程比喻為<strong>磨光寶石</strong>，每一層都使表徵更加精煉，並表達了建立一個能理解宇宙萬物的網絡的宏願。這份對知識的<strong>永恆追求</strong>被視為人生的伴侶，而非終點。</p><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/M1FhHfY82mY_44.jpg" /></p><p>隨後，面對山脊崩塌的危險，作者展現了<strong>堅定的決心</strong>，不願回頭，堅持要衝向頂峰。這種不屈不撓的精神，象徵著在困境中追求目標的勇氣。</p><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/M1FhHfY82mY_52.jpg" /></p><p>緊接著，作者踏上了在太平洋西北地區尋找<strong>傳奇生物大腳怪</strong>的冒險。在濃密的森林中，他發現了巨大的腳印，並在觀察到動靜後，立即展開追蹤，展現了探索未知世界的<strong>好奇心與行動力</strong>。</p><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/M1FhHfY82mY_62.jpg" /></p><h2>奇遇與夥伴</h2><p>影片進入奇幻篇章，作者驚訝地發現自己擁有了<strong>飛行能力</strong>。他描述了在空中漂浮的不可思議感，儘管仍在摸索降落技巧，但飛行的體驗卻讓他感到<strong>無比的自由與喜悅</strong>。</p><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/M1FhHfY82mY_73.jpg" /></p><p>一場歡樂的<strong>帽子搶奪戰</strong>隨後展開，一隻調皮的鳥搶走了作者的帽子。在父子的協力下，他們試圖追回，這段插曲為影片增添了輕鬆幽默的氛圍。</p><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/M1FhHfY82mY_88.jpg" /></p><p>關鍵時刻，一隻名為<strong>「火箭」的忠誠狗狗</strong>英勇地幫助作者奪回了帽子，作者對此表達了由衷的感激，甚至稱<strong>火箭</strong>救了他的命，凸顯了人與動物間的<strong>深厚情誼</strong>。</p><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/M1FhHfY82mY_102.jpg" /></p><p>最後，作者與<strong>火箭</strong>一同展開了一段<strong>狂野的騎乘之旅</strong>，感受著<strong>火箭</strong>強大的力量，體驗著驚險刺激的冒險，為整段影片畫上了一個充滿活力和想像力的句號。</p><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/M1FhHfY82mY_109.jpg" /></p><hr /><hr />================================================</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">[Music]. Steady. Damn it. Ready when you are. 3 2 1 go. Dude, you all right? I'm good. [Music]. Ladies and gentlemen,. with a cat. 1 2 3. [Music]. [Music]. </p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">That's a landing. [Music]. Folks keep saying you can't ride a. horse. It's riding a horse. Guess nobody told these two. Steadiest pair I've ever had under me. Cannon bubble. That's cold. Mine. Got it. Up. Up. Cover. Middle. Outside. Outside. I'm on it. Set. Go. </p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">Close. Nice swing. Good job, y'all. [Music]. On steady pace, ease that shield wall. for the north. [Music]. [Applause]. [Music]. [Applause]. [Music]. Affirmative, Sora. Each layer refineses. the representation, like polishing a. </p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">gem. Then maybe one day I'll build a network. that understands every star. We have to turn back. The ridge is. collapsing. No, we pushed to the summit. Now we have. a. [Music]. Give me. [Music]. Knowledge is not a destination. It's a. companion for the road. </p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">And ready, begin. We're out here in the Pacific Northwest. hunting for the legendary Bigfoot. The. forest is perfect for hiding. I think I. just found something. Look at the size. of this print. Wait, did you see that? Something's moving over there. That's. got to be him. I'm going after it. Hey there, traveler. So,. this is kind of wild. Turns out I can. actually fly. Watch. Ready? Look. Totally off the ground. Let's go. </p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">outside. The view up here is insane. I. can just drift around like still. figuring out the landing, but it's the. best feeling ever. Mhm. Come on. Not yet. There it is. That's a pin. Yeah. Yeah. You got. Whoa. Your hat. Hey. Hey. Give that back. Come on. That's mine. You hear me? My hat. </p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">Dad, it's running. Come back here, you feathered thief. You're not going to catch it. Oh, I'm. trying. Rocket. Thank you, Rocket. I owe you my life. Rocket. Whoa. Hang on, buddy. Oh, it's got some. power. Woo! Easy, easy. This is wild. You still. with me? Come on. Yes. [Music]. </p>

<hr style="clear:both" />
=============
<p><a href="https://www.youtube.com/watch?v=M1FhHfY82mY">https://www.youtube.com/watch?v=M1FhHfY82mY</a></p><p>Warning</p><p>All videos below were generated by Sora 2.</p>]]>
      </description>
      <content:encoded><![CDATA[<hr style="clear:both" />

<p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://img.youtube.com/vi/M1FhHfY82mY/maxresdefault.jpg" /></p><p><a href="https://www.youtube.com/watch?v=M1FhHfY82mY">https://www.youtube.com/watch?v=M1FhHfY82mY</a></p><h1>值得閱讀的理由</h1><ul><li>探索人類挑戰極限和發現奇特能力的多樣面貌。</li><li>體驗一場充滿刺激冒險、溫馨互動與深刻反思的旅程。</li><li>見證團隊合作、堅韌不拔及對知識永無止境的追求。</li></ul><hr /><h1>摘要</h1><h2>挑戰與技巧</h2><p>影片開場以一場驚險的特技或墜落展開，作者在看似危險的情況下展現了<strong>從容不迫</strong>的態度。隨後，作者展示了與一隻貓的互動，暗示著某種表演或協作。接著，作者駁斥了「不能騎馬」的說法，與搭檔一同展現了<strong>精湛而穩定的騎術</strong>，證明了他們的默契與技巧。</p><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/M1FhHfY82mY_20.jpg" /></p><p>在接下來的片段中，作者參與了一場戰術性的行動，詳細描述了<strong>團隊協作</strong>的過程，從位置部署到攻擊執行，強調了精準與效率，展現了面對挑戰時的<strong>策略性思考</strong>。</p><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/M1FhHfY82mY_34.jpg" /></p><h2>知識與探索</h2><p>影片深入探討了知識的本質，作者與「索拉」討論，將學習過程比喻為<strong>磨光寶石</strong>，每一層都使表徵更加精煉，並表達了建立一個能理解宇宙萬物的網絡的宏願。這份對知識的<strong>永恆追求</strong>被視為人生的伴侶，而非終點。</p><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/M1FhHfY82mY_44.jpg" /></p><p>隨後，面對山脊崩塌的危險，作者展現了<strong>堅定的決心</strong>，不願回頭，堅持要衝向頂峰。這種不屈不撓的精神，象徵著在困境中追求目標的勇氣。</p><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/M1FhHfY82mY_52.jpg" /></p><p>緊接著，作者踏上了在太平洋西北地區尋找<strong>傳奇生物大腳怪</strong>的冒險。在濃密的森林中，他發現了巨大的腳印，並在觀察到動靜後，立即展開追蹤，展現了探索未知世界的<strong>好奇心與行動力</strong>。</p><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/M1FhHfY82mY_62.jpg" /></p><h2>奇遇與夥伴</h2><p>影片進入奇幻篇章，作者驚訝地發現自己擁有了<strong>飛行能力</strong>。他描述了在空中漂浮的不可思議感，儘管仍在摸索降落技巧，但飛行的體驗卻讓他感到<strong>無比的自由與喜悅</strong>。</p><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/M1FhHfY82mY_73.jpg" /></p><p>一場歡樂的<strong>帽子搶奪戰</strong>隨後展開，一隻調皮的鳥搶走了作者的帽子。在父子的協力下，他們試圖追回，這段插曲為影片增添了輕鬆幽默的氛圍。</p><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/M1FhHfY82mY_88.jpg" /></p><p>關鍵時刻，一隻名為<strong>「火箭」的忠誠狗狗</strong>英勇地幫助作者奪回了帽子，作者對此表達了由衷的感激，甚至稱<strong>火箭</strong>救了他的命，凸顯了人與動物間的<strong>深厚情誼</strong>。</p><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/M1FhHfY82mY_102.jpg" /></p><p>最後，作者與<strong>火箭</strong>一同展開了一段<strong>狂野的騎乘之旅</strong>，感受著<strong>火箭</strong>強大的力量，體驗著驚險刺激的冒險，為整段影片畫上了一個充滿活力和想像力的句號。</p><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/M1FhHfY82mY_109.jpg" /></p><hr /><hr />================================================</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">[Music]. Steady. Damn it. Ready when you are. 3 2 1 go. Dude, you all right? I'm good. [Music]. Ladies and gentlemen,. with a cat. 1 2 3. [Music]. [Music]. </p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">That's a landing. [Music]. Folks keep saying you can't ride a. horse. It's riding a horse. Guess nobody told these two. Steadiest pair I've ever had under me. Cannon bubble. That's cold. Mine. Got it. Up. Up. Cover. Middle. Outside. Outside. I'm on it. Set. Go. </p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">Close. Nice swing. Good job, y'all. [Music]. On steady pace, ease that shield wall. for the north. [Music]. [Applause]. [Music]. [Applause]. [Music]. Affirmative, Sora. Each layer refineses. the representation, like polishing a. </p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">gem. Then maybe one day I'll build a network. that understands every star. We have to turn back. The ridge is. collapsing. No, we pushed to the summit. Now we have. a. [Music]. Give me. [Music]. Knowledge is not a destination. It's a. companion for the road. </p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">And ready, begin. We're out here in the Pacific Northwest. hunting for the legendary Bigfoot. The. forest is perfect for hiding. I think I. just found something. Look at the size. of this print. Wait, did you see that? Something's moving over there. That's. got to be him. I'm going after it. Hey there, traveler. So,. this is kind of wild. Turns out I can. actually fly. Watch. Ready? Look. Totally off the ground. Let's go. </p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">outside. The view up here is insane. I. can just drift around like still. figuring out the landing, but it's the. best feeling ever. Mhm. Come on. Not yet. There it is. That's a pin. Yeah. Yeah. You got. Whoa. Your hat. Hey. Hey. Give that back. Come on. That's mine. You hear me? My hat. </p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">Dad, it's running. Come back here, you feathered thief. You're not going to catch it. Oh, I'm. trying. Rocket. Thank you, Rocket. I owe you my life. Rocket. Whoa. Hang on, buddy. Oh, it's got some. power. Woo! Easy, easy. This is wild. You still. with me? Come on. Yes. [Music]. </p>

<hr style="clear:both" />
=============
<p><a href="https://www.youtube.com/watch?v=M1FhHfY82mY">https://www.youtube.com/watch?v=M1FhHfY82mY</a></p><p>Warning</p><p>All videos below were generated by Sora 2.</p>]]></content:encoded>
      <itunes:image href="https://i.ytimg.com/vi/M1FhHfY82mY/hqdefault.jpg"/>
      <pubDate>2025-10-02T00:01:09.000Z</pubDate>
    </item><item>
      <title><![CDATA[【人工智能】DeepSeek发布新模型V3.2-Exp | 全新DSA稀疏注意力机制 | 闪电索引器 | 长上下文效率优化 | 细粒度的token选择 | MLA架构 | 密集预热+稀疏训练 | 蒸馏]]></title>
      <link>https://www.youtube.com/watch?v=CA2lzW9INrQ</link>
      <itunes:title><![CDATA[【人工智能】DeepSeek发布新模型V3.2-Exp | 全新DSA稀疏注意力机制 | 闪电索引器 | 长上下文效率优化 | 细粒度的token选择 | MLA架构 | 密集预热+稀疏训练 | 蒸馏]]></itunes:title>
      <itunes:author><![CDATA[最佳拍档]]></itunes:author>
      <itunes:summary>
        <![CDATA[<hr style="clear:both" />

<p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://img.youtube.com/vi/CA2lzW9INrQ/maxresdefault.jpg" /></p><p><a href="https://www.youtube.com/watch?v=CA2lzW9INrQ">https://www.youtube.com/watch?v=CA2lzW9INrQ</a></p><h1>值得閱讀的理由</h1> <ul> <li>深入了解DeepSeek-V3.2-Exp如何透過其獨特的<strong>稀疏注意力機制（DSA）</strong>，在處理<strong>長上下文</strong>任務時實現顯著的效率提升。</li> <li>探索模型的<strong>架構細節</strong>，包括<strong>閃電索引器</strong>與<strong>細粒度token選擇機制</strong>如何協同運作，以及其訓練流程中<strong>密集預熱</strong>與<strong>稀疏訓練</strong>階段的精妙設計。</li> <li>掌握該模型在<strong>能力評估</strong>與<strong>推理成本</strong>上的實際表現，了解DeepSeek團隊如何平衡<strong>性能</strong>與<strong>效率</strong>，並為未來的AI應用提供更具經濟效益的解決方案。</li> </ul> <hr /> <h1>摘要</h1> <p>在大飛的介紹下，DeepSeek團隊推出了其最新實驗性模型<strong>DeepSeek-V3.2-Exp</strong>。該模型在DeepSeek-V3.1-Terminus的基礎上，透過持續訓練引入了全新的<strong>稀疏注意力機制（DeepSeek Sparse Attention, DSA）</strong>。結合高效的<strong>閃電索引器</strong>，V3.2-Exp在訓練和推理效率上實現了顯著提升，尤其在處理<strong>長上下文</strong>場景時，其效率優勢更為突出。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_50.jpg" /></p> <h2>架構分析</h2> <p>DeepSeek-V3.2-Exp的架構核心是DSA，主要包含兩個關鍵組件：<strong>閃電索引器</strong>和<strong>細粒度的token選擇機制</strong>。<strong>閃電索引器</strong>負責計算查詢token與前面token之間的索引分數，並根據分數決定哪些token進行後續注意力計算。為提升計算效率，索引器採用ReLU激活函數，且頭數較少並支援FP8精度，確保其計算成本極低。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_160.jpg" /></p> <p>在索引器篩選出關鍵token後，<strong>細粒度的token選擇機制</strong>會根據索引分數，只挑選出排名前k位的token所對應的鍵值對進行注意力運算。這使得模型無需對所有token都進行計算，大幅降低了計算量。DSA是基於DeepSeek在2024年提出的<strong>MLA（Multi-Query Attention, 多查詢注意力）架構</strong>實現的，以確保與V3.1的訓練連貫性，並支援鍵值對在多個查詢間共享，進一步提升計算效率。詳細的架構和開源實現可供進一步參考。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_320.jpg" /></p> <h2>訓練流程</h2> <p>DeepSeek-V3.2-Exp的訓練始於DeepSeek-V3.1-Terminus的基礎檢查點，其上下文長度已擴展至128K。整個訓練過程分為兩個主要階段：<strong>持續預訓練</strong>和<strong>後訓練</strong>。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_350.jpg" /></p> <h3>持續預訓練階段</h3> <p>持續預訓練包含<strong>密集預熱階段</strong>和<strong>稀疏訓練階段</strong>，兩者使用與V3.1擴展長上下文時相同的數據分佈。在<strong>密集預熱階段</strong>，模型仍採密集注意力計算，只初始化<strong>閃電索引器</strong>，而模型其他參數凍結。此階段的目標是透過最小化索引器輸出與主注意力分佈之間的<strong>KL散度損失</strong>，讓索引器的輸出與主注意力分布保持一致。此階段訓練量約為2.1B個token，旨在短時間內完成索引器初始化。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_495.jpg" /></p> <p>完成預熱後，進入<strong>稀疏訓練階段</strong>。此時模型啟用DSA的稀疏模式，並優化所有參數。KL散度損失函數調整為僅計算經過篩選的token。值得一提的是，索引器的輸入與主模型的計算圖分離，索引器僅依賴KL散度損失優化，而主模型則依據語言建模損失，確保兩者優化過程相互獨立。此階段訓練量約為943.7B個token，確保模型充分適應稀疏模式下的性能。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_640.jpg" /></p> <h3>後訓練階段</h3> <p>後訓練階段旨在打造最終的DeepSeek-V3.2-Exp模型，並保持與稀疏持續預訓練階段相同的稀疏注意力使用方式。為嚴謹評估DSA的影響，後訓練流水線、算法和數據均與DeepSeek-V3.1-Terminus完全相同。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_690.jpg" /></p> <p>此階段包含<strong>專家蒸餾</strong>和<strong>混合RL訓練</strong>。<strong>專家蒸餾</strong>透過從同一預訓練基礎檢查點微調，開發出多個專注於不同領域（如數學、編程、邏輯推理等）的<strong>專家模型</strong>。這些專家模型用於生成兩種訓練數據：<strong>長鏈式推理（思考模式）</strong>和<strong>直接響應生成（非思考模式）</strong>。最終，這些蒸餾後的數據用於訓練DeepSeek-V3.2-Exp模型，使其在多個領域具備接近專家模型的能力，而性能差距可透過後續RL訓練消除。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_840.jpg" /></p> <p><strong>混合RL訓練</strong>階段，DeepSeek-V3.2-Exp採用<strong>分組相對策略優化（GRPO）</strong>，並將推理訓練、智能體訓練和人類對齊訓練<strong>合併為單一RL階段</strong>。這種合併不僅在不同領域間實現了更好的性能平衡，還成功規避了多階段訓練中常見的「<strong>災難性遺忘</strong>」問題。獎勵設計也十分細緻：推理和智能體任務採用基於規則的結果獎勵、長度懲罰和語言一致性獎勵；通用任務則採用生成式獎勵模型及評估標準。獎勵設計著重平衡「長度與準確性」以及「語言一致性與準確性」。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_1060.jpg" /></p> <h2>評估結果</h2> <p>DeepSeek-V3.2-Exp的評估涵蓋模型能力、推理成本及未來驗證計劃。在<strong>模型能力</strong>方面，與DeepSeek-V3.1-Terminus進行了全面基準測試對比。結果顯示，儘管效率顯著提升，V3.2-Exp在短、長上下文任務中均<strong>未出現明顯性能下降</strong>。在部分基準測試中，微小的性能下降主要是由於生成token數量減少，若生成token數與V3.1相近，性能差距便會消失。此外，兩款模型的強化學習訓練曲線趨勢高度接近，證明DSA機制不影響模型的訓練穩定性。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_20.jpg" /></p> <p><strong>推理成本</strong>是V3.2-Exp的核心優勢。理論上，傳統密集注意力的核心計算複雜度為O(L²)，而引入DSA後降至O(L×k)，實現從平方級到線性級的顯著降低。雖然閃電索引器本身的計算複雜度仍為O(L²)，但由於其頭數少且支援FP8精度，實際計算成本極低。在H800 GPU集群上的實際服務測試顯示，DeepSeek-V3.2-Exp的每百萬token成本始終低於DeepSeek-V3.1-Terminus，尤其在128K等<strong>長上下文</strong>場景下，成本優勢更為明顯。為優化短序列處理效率，團隊還引入了掩碼MHA模式。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_24.jpg" /></p> <p>最後，DeepSeek團隊計劃在<strong>真實世界場景中進行更大規模的測試</strong>，以發現並解決稀疏注意力架構可能存在的潛在局限，確保DeepSeek-V3.2-Exp在實際應用中具備更穩定、更出色的表現。總體而言，DeepSeek-V3.2-Exp透過引入DSA，成功在長上下文處理效率上取得突破，同時保持了與前一版本相當的性能水平，為AI任務提供了更高效、更經濟的選擇。</p> <hr /><hr />================================================</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">大家好，这里是最佳拍档，我是大飞。昨天。DeepSeek团队推出了最新模型DeepSeek-V3.2-Exp。在之前 DeepSeek-V3.1-Terminus 的基础上。通过持续训练集成了一种全新的稀疏注意力机制。也就是DeepSeek 稀疏注意力。简称DSA。摇身一变成为了一款实验性的稀疏注意力模型。再搭配上一个高效的闪电索引器。V3.2-Exp在训练和推理两个关键环节都实现了显著的效率提升。尤其是在处理长上下文场景的时候。这种效率优势会更加明显。这期视频我们就从架构、训练、评估这几个核心维度。给大家拆解一下这款模型的细节。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先来看架构部分。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这次DeepSeek-V3.2-Exp在架构上唯一的修改。就是通过持续训练。引入了DSA。那这个DSA到底是由什么构成的呢？它主要包含两个关键组件。一个是闪电索引器。另一个是细粒度的token选择机制。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_62.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">我们先来了解一下闪电索引器。它的核心作用是计算查询token（query token）和前面的某个token之间的索引分数It,s。然后通过这个分数来决定。查询token要选择哪些token进行后续的注意力计算。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这个索引分数的计算公式是这样的。这里面有几个关键参数需要跟大家解释清楚。其中Hᴵ代表的是索引器头（indexer heads）的数量；。qt,jᴵ和wt,jᴵ是一个实数。是从查询token ht中推导出来的；。而ksᴵ是从前面的token hs中推导出来的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">可能有朋友会问。为什么选择ReLU作为激活函数呢？这里主要是出于吞吐量的考虑。ReLU函数在计算效率上有一定的优势。能够更好地配合闪电索引器实现高效计算。而且还有一个很重要的点。闪电索引器的头数比较少。同时还可以用FP8精度来实现。这就使得它的计算效率非常突出。不会因为引入新的组件而导致计算成本大幅增加。这也是后续模型整体效率提升的一个重要基础。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_130.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">有了闪电索引器计算出的索引分数之后。接下来就要靠细粒度的token选择机制来发挥作用了。对于每个查询token ht。这个机制会根据索引分数。只筛选出索引分数排在前k位的token所对应的键值对（key-value entries）。然后，注意力输出ut的计算。就是让查询token ht和这些经过稀疏筛选后的键值对。用{cs}表示，进行注意力机制的运算。具体公式是这样的。这样一来。模型就不需要对所有的token都进行注意力计算。只针对筛选后的关键token进行处理。从而在保证效果的前提下。大幅降低计算量。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">不过，DSA并不是孤立存在的。它是在DeepSeek在2024年提出的MLA架构的基础上实例化来的。为什么要选择这样的方式呢？主要是考虑到DeepSeek-V3.2-Exp是从DeepSeek-V3.1-Terminus进行持续训练得到的。基于MLA来实例化DSA。能够更好地保证训练的连贯性和兼容性。而且在kernel层面。为了提升计算效率。每个键值对都必须在多个查询之间共享。这一点是袁等人在2025年的研究中提到的关键结论。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_203.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">基于这样的需求。研发团队选择了在MLA的MQA模式。也就是多查询注意力（Multi-Query Attention）的基础上实现DSA。这种MQA模式是Transformer作者之一诺姆·沙泽尔（N。Shazeer）在2019年提出的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在这种模式下，MLA中的每个潜在向量。都会在查询token的所有查询头（query heads）之间共享。这样就能很好地满足键值对共享的需求。进一步提升计算效率。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">关于DSA基于MLA的具体架构。大家可以参考原文中的图1。里面清晰地展示了整个注意力架构的细节。尤其是绿色部分。直观地呈现了DSA如何根据索引器筛选出前k个键值对的过程。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">另外。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">为了让大家能够更清晰地了解具体的实现细节。研发团队还提供了DeepSeek-V3.2-Exp的开源实现。大家如果想深入研究代码层面的逻辑。这个地址会非常有帮助。同时。原文的附录A还详细说明了MLA的MQA模式和MHA模式（Multi-Head Attention。多头注意力）之间的区别。感兴趣的朋友也可以去附录部分进一步阅读。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_265.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">讲完了架构，咱们再来看训练流程。DeepSeek-V3.2-Exp的训练是从DeepSeek-V3.1-Terminus的基础checkpoint开始的。这个基础checkpoint的上下文长度已经扩展到了128K。为后续处理长上下文任务打下了很好的基础。整个训练过程分为两个主要阶段。持续预训练和后训练。每个阶段又有各自细分的步骤和目标。咱们一步步来看。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先是持续预训练阶段。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这个阶段又分为两个训练步骤。分别是密集预热阶段和稀疏训练阶段。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">需要先说明的是。这两个阶段所使用的训练数据分布。是完全和DeepSeek-V3.1-Terminus在扩展128K长上下文时所用的数据保持一致的。这样做的目的是为了保证数据的连贯性。减少因数据分布差异对模型训练效果产生的干扰。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第一个步骤是密集预热阶段。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这个阶段的核心目标是初始化闪电索引器。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在这个阶段。模型仍然保持着密集注意力的计算方式。也就是说。此时还没有启用稀疏筛选的机制。同时。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_332.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">除了闪电索引器之外。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">模型的其他所有参数都处于冻结状态。不进行更新。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">那为什么要这么做呢？主要是为了让索引器的输出能够和主注意力的分布保持一致。为后续的稀疏训练做好铺垫。具体怎么实现这种对齐呢？对于第t个查询token。研发团队首先会将主注意力的分数在所有注意力头上进行求和。然后沿着序列维度对这个求和结果进行L1归一化。得到一个目标分布p。然后，基于这个目标分布p。团队设置了KL散度损失作为闪电索引器的训练目标。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">具体的损失函数公式是这样的。其中Softmax函数是为了将索引器计算出的索引分数转换为概率分布。以便和目标分布p进行比较。通过最小化两者之间的KL散度。让索引器的输出尽可能接近主注意力的分布。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在这个预热阶段。所使用的学习率是10的负3次方。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">训练步数非常少，只训练了1000步。每一步包含16个序列。每个序列有128K个token。咱们可以算一下。整个密集预热阶段总共训练的token数量是1000步 × 16序列/步 × 128K token/序列。约等于2.1B个token。这样的训练量能够在短时间内完成索引器的初始化。同时避免过度训练导致其他问题。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_411.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">完成了密集预热之后。就进入到了持续预训练的第二个步骤。稀疏训练阶段。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在这个阶段。研发团队引入了前面提到的细粒度token选择机制。开始让模型适应DSA的稀疏模式。并且此时会对模型的所有参数进行优化更新。而不再是只训练索引器。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在这个阶段。仍然需要保持索引器的输出和主注意力分布的对齐。但和密集预热阶段不同的是。此时只考虑经过筛选后的token集合Sₜ。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这个集合St的定义是所有满足It,s属于It,:中前k个分数的s所构成的集合。因此。KL散度损失函数也相应地进行了调整。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">也就是只计算筛选后的token在目标分布和索引器输出分布之间的KL散度。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这里有一个非常关键的技术细节。研发团队将索引器的输入从计算图中分离了出来。进行单独的优化。这意味着什么呢？简单来说。索引器的训练信号只来自于刚才提到的KL散度损失。而主模型的优化则完全依据语言建模损失（language modeling loss）。两者的优化过程相互独立。这样做能够更好地平衡索引器和主模型的训练。避免相互干扰。保证各自训练目标的实现。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_485.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在稀疏训练阶段。所使用的学习率是7.3×10⁻⁶。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">每个查询token会选择2048个键值对token进行后续的注意力计算。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">训练步数比预热阶段多很多。总共训练了15000步。每一步包含480个序列。每个序列同样是128K个token。咱们再来计算一下这个阶段的总训练token量。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">15000步 × 480序列/步 × 128K token/序列。约为943.7B个token。如此大的训练量能够让模型充分适应稀疏模式。保证模型在稀疏注意力机制下的性能。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">持续预训练完成之后。就进入到了后训练阶段。这个阶段的目标是打造最终的DeepSeek-V3.2-Exp模型。需要强调的是，在后训练阶段。模型仍然采用和稀疏持续预训练阶段相同的方式来使用稀疏注意力。这样可以保证训练过程的一致性。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">为了严谨地评估引入DSA之后对模型的影响。研发团队在DeepSeek-V3.2-Exp的后训练过程中。保持了和DeepSeek-V3.1-Terminus完全相同的后训练流水线、算法以及数据。这样一来。两者在后训练阶段的差异就只来源于是否引入了DSA。从而能够更准确地判断DSA对模型性能和效率的影响。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_565.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">后训练阶段主要包括两个核心环节。分别是专家蒸馏（Specialist Distillation）和混合RL训练（Mixed RL Training）。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">先来看专家蒸馏环节。这个环节的思路是。针对每个特定的任务或领域。先开发一个专门的模型。也就是“专家模型”，这些专家模型都只专注于自己所对应的领域。所有的专家模型都是从同一个预训练的DeepSeek-V3.2基础checkpoint进行微调得到的。这样可以保证专家模型之间的基础能力处于同一水平。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">具体涵盖了哪些领域呢？除了常见的写作任务和通用问答任务之外。还包括五个专门的领域。包括数学、竞赛编程（competitive programming）、通用逻辑推理（general logical reasoning）、智能体编程（agentic coding）以及智能体搜索（agentic search）。每个专家模型的训练都投入了大规模的强化学习（RL）计算资源。确保专家模型在各自领域内能够达到较高的性能水平。而且。为了让训练数据更贴合不同的推理和生成需求。研发团队还使用了不同的模型来生成两种类型的训练数据。一种是用于长链式推理（long chain-of-thought reasoning）的训练数据。对应的是“思考模式”（thinking mode）；。另一种是用于直接响应生成（direct response generation）的训练数据。对应的是“非思考模式”（non-thinking mode）。当这些专家模型训练完成之后。它们就会被用来生成特定领域的数据。这些数据会用于最终DeepSeek-V3.2-Exp模型checkpoint的训练。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_643.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">从实验结果来看。使用这些蒸馏后的数据训练出来的模型。性能只比那些专门的领域专家模型略低一点。而且通过后续的RL训练。这个性能差距能够被有效的消除。这就意味着，通过专家蒸馏。DeepSeek-V3.2-Exp能够在多个领域同时具备接近专家模型的能力。而不需要为每个领域单独训练一个模型。大大提升了模型的通用性和效率。接下来呢。是混合RL训练环节。在DeepSeek V3.2 EXP的后训练中。研发团队仍然采用了分组相对策略优化GRPO作为RL训练的算法。不过，和之前的DeepSeek模型相比。这里有一个重要的改进。那就是之前的模型采用的是多阶段强化学习训练。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而DeepSeek-V3.2-Exp则将推理训练、智能体训练以及人类对齐训练这三个原本分开的阶段合并成了一个RL阶段。这种合并带来了两个显著的好处。一方面。它能够在不同的领域之间实现更好的性能平衡。避免某个领域因为训练阶段的侧重不同而导致性能过强或过弱；。另一方面。它成功规避了多阶段训练范式中常见的“灾难性遗忘”问题。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_715.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">所谓灾难性遗忘。就是模型在学习新任务或新领域知识的时候。会忘记之前已经学会的知识。而单阶段训练则很好地缓解了这个问题。保证了模型在各个领域知识的稳定性。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在奖励设计方面。研发团队也考虑得非常细致。针对不同类型的任务设置了不同的奖励机制。对于推理任务和智能体任务。采用了基于规则的结果奖励（rule-based outcome reward）、长度惩罚（length penalty）以及语言一致性奖励（language consistency reward）。基于规则的结果奖励主要是根据任务的客观结果来判断模型输出是否正确。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如推理任务是否得出正确答案。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">智能体任务是否完成指定目标；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">长度惩罚则是为了避免模型生成过长但无意义的内容。鼓励简洁有效的输出；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">语言一致性奖励则是保证模型输出的语言在逻辑、风格等方面保持一致。提升输出质量。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而对于通用任务。研发团队采用了生成式奖励模型（generative reward model）。并且为每个提示词都设置了专门的评估标准（rubrics）。这种奖励模型能够更灵活地评估通用任务中模型输出的质量。因为通用任务的评价标准往往不像推理或智能体任务那样客观明确。需要更细致的评估维度。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_787.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在整个奖励设计过程中。研发团队重点平衡了两个关键的权衡关系。第一个是“长度与准确性”的权衡。也就是既要避免模型为了追求准确性而生成过长的内容。也要防止为了缩短长度而牺牲准确性；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第二个是“语言一致性与准确性”的权衡。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">即保证语言一致性的同时。不影响模型输出的准确性。确保模型在多个评估维度上都能有出色的表现。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">讲完了训练流程。咱们再来看大家最关心的评估结果。毕竟一款模型的好坏。最终还是要靠实际的评估数据来证明。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">DeepSeek-V3.2-Exp的评估主要从三个方面展开。分别是模型能力、推理成本。以及未来的真实场景验证计划。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先是模型能力的评估。研发团队选择了一系列涵盖不同能力的基准测试（benchmark）。将DeepSeek-V3.2-Exp和DeepSeek-V3.1-Terminus进行了全面的对比。具体的对比数据大家可以参考这张表。从整体结果来看。尽管DeepSeek-V3.2-Exp在长序列处理的计算效率上有了显著提升。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_852.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">但是无论是在短上下文任务还是长上下文任务中。它都没有出现明显的性能下降。这一点非常关键。因为很多时候模型效率的提升往往会伴随着性能的损失。而DeepSeek-V3.2-Exp很好地兼顾了效率和性能。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">不过，在个别基准测试中。DeepSeek-V3.2-Exp的性能确实比DeepSeek-V3.1-Terminus略低一些。比如在GPQA-Diamond（Pass@1）、HLE（Pass@1）以及HMMT 2025（Pass@1）这几项测试中。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">那是什么原因导致了这种性能差异呢？研发团队经过分析发现。主要是因为DeepSeek-V3.2-Exp生成的推理token数量更少。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而有趣的是。当使用那些生成的token数量。与DeepSeek-V3.1-Terminus相近的中间checkpoint进行测试时。这种性能差距就消失了。这说明。性能差异并不是因为DSA机制本身导致的。而是和模型生成内容的长度相关。只要调整好生成token的数量。DeepSeek-V3.2-Exp完全能够达到和之前版本相当的性能水平。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_919.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">除了基准测试的静态结果之外。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">研发团队还对比了两款模型的强化学习训练曲线。其中图（a）是BrowseComp任务的训练曲线。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">图（b）是SWE Verified任务的训练曲线。从曲线可以看出，在整个训练过程中。DeepSeek-V3.1-Terminus和DeepSeek-V3.2-Exp在这两项任务上的准确率都在稳步提升。而且两条曲线的走势非常接近。几乎是同步上升的。同时。图中的虚线代表模型输出的平均token数量。两款模型的输出token数量也保持着相似的变化趋势。这种接近的训练曲线充分说明了DSA机制并没有影响模型的训练稳定性。DeepSeek-V3.2-Exp在训练过程中能够像之前的版本一样稳定地学习和提升性能。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">接下来是推理成本的评估。这也是DeepSeek-V3.2-Exp的核心优势所在。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">咱们先从理论层面来看。DSA机制到底是如何降低推理成本的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在传统的密集注意力机制中。主模型的核心注意力计算复杂度是O(L²)，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_981.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">其中L是序列的长度。这意味着当序列长度大幅增加时。计算量会呈平方级增长。这也是长上下文处理效率低的主要原因之一。而引入DSA之后。核心注意力计算复杂度降低到了O(L×k)，其中k是每个查询token选择的键值对token数量。而且k远小于L（k≪L）。这样一来，当处理长序列时。计算量的增长速度就从平方级变成了线性级。大幅降低了计算成本。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">可能有朋友会问。那闪电索引器的计算复杂度呢？确实。闪电索引器的计算复杂度仍然是O(L²)，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">但是它的计算量和DeepSeek-V3.1-Terminus中MLA的计算量相比。要少得多。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">一方面是因为闪电索引器的头数少。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">另一方面是因为它可以用FP8精度实现。这些都使得闪电索引器的实际计算成本非常低。再加上研发团队针对DSA做的优化实现。最终使得DeepSeek-V3.2-Exp在长上下文场景下实现了显著的端到端加速。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_1045.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">为了让大家更直观地感受到这种成本差异。研发团队在H800 GPU集群上部署了实际的服务。并且对两款模型的推理成本进行了 benchmark 测试。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">测试时GPU的租赁价格按照每小时2美元计算。具体的成本对比数据可以参考这张图。其中图（a）是预填充（Prefilling）阶段的成本。图（b）是解码（Decoding）阶段的成本。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">从图中可以清晰地看到。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">随着序列中token位置的增加。比如从0K到128K。DeepSeek-V3.2-Exp的每百万token成本始终低于DeepSeek-V3.1-Terminus。而且在token位置达到128K这种长上下文场景下。成本优势更加明显。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">另外。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">还有一个细节需要跟大家提一下。在处理短序列预填充的时候。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">研发团队专门实现了一种掩码MHA（masked MHA）模式来模拟DSA的效果。这样做的目的是在短上下文条件下。也能让模型达到更高的效率。避免在短序列场景下因为DSA的稀疏机制反而导致效率下降。充分考虑了不同序列长度场景下的效率优化。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_1107.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">最后，关于未来的验证计划。研发团队也表现出了非常严谨的态度。虽然目前的内部评估结果显示DeepSeek-V3.2-Exp的表现很有前景。但是团队并没有就此止步。而是计划在真实世界场景中进行更大规模的测试。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">为什么要这么做呢？因为实验室环境下的基准测试往往无法完全覆盖真实场景中的各种复杂情况。真实场景中可能会遇到各种意想不到的问题。比如不同类型的长上下文数据、不同的推理任务需求等。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">通过大规模的真实场景测试。研发团队希望能够发现稀疏注意力架构可能存在的潜在局限。从而进一步优化模型。让DeepSeek-V3.2-Exp在实际应用中能够有更稳定、更出色的表现。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">到这里。DeepSeek-V3.2-Exp的核心内容就给大家介绍得差不多了。这款模型通过引入DSA稀疏注意力机制。成功在长上下文处理的效率上实现了突破。同时又保持了和之前版本相当的性能水平。为长上下文AI任务的应用提供了一个更高效、更经济的选择。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_1170.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">无论是架构设计中的细节考量。还是训练过程中的严谨优化。再到评估阶段的全面验证。都能看出DeepSeek-AI团队在这款模型上投入的心血和专业能力。当然。AI模型的发展是一个持续迭代的过程。DeepSeek-V3.2-Exp作为一款实验性模型。未来还有很大的优化空间。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如进一步提升稀疏筛选的准确性、在更多真实场景中验证性能等。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">相信随着后续的不断改进。这款模型会在更多领域发挥重要作用。也期待DeepSeek团队能够带来更多像这样兼顾性能和效率的优秀模型。感谢收看本期视频，我们下期再见。</p>

<hr style="clear:both" />
=============
<p><a href="https://www.youtube.com/watch?v=CA2lzW9INrQ">https://www.youtube.com/watch?v=CA2lzW9INrQ</a></p><p>昨天，DeepSeek团队推出了最新模型DeepSeek-V3.2-Exp，在之前 DeepSeek-V3.1-Terminus 的基础上，通过持续训练集成了一种全新的稀疏注意力机制，也就是DeepSeek 稀疏注意力，简称DSA，摇身一变成为了一款实验性的稀疏注意力模型。再搭配上一个高效的闪电索引器，V3.2-Exp在训练和推理两个关键环节都实现了显著的效率提升，尤其是在处理长上下文场景的时候，这种效率优势会更加明显。这期视频我们就从架构、训练、评估这几个核心维度，给大家拆解一下这款模型的细节。</p><p><a href="https://api-docs.deepseek.com/zh-cn/news/news250929">https://api-docs.deepseek.com/zh-cn/news/news250929</a></p><p><a href="https://github.com/deepseek-ai/DeepSeek-V3.2-Exp/blob/main/DeepSeek_V3_2.pdf">https://github.com/deepseek-ai/DeepSeek-V3.2-Exp/blob/main/DeepSeek_V3_2.pdf</a></p><p><a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Exp/tree/main/inference">https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Exp/tree/main/inference</a></p>]]>
      </itunes:summary>
      <description>
        <![CDATA[<hr style="clear:both" />

<p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://img.youtube.com/vi/CA2lzW9INrQ/maxresdefault.jpg" /></p><p><a href="https://www.youtube.com/watch?v=CA2lzW9INrQ">https://www.youtube.com/watch?v=CA2lzW9INrQ</a></p><h1>值得閱讀的理由</h1> <ul> <li>深入了解DeepSeek-V3.2-Exp如何透過其獨特的<strong>稀疏注意力機制（DSA）</strong>，在處理<strong>長上下文</strong>任務時實現顯著的效率提升。</li> <li>探索模型的<strong>架構細節</strong>，包括<strong>閃電索引器</strong>與<strong>細粒度token選擇機制</strong>如何協同運作，以及其訓練流程中<strong>密集預熱</strong>與<strong>稀疏訓練</strong>階段的精妙設計。</li> <li>掌握該模型在<strong>能力評估</strong>與<strong>推理成本</strong>上的實際表現，了解DeepSeek團隊如何平衡<strong>性能</strong>與<strong>效率</strong>，並為未來的AI應用提供更具經濟效益的解決方案。</li> </ul> <hr /> <h1>摘要</h1> <p>在大飛的介紹下，DeepSeek團隊推出了其最新實驗性模型<strong>DeepSeek-V3.2-Exp</strong>。該模型在DeepSeek-V3.1-Terminus的基礎上，透過持續訓練引入了全新的<strong>稀疏注意力機制（DeepSeek Sparse Attention, DSA）</strong>。結合高效的<strong>閃電索引器</strong>，V3.2-Exp在訓練和推理效率上實現了顯著提升，尤其在處理<strong>長上下文</strong>場景時，其效率優勢更為突出。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_50.jpg" /></p> <h2>架構分析</h2> <p>DeepSeek-V3.2-Exp的架構核心是DSA，主要包含兩個關鍵組件：<strong>閃電索引器</strong>和<strong>細粒度的token選擇機制</strong>。<strong>閃電索引器</strong>負責計算查詢token與前面token之間的索引分數，並根據分數決定哪些token進行後續注意力計算。為提升計算效率，索引器採用ReLU激活函數，且頭數較少並支援FP8精度，確保其計算成本極低。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_160.jpg" /></p> <p>在索引器篩選出關鍵token後，<strong>細粒度的token選擇機制</strong>會根據索引分數，只挑選出排名前k位的token所對應的鍵值對進行注意力運算。這使得模型無需對所有token都進行計算，大幅降低了計算量。DSA是基於DeepSeek在2024年提出的<strong>MLA（Multi-Query Attention, 多查詢注意力）架構</strong>實現的，以確保與V3.1的訓練連貫性，並支援鍵值對在多個查詢間共享，進一步提升計算效率。詳細的架構和開源實現可供進一步參考。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_320.jpg" /></p> <h2>訓練流程</h2> <p>DeepSeek-V3.2-Exp的訓練始於DeepSeek-V3.1-Terminus的基礎檢查點，其上下文長度已擴展至128K。整個訓練過程分為兩個主要階段：<strong>持續預訓練</strong>和<strong>後訓練</strong>。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_350.jpg" /></p> <h3>持續預訓練階段</h3> <p>持續預訓練包含<strong>密集預熱階段</strong>和<strong>稀疏訓練階段</strong>，兩者使用與V3.1擴展長上下文時相同的數據分佈。在<strong>密集預熱階段</strong>，模型仍採密集注意力計算，只初始化<strong>閃電索引器</strong>，而模型其他參數凍結。此階段的目標是透過最小化索引器輸出與主注意力分佈之間的<strong>KL散度損失</strong>，讓索引器的輸出與主注意力分布保持一致。此階段訓練量約為2.1B個token，旨在短時間內完成索引器初始化。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_495.jpg" /></p> <p>完成預熱後，進入<strong>稀疏訓練階段</strong>。此時模型啟用DSA的稀疏模式，並優化所有參數。KL散度損失函數調整為僅計算經過篩選的token。值得一提的是，索引器的輸入與主模型的計算圖分離，索引器僅依賴KL散度損失優化，而主模型則依據語言建模損失，確保兩者優化過程相互獨立。此階段訓練量約為943.7B個token，確保模型充分適應稀疏模式下的性能。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_640.jpg" /></p> <h3>後訓練階段</h3> <p>後訓練階段旨在打造最終的DeepSeek-V3.2-Exp模型，並保持與稀疏持續預訓練階段相同的稀疏注意力使用方式。為嚴謹評估DSA的影響，後訓練流水線、算法和數據均與DeepSeek-V3.1-Terminus完全相同。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_690.jpg" /></p> <p>此階段包含<strong>專家蒸餾</strong>和<strong>混合RL訓練</strong>。<strong>專家蒸餾</strong>透過從同一預訓練基礎檢查點微調，開發出多個專注於不同領域（如數學、編程、邏輯推理等）的<strong>專家模型</strong>。這些專家模型用於生成兩種訓練數據：<strong>長鏈式推理（思考模式）</strong>和<strong>直接響應生成（非思考模式）</strong>。最終，這些蒸餾後的數據用於訓練DeepSeek-V3.2-Exp模型，使其在多個領域具備接近專家模型的能力，而性能差距可透過後續RL訓練消除。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_840.jpg" /></p> <p><strong>混合RL訓練</strong>階段，DeepSeek-V3.2-Exp採用<strong>分組相對策略優化（GRPO）</strong>，並將推理訓練、智能體訓練和人類對齊訓練<strong>合併為單一RL階段</strong>。這種合併不僅在不同領域間實現了更好的性能平衡，還成功規避了多階段訓練中常見的「<strong>災難性遺忘</strong>」問題。獎勵設計也十分細緻：推理和智能體任務採用基於規則的結果獎勵、長度懲罰和語言一致性獎勵；通用任務則採用生成式獎勵模型及評估標準。獎勵設計著重平衡「長度與準確性」以及「語言一致性與準確性」。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_1060.jpg" /></p> <h2>評估結果</h2> <p>DeepSeek-V3.2-Exp的評估涵蓋模型能力、推理成本及未來驗證計劃。在<strong>模型能力</strong>方面，與DeepSeek-V3.1-Terminus進行了全面基準測試對比。結果顯示，儘管效率顯著提升，V3.2-Exp在短、長上下文任務中均<strong>未出現明顯性能下降</strong>。在部分基準測試中，微小的性能下降主要是由於生成token數量減少，若生成token數與V3.1相近，性能差距便會消失。此外，兩款模型的強化學習訓練曲線趨勢高度接近，證明DSA機制不影響模型的訓練穩定性。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_20.jpg" /></p> <p><strong>推理成本</strong>是V3.2-Exp的核心優勢。理論上，傳統密集注意力的核心計算複雜度為O(L²)，而引入DSA後降至O(L×k)，實現從平方級到線性級的顯著降低。雖然閃電索引器本身的計算複雜度仍為O(L²)，但由於其頭數少且支援FP8精度，實際計算成本極低。在H800 GPU集群上的實際服務測試顯示，DeepSeek-V3.2-Exp的每百萬token成本始終低於DeepSeek-V3.1-Terminus，尤其在128K等<strong>長上下文</strong>場景下，成本優勢更為明顯。為優化短序列處理效率，團隊還引入了掩碼MHA模式。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_24.jpg" /></p> <p>最後，DeepSeek團隊計劃在<strong>真實世界場景中進行更大規模的測試</strong>，以發現並解決稀疏注意力架構可能存在的潛在局限，確保DeepSeek-V3.2-Exp在實際應用中具備更穩定、更出色的表現。總體而言，DeepSeek-V3.2-Exp透過引入DSA，成功在長上下文處理效率上取得突破，同時保持了與前一版本相當的性能水平，為AI任務提供了更高效、更經濟的選擇。</p> <hr /><hr />================================================</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">大家好，这里是最佳拍档，我是大飞。昨天。DeepSeek团队推出了最新模型DeepSeek-V3.2-Exp。在之前 DeepSeek-V3.1-Terminus 的基础上。通过持续训练集成了一种全新的稀疏注意力机制。也就是DeepSeek 稀疏注意力。简称DSA。摇身一变成为了一款实验性的稀疏注意力模型。再搭配上一个高效的闪电索引器。V3.2-Exp在训练和推理两个关键环节都实现了显著的效率提升。尤其是在处理长上下文场景的时候。这种效率优势会更加明显。这期视频我们就从架构、训练、评估这几个核心维度。给大家拆解一下这款模型的细节。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先来看架构部分。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这次DeepSeek-V3.2-Exp在架构上唯一的修改。就是通过持续训练。引入了DSA。那这个DSA到底是由什么构成的呢？它主要包含两个关键组件。一个是闪电索引器。另一个是细粒度的token选择机制。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_62.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">我们先来了解一下闪电索引器。它的核心作用是计算查询token（query token）和前面的某个token之间的索引分数It,s。然后通过这个分数来决定。查询token要选择哪些token进行后续的注意力计算。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这个索引分数的计算公式是这样的。这里面有几个关键参数需要跟大家解释清楚。其中Hᴵ代表的是索引器头（indexer heads）的数量；。qt,jᴵ和wt,jᴵ是一个实数。是从查询token ht中推导出来的；。而ksᴵ是从前面的token hs中推导出来的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">可能有朋友会问。为什么选择ReLU作为激活函数呢？这里主要是出于吞吐量的考虑。ReLU函数在计算效率上有一定的优势。能够更好地配合闪电索引器实现高效计算。而且还有一个很重要的点。闪电索引器的头数比较少。同时还可以用FP8精度来实现。这就使得它的计算效率非常突出。不会因为引入新的组件而导致计算成本大幅增加。这也是后续模型整体效率提升的一个重要基础。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_130.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">有了闪电索引器计算出的索引分数之后。接下来就要靠细粒度的token选择机制来发挥作用了。对于每个查询token ht。这个机制会根据索引分数。只筛选出索引分数排在前k位的token所对应的键值对（key-value entries）。然后，注意力输出ut的计算。就是让查询token ht和这些经过稀疏筛选后的键值对。用{cs}表示，进行注意力机制的运算。具体公式是这样的。这样一来。模型就不需要对所有的token都进行注意力计算。只针对筛选后的关键token进行处理。从而在保证效果的前提下。大幅降低计算量。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">不过，DSA并不是孤立存在的。它是在DeepSeek在2024年提出的MLA架构的基础上实例化来的。为什么要选择这样的方式呢？主要是考虑到DeepSeek-V3.2-Exp是从DeepSeek-V3.1-Terminus进行持续训练得到的。基于MLA来实例化DSA。能够更好地保证训练的连贯性和兼容性。而且在kernel层面。为了提升计算效率。每个键值对都必须在多个查询之间共享。这一点是袁等人在2025年的研究中提到的关键结论。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_203.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">基于这样的需求。研发团队选择了在MLA的MQA模式。也就是多查询注意力（Multi-Query Attention）的基础上实现DSA。这种MQA模式是Transformer作者之一诺姆·沙泽尔（N。Shazeer）在2019年提出的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在这种模式下，MLA中的每个潜在向量。都会在查询token的所有查询头（query heads）之间共享。这样就能很好地满足键值对共享的需求。进一步提升计算效率。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">关于DSA基于MLA的具体架构。大家可以参考原文中的图1。里面清晰地展示了整个注意力架构的细节。尤其是绿色部分。直观地呈现了DSA如何根据索引器筛选出前k个键值对的过程。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">另外。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">为了让大家能够更清晰地了解具体的实现细节。研发团队还提供了DeepSeek-V3.2-Exp的开源实现。大家如果想深入研究代码层面的逻辑。这个地址会非常有帮助。同时。原文的附录A还详细说明了MLA的MQA模式和MHA模式（Multi-Head Attention。多头注意力）之间的区别。感兴趣的朋友也可以去附录部分进一步阅读。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_265.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">讲完了架构，咱们再来看训练流程。DeepSeek-V3.2-Exp的训练是从DeepSeek-V3.1-Terminus的基础checkpoint开始的。这个基础checkpoint的上下文长度已经扩展到了128K。为后续处理长上下文任务打下了很好的基础。整个训练过程分为两个主要阶段。持续预训练和后训练。每个阶段又有各自细分的步骤和目标。咱们一步步来看。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先是持续预训练阶段。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这个阶段又分为两个训练步骤。分别是密集预热阶段和稀疏训练阶段。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">需要先说明的是。这两个阶段所使用的训练数据分布。是完全和DeepSeek-V3.1-Terminus在扩展128K长上下文时所用的数据保持一致的。这样做的目的是为了保证数据的连贯性。减少因数据分布差异对模型训练效果产生的干扰。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第一个步骤是密集预热阶段。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这个阶段的核心目标是初始化闪电索引器。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在这个阶段。模型仍然保持着密集注意力的计算方式。也就是说。此时还没有启用稀疏筛选的机制。同时。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_332.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">除了闪电索引器之外。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">模型的其他所有参数都处于冻结状态。不进行更新。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">那为什么要这么做呢？主要是为了让索引器的输出能够和主注意力的分布保持一致。为后续的稀疏训练做好铺垫。具体怎么实现这种对齐呢？对于第t个查询token。研发团队首先会将主注意力的分数在所有注意力头上进行求和。然后沿着序列维度对这个求和结果进行L1归一化。得到一个目标分布p。然后，基于这个目标分布p。团队设置了KL散度损失作为闪电索引器的训练目标。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">具体的损失函数公式是这样的。其中Softmax函数是为了将索引器计算出的索引分数转换为概率分布。以便和目标分布p进行比较。通过最小化两者之间的KL散度。让索引器的输出尽可能接近主注意力的分布。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在这个预热阶段。所使用的学习率是10的负3次方。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">训练步数非常少，只训练了1000步。每一步包含16个序列。每个序列有128K个token。咱们可以算一下。整个密集预热阶段总共训练的token数量是1000步 × 16序列/步 × 128K token/序列。约等于2.1B个token。这样的训练量能够在短时间内完成索引器的初始化。同时避免过度训练导致其他问题。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_411.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">完成了密集预热之后。就进入到了持续预训练的第二个步骤。稀疏训练阶段。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在这个阶段。研发团队引入了前面提到的细粒度token选择机制。开始让模型适应DSA的稀疏模式。并且此时会对模型的所有参数进行优化更新。而不再是只训练索引器。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在这个阶段。仍然需要保持索引器的输出和主注意力分布的对齐。但和密集预热阶段不同的是。此时只考虑经过筛选后的token集合Sₜ。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这个集合St的定义是所有满足It,s属于It,:中前k个分数的s所构成的集合。因此。KL散度损失函数也相应地进行了调整。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">也就是只计算筛选后的token在目标分布和索引器输出分布之间的KL散度。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这里有一个非常关键的技术细节。研发团队将索引器的输入从计算图中分离了出来。进行单独的优化。这意味着什么呢？简单来说。索引器的训练信号只来自于刚才提到的KL散度损失。而主模型的优化则完全依据语言建模损失（language modeling loss）。两者的优化过程相互独立。这样做能够更好地平衡索引器和主模型的训练。避免相互干扰。保证各自训练目标的实现。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_485.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在稀疏训练阶段。所使用的学习率是7.3×10⁻⁶。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">每个查询token会选择2048个键值对token进行后续的注意力计算。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">训练步数比预热阶段多很多。总共训练了15000步。每一步包含480个序列。每个序列同样是128K个token。咱们再来计算一下这个阶段的总训练token量。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">15000步 × 480序列/步 × 128K token/序列。约为943.7B个token。如此大的训练量能够让模型充分适应稀疏模式。保证模型在稀疏注意力机制下的性能。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">持续预训练完成之后。就进入到了后训练阶段。这个阶段的目标是打造最终的DeepSeek-V3.2-Exp模型。需要强调的是，在后训练阶段。模型仍然采用和稀疏持续预训练阶段相同的方式来使用稀疏注意力。这样可以保证训练过程的一致性。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">为了严谨地评估引入DSA之后对模型的影响。研发团队在DeepSeek-V3.2-Exp的后训练过程中。保持了和DeepSeek-V3.1-Terminus完全相同的后训练流水线、算法以及数据。这样一来。两者在后训练阶段的差异就只来源于是否引入了DSA。从而能够更准确地判断DSA对模型性能和效率的影响。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_565.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">后训练阶段主要包括两个核心环节。分别是专家蒸馏（Specialist Distillation）和混合RL训练（Mixed RL Training）。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">先来看专家蒸馏环节。这个环节的思路是。针对每个特定的任务或领域。先开发一个专门的模型。也就是“专家模型”，这些专家模型都只专注于自己所对应的领域。所有的专家模型都是从同一个预训练的DeepSeek-V3.2基础checkpoint进行微调得到的。这样可以保证专家模型之间的基础能力处于同一水平。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">具体涵盖了哪些领域呢？除了常见的写作任务和通用问答任务之外。还包括五个专门的领域。包括数学、竞赛编程（competitive programming）、通用逻辑推理（general logical reasoning）、智能体编程（agentic coding）以及智能体搜索（agentic search）。每个专家模型的训练都投入了大规模的强化学习（RL）计算资源。确保专家模型在各自领域内能够达到较高的性能水平。而且。为了让训练数据更贴合不同的推理和生成需求。研发团队还使用了不同的模型来生成两种类型的训练数据。一种是用于长链式推理（long chain-of-thought reasoning）的训练数据。对应的是“思考模式”（thinking mode）；。另一种是用于直接响应生成（direct response generation）的训练数据。对应的是“非思考模式”（non-thinking mode）。当这些专家模型训练完成之后。它们就会被用来生成特定领域的数据。这些数据会用于最终DeepSeek-V3.2-Exp模型checkpoint的训练。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_643.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">从实验结果来看。使用这些蒸馏后的数据训练出来的模型。性能只比那些专门的领域专家模型略低一点。而且通过后续的RL训练。这个性能差距能够被有效的消除。这就意味着，通过专家蒸馏。DeepSeek-V3.2-Exp能够在多个领域同时具备接近专家模型的能力。而不需要为每个领域单独训练一个模型。大大提升了模型的通用性和效率。接下来呢。是混合RL训练环节。在DeepSeek V3.2 EXP的后训练中。研发团队仍然采用了分组相对策略优化GRPO作为RL训练的算法。不过，和之前的DeepSeek模型相比。这里有一个重要的改进。那就是之前的模型采用的是多阶段强化学习训练。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而DeepSeek-V3.2-Exp则将推理训练、智能体训练以及人类对齐训练这三个原本分开的阶段合并成了一个RL阶段。这种合并带来了两个显著的好处。一方面。它能够在不同的领域之间实现更好的性能平衡。避免某个领域因为训练阶段的侧重不同而导致性能过强或过弱；。另一方面。它成功规避了多阶段训练范式中常见的“灾难性遗忘”问题。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_715.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">所谓灾难性遗忘。就是模型在学习新任务或新领域知识的时候。会忘记之前已经学会的知识。而单阶段训练则很好地缓解了这个问题。保证了模型在各个领域知识的稳定性。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在奖励设计方面。研发团队也考虑得非常细致。针对不同类型的任务设置了不同的奖励机制。对于推理任务和智能体任务。采用了基于规则的结果奖励（rule-based outcome reward）、长度惩罚（length penalty）以及语言一致性奖励（language consistency reward）。基于规则的结果奖励主要是根据任务的客观结果来判断模型输出是否正确。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如推理任务是否得出正确答案。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">智能体任务是否完成指定目标；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">长度惩罚则是为了避免模型生成过长但无意义的内容。鼓励简洁有效的输出；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">语言一致性奖励则是保证模型输出的语言在逻辑、风格等方面保持一致。提升输出质量。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而对于通用任务。研发团队采用了生成式奖励模型（generative reward model）。并且为每个提示词都设置了专门的评估标准（rubrics）。这种奖励模型能够更灵活地评估通用任务中模型输出的质量。因为通用任务的评价标准往往不像推理或智能体任务那样客观明确。需要更细致的评估维度。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_787.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在整个奖励设计过程中。研发团队重点平衡了两个关键的权衡关系。第一个是“长度与准确性”的权衡。也就是既要避免模型为了追求准确性而生成过长的内容。也要防止为了缩短长度而牺牲准确性；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第二个是“语言一致性与准确性”的权衡。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">即保证语言一致性的同时。不影响模型输出的准确性。确保模型在多个评估维度上都能有出色的表现。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">讲完了训练流程。咱们再来看大家最关心的评估结果。毕竟一款模型的好坏。最终还是要靠实际的评估数据来证明。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">DeepSeek-V3.2-Exp的评估主要从三个方面展开。分别是模型能力、推理成本。以及未来的真实场景验证计划。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先是模型能力的评估。研发团队选择了一系列涵盖不同能力的基准测试（benchmark）。将DeepSeek-V3.2-Exp和DeepSeek-V3.1-Terminus进行了全面的对比。具体的对比数据大家可以参考这张表。从整体结果来看。尽管DeepSeek-V3.2-Exp在长序列处理的计算效率上有了显著提升。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_852.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">但是无论是在短上下文任务还是长上下文任务中。它都没有出现明显的性能下降。这一点非常关键。因为很多时候模型效率的提升往往会伴随着性能的损失。而DeepSeek-V3.2-Exp很好地兼顾了效率和性能。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">不过，在个别基准测试中。DeepSeek-V3.2-Exp的性能确实比DeepSeek-V3.1-Terminus略低一些。比如在GPQA-Diamond（Pass@1）、HLE（Pass@1）以及HMMT 2025（Pass@1）这几项测试中。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">那是什么原因导致了这种性能差异呢？研发团队经过分析发现。主要是因为DeepSeek-V3.2-Exp生成的推理token数量更少。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而有趣的是。当使用那些生成的token数量。与DeepSeek-V3.1-Terminus相近的中间checkpoint进行测试时。这种性能差距就消失了。这说明。性能差异并不是因为DSA机制本身导致的。而是和模型生成内容的长度相关。只要调整好生成token的数量。DeepSeek-V3.2-Exp完全能够达到和之前版本相当的性能水平。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_919.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">除了基准测试的静态结果之外。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">研发团队还对比了两款模型的强化学习训练曲线。其中图（a）是BrowseComp任务的训练曲线。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">图（b）是SWE Verified任务的训练曲线。从曲线可以看出，在整个训练过程中。DeepSeek-V3.1-Terminus和DeepSeek-V3.2-Exp在这两项任务上的准确率都在稳步提升。而且两条曲线的走势非常接近。几乎是同步上升的。同时。图中的虚线代表模型输出的平均token数量。两款模型的输出token数量也保持着相似的变化趋势。这种接近的训练曲线充分说明了DSA机制并没有影响模型的训练稳定性。DeepSeek-V3.2-Exp在训练过程中能够像之前的版本一样稳定地学习和提升性能。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">接下来是推理成本的评估。这也是DeepSeek-V3.2-Exp的核心优势所在。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">咱们先从理论层面来看。DSA机制到底是如何降低推理成本的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在传统的密集注意力机制中。主模型的核心注意力计算复杂度是O(L²)，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_981.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">其中L是序列的长度。这意味着当序列长度大幅增加时。计算量会呈平方级增长。这也是长上下文处理效率低的主要原因之一。而引入DSA之后。核心注意力计算复杂度降低到了O(L×k)，其中k是每个查询token选择的键值对token数量。而且k远小于L（k≪L）。这样一来，当处理长序列时。计算量的增长速度就从平方级变成了线性级。大幅降低了计算成本。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">可能有朋友会问。那闪电索引器的计算复杂度呢？确实。闪电索引器的计算复杂度仍然是O(L²)，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">但是它的计算量和DeepSeek-V3.1-Terminus中MLA的计算量相比。要少得多。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">一方面是因为闪电索引器的头数少。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">另一方面是因为它可以用FP8精度实现。这些都使得闪电索引器的实际计算成本非常低。再加上研发团队针对DSA做的优化实现。最终使得DeepSeek-V3.2-Exp在长上下文场景下实现了显著的端到端加速。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_1045.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">为了让大家更直观地感受到这种成本差异。研发团队在H800 GPU集群上部署了实际的服务。并且对两款模型的推理成本进行了 benchmark 测试。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">测试时GPU的租赁价格按照每小时2美元计算。具体的成本对比数据可以参考这张图。其中图（a）是预填充（Prefilling）阶段的成本。图（b）是解码（Decoding）阶段的成本。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">从图中可以清晰地看到。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">随着序列中token位置的增加。比如从0K到128K。DeepSeek-V3.2-Exp的每百万token成本始终低于DeepSeek-V3.1-Terminus。而且在token位置达到128K这种长上下文场景下。成本优势更加明显。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">另外。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">还有一个细节需要跟大家提一下。在处理短序列预填充的时候。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">研发团队专门实现了一种掩码MHA（masked MHA）模式来模拟DSA的效果。这样做的目的是在短上下文条件下。也能让模型达到更高的效率。避免在短序列场景下因为DSA的稀疏机制反而导致效率下降。充分考虑了不同序列长度场景下的效率优化。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_1107.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">最后，关于未来的验证计划。研发团队也表现出了非常严谨的态度。虽然目前的内部评估结果显示DeepSeek-V3.2-Exp的表现很有前景。但是团队并没有就此止步。而是计划在真实世界场景中进行更大规模的测试。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">为什么要这么做呢？因为实验室环境下的基准测试往往无法完全覆盖真实场景中的各种复杂情况。真实场景中可能会遇到各种意想不到的问题。比如不同类型的长上下文数据、不同的推理任务需求等。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">通过大规模的真实场景测试。研发团队希望能够发现稀疏注意力架构可能存在的潜在局限。从而进一步优化模型。让DeepSeek-V3.2-Exp在实际应用中能够有更稳定、更出色的表现。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">到这里。DeepSeek-V3.2-Exp的核心内容就给大家介绍得差不多了。这款模型通过引入DSA稀疏注意力机制。成功在长上下文处理的效率上实现了突破。同时又保持了和之前版本相当的性能水平。为长上下文AI任务的应用提供了一个更高效、更经济的选择。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_1170.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">无论是架构设计中的细节考量。还是训练过程中的严谨优化。再到评估阶段的全面验证。都能看出DeepSeek-AI团队在这款模型上投入的心血和专业能力。当然。AI模型的发展是一个持续迭代的过程。DeepSeek-V3.2-Exp作为一款实验性模型。未来还有很大的优化空间。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如进一步提升稀疏筛选的准确性、在更多真实场景中验证性能等。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">相信随着后续的不断改进。这款模型会在更多领域发挥重要作用。也期待DeepSeek团队能够带来更多像这样兼顾性能和效率的优秀模型。感谢收看本期视频，我们下期再见。</p>

<hr style="clear:both" />
=============
<p><a href="https://www.youtube.com/watch?v=CA2lzW9INrQ">https://www.youtube.com/watch?v=CA2lzW9INrQ</a></p><p>昨天，DeepSeek团队推出了最新模型DeepSeek-V3.2-Exp，在之前 DeepSeek-V3.1-Terminus 的基础上，通过持续训练集成了一种全新的稀疏注意力机制，也就是DeepSeek 稀疏注意力，简称DSA，摇身一变成为了一款实验性的稀疏注意力模型。再搭配上一个高效的闪电索引器，V3.2-Exp在训练和推理两个关键环节都实现了显著的效率提升，尤其是在处理长上下文场景的时候，这种效率优势会更加明显。这期视频我们就从架构、训练、评估这几个核心维度，给大家拆解一下这款模型的细节。</p><p><a href="https://api-docs.deepseek.com/zh-cn/news/news250929">https://api-docs.deepseek.com/zh-cn/news/news250929</a></p><p><a href="https://github.com/deepseek-ai/DeepSeek-V3.2-Exp/blob/main/DeepSeek_V3_2.pdf">https://github.com/deepseek-ai/DeepSeek-V3.2-Exp/blob/main/DeepSeek_V3_2.pdf</a></p><p><a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Exp/tree/main/inference">https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Exp/tree/main/inference</a></p>]]>
      </description>
      <content:encoded><![CDATA[<hr style="clear:both" />

<p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://img.youtube.com/vi/CA2lzW9INrQ/maxresdefault.jpg" /></p><p><a href="https://www.youtube.com/watch?v=CA2lzW9INrQ">https://www.youtube.com/watch?v=CA2lzW9INrQ</a></p><h1>值得閱讀的理由</h1> <ul> <li>深入了解DeepSeek-V3.2-Exp如何透過其獨特的<strong>稀疏注意力機制（DSA）</strong>，在處理<strong>長上下文</strong>任務時實現顯著的效率提升。</li> <li>探索模型的<strong>架構細節</strong>，包括<strong>閃電索引器</strong>與<strong>細粒度token選擇機制</strong>如何協同運作，以及其訓練流程中<strong>密集預熱</strong>與<strong>稀疏訓練</strong>階段的精妙設計。</li> <li>掌握該模型在<strong>能力評估</strong>與<strong>推理成本</strong>上的實際表現，了解DeepSeek團隊如何平衡<strong>性能</strong>與<strong>效率</strong>，並為未來的AI應用提供更具經濟效益的解決方案。</li> </ul> <hr /> <h1>摘要</h1> <p>在大飛的介紹下，DeepSeek團隊推出了其最新實驗性模型<strong>DeepSeek-V3.2-Exp</strong>。該模型在DeepSeek-V3.1-Terminus的基礎上，透過持續訓練引入了全新的<strong>稀疏注意力機制（DeepSeek Sparse Attention, DSA）</strong>。結合高效的<strong>閃電索引器</strong>，V3.2-Exp在訓練和推理效率上實現了顯著提升，尤其在處理<strong>長上下文</strong>場景時，其效率優勢更為突出。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_50.jpg" /></p> <h2>架構分析</h2> <p>DeepSeek-V3.2-Exp的架構核心是DSA，主要包含兩個關鍵組件：<strong>閃電索引器</strong>和<strong>細粒度的token選擇機制</strong>。<strong>閃電索引器</strong>負責計算查詢token與前面token之間的索引分數，並根據分數決定哪些token進行後續注意力計算。為提升計算效率，索引器採用ReLU激活函數，且頭數較少並支援FP8精度，確保其計算成本極低。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_160.jpg" /></p> <p>在索引器篩選出關鍵token後，<strong>細粒度的token選擇機制</strong>會根據索引分數，只挑選出排名前k位的token所對應的鍵值對進行注意力運算。這使得模型無需對所有token都進行計算，大幅降低了計算量。DSA是基於DeepSeek在2024年提出的<strong>MLA（Multi-Query Attention, 多查詢注意力）架構</strong>實現的，以確保與V3.1的訓練連貫性，並支援鍵值對在多個查詢間共享，進一步提升計算效率。詳細的架構和開源實現可供進一步參考。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_320.jpg" /></p> <h2>訓練流程</h2> <p>DeepSeek-V3.2-Exp的訓練始於DeepSeek-V3.1-Terminus的基礎檢查點，其上下文長度已擴展至128K。整個訓練過程分為兩個主要階段：<strong>持續預訓練</strong>和<strong>後訓練</strong>。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_350.jpg" /></p> <h3>持續預訓練階段</h3> <p>持續預訓練包含<strong>密集預熱階段</strong>和<strong>稀疏訓練階段</strong>，兩者使用與V3.1擴展長上下文時相同的數據分佈。在<strong>密集預熱階段</strong>，模型仍採密集注意力計算，只初始化<strong>閃電索引器</strong>，而模型其他參數凍結。此階段的目標是透過最小化索引器輸出與主注意力分佈之間的<strong>KL散度損失</strong>，讓索引器的輸出與主注意力分布保持一致。此階段訓練量約為2.1B個token，旨在短時間內完成索引器初始化。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_495.jpg" /></p> <p>完成預熱後，進入<strong>稀疏訓練階段</strong>。此時模型啟用DSA的稀疏模式，並優化所有參數。KL散度損失函數調整為僅計算經過篩選的token。值得一提的是，索引器的輸入與主模型的計算圖分離，索引器僅依賴KL散度損失優化，而主模型則依據語言建模損失，確保兩者優化過程相互獨立。此階段訓練量約為943.7B個token，確保模型充分適應稀疏模式下的性能。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_640.jpg" /></p> <h3>後訓練階段</h3> <p>後訓練階段旨在打造最終的DeepSeek-V3.2-Exp模型，並保持與稀疏持續預訓練階段相同的稀疏注意力使用方式。為嚴謹評估DSA的影響，後訓練流水線、算法和數據均與DeepSeek-V3.1-Terminus完全相同。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_690.jpg" /></p> <p>此階段包含<strong>專家蒸餾</strong>和<strong>混合RL訓練</strong>。<strong>專家蒸餾</strong>透過從同一預訓練基礎檢查點微調，開發出多個專注於不同領域（如數學、編程、邏輯推理等）的<strong>專家模型</strong>。這些專家模型用於生成兩種訓練數據：<strong>長鏈式推理（思考模式）</strong>和<strong>直接響應生成（非思考模式）</strong>。最終，這些蒸餾後的數據用於訓練DeepSeek-V3.2-Exp模型，使其在多個領域具備接近專家模型的能力，而性能差距可透過後續RL訓練消除。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_840.jpg" /></p> <p><strong>混合RL訓練</strong>階段，DeepSeek-V3.2-Exp採用<strong>分組相對策略優化（GRPO）</strong>，並將推理訓練、智能體訓練和人類對齊訓練<strong>合併為單一RL階段</strong>。這種合併不僅在不同領域間實現了更好的性能平衡，還成功規避了多階段訓練中常見的「<strong>災難性遺忘</strong>」問題。獎勵設計也十分細緻：推理和智能體任務採用基於規則的結果獎勵、長度懲罰和語言一致性獎勵；通用任務則採用生成式獎勵模型及評估標準。獎勵設計著重平衡「長度與準確性」以及「語言一致性與準確性」。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_1060.jpg" /></p> <h2>評估結果</h2> <p>DeepSeek-V3.2-Exp的評估涵蓋模型能力、推理成本及未來驗證計劃。在<strong>模型能力</strong>方面，與DeepSeek-V3.1-Terminus進行了全面基準測試對比。結果顯示，儘管效率顯著提升，V3.2-Exp在短、長上下文任務中均<strong>未出現明顯性能下降</strong>。在部分基準測試中，微小的性能下降主要是由於生成token數量減少，若生成token數與V3.1相近，性能差距便會消失。此外，兩款模型的強化學習訓練曲線趨勢高度接近，證明DSA機制不影響模型的訓練穩定性。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_20.jpg" /></p> <p><strong>推理成本</strong>是V3.2-Exp的核心優勢。理論上，傳統密集注意力的核心計算複雜度為O(L²)，而引入DSA後降至O(L×k)，實現從平方級到線性級的顯著降低。雖然閃電索引器本身的計算複雜度仍為O(L²)，但由於其頭數少且支援FP8精度，實際計算成本極低。在H800 GPU集群上的實際服務測試顯示，DeepSeek-V3.2-Exp的每百萬token成本始終低於DeepSeek-V3.1-Terminus，尤其在128K等<strong>長上下文</strong>場景下，成本優勢更為明顯。為優化短序列處理效率，團隊還引入了掩碼MHA模式。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_24.jpg" /></p> <p>最後，DeepSeek團隊計劃在<strong>真實世界場景中進行更大規模的測試</strong>，以發現並解決稀疏注意力架構可能存在的潛在局限，確保DeepSeek-V3.2-Exp在實際應用中具備更穩定、更出色的表現。總體而言，DeepSeek-V3.2-Exp透過引入DSA，成功在長上下文處理效率上取得突破，同時保持了與前一版本相當的性能水平，為AI任務提供了更高效、更經濟的選擇。</p> <hr /><hr />================================================</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">大家好，这里是最佳拍档，我是大飞。昨天。DeepSeek团队推出了最新模型DeepSeek-V3.2-Exp。在之前 DeepSeek-V3.1-Terminus 的基础上。通过持续训练集成了一种全新的稀疏注意力机制。也就是DeepSeek 稀疏注意力。简称DSA。摇身一变成为了一款实验性的稀疏注意力模型。再搭配上一个高效的闪电索引器。V3.2-Exp在训练和推理两个关键环节都实现了显著的效率提升。尤其是在处理长上下文场景的时候。这种效率优势会更加明显。这期视频我们就从架构、训练、评估这几个核心维度。给大家拆解一下这款模型的细节。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先来看架构部分。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这次DeepSeek-V3.2-Exp在架构上唯一的修改。就是通过持续训练。引入了DSA。那这个DSA到底是由什么构成的呢？它主要包含两个关键组件。一个是闪电索引器。另一个是细粒度的token选择机制。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_62.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">我们先来了解一下闪电索引器。它的核心作用是计算查询token（query token）和前面的某个token之间的索引分数It,s。然后通过这个分数来决定。查询token要选择哪些token进行后续的注意力计算。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这个索引分数的计算公式是这样的。这里面有几个关键参数需要跟大家解释清楚。其中Hᴵ代表的是索引器头（indexer heads）的数量；。qt,jᴵ和wt,jᴵ是一个实数。是从查询token ht中推导出来的；。而ksᴵ是从前面的token hs中推导出来的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">可能有朋友会问。为什么选择ReLU作为激活函数呢？这里主要是出于吞吐量的考虑。ReLU函数在计算效率上有一定的优势。能够更好地配合闪电索引器实现高效计算。而且还有一个很重要的点。闪电索引器的头数比较少。同时还可以用FP8精度来实现。这就使得它的计算效率非常突出。不会因为引入新的组件而导致计算成本大幅增加。这也是后续模型整体效率提升的一个重要基础。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_130.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">有了闪电索引器计算出的索引分数之后。接下来就要靠细粒度的token选择机制来发挥作用了。对于每个查询token ht。这个机制会根据索引分数。只筛选出索引分数排在前k位的token所对应的键值对（key-value entries）。然后，注意力输出ut的计算。就是让查询token ht和这些经过稀疏筛选后的键值对。用{cs}表示，进行注意力机制的运算。具体公式是这样的。这样一来。模型就不需要对所有的token都进行注意力计算。只针对筛选后的关键token进行处理。从而在保证效果的前提下。大幅降低计算量。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">不过，DSA并不是孤立存在的。它是在DeepSeek在2024年提出的MLA架构的基础上实例化来的。为什么要选择这样的方式呢？主要是考虑到DeepSeek-V3.2-Exp是从DeepSeek-V3.1-Terminus进行持续训练得到的。基于MLA来实例化DSA。能够更好地保证训练的连贯性和兼容性。而且在kernel层面。为了提升计算效率。每个键值对都必须在多个查询之间共享。这一点是袁等人在2025年的研究中提到的关键结论。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_203.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">基于这样的需求。研发团队选择了在MLA的MQA模式。也就是多查询注意力（Multi-Query Attention）的基础上实现DSA。这种MQA模式是Transformer作者之一诺姆·沙泽尔（N。Shazeer）在2019年提出的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在这种模式下，MLA中的每个潜在向量。都会在查询token的所有查询头（query heads）之间共享。这样就能很好地满足键值对共享的需求。进一步提升计算效率。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">关于DSA基于MLA的具体架构。大家可以参考原文中的图1。里面清晰地展示了整个注意力架构的细节。尤其是绿色部分。直观地呈现了DSA如何根据索引器筛选出前k个键值对的过程。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">另外。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">为了让大家能够更清晰地了解具体的实现细节。研发团队还提供了DeepSeek-V3.2-Exp的开源实现。大家如果想深入研究代码层面的逻辑。这个地址会非常有帮助。同时。原文的附录A还详细说明了MLA的MQA模式和MHA模式（Multi-Head Attention。多头注意力）之间的区别。感兴趣的朋友也可以去附录部分进一步阅读。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_265.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">讲完了架构，咱们再来看训练流程。DeepSeek-V3.2-Exp的训练是从DeepSeek-V3.1-Terminus的基础checkpoint开始的。这个基础checkpoint的上下文长度已经扩展到了128K。为后续处理长上下文任务打下了很好的基础。整个训练过程分为两个主要阶段。持续预训练和后训练。每个阶段又有各自细分的步骤和目标。咱们一步步来看。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先是持续预训练阶段。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这个阶段又分为两个训练步骤。分别是密集预热阶段和稀疏训练阶段。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">需要先说明的是。这两个阶段所使用的训练数据分布。是完全和DeepSeek-V3.1-Terminus在扩展128K长上下文时所用的数据保持一致的。这样做的目的是为了保证数据的连贯性。减少因数据分布差异对模型训练效果产生的干扰。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第一个步骤是密集预热阶段。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这个阶段的核心目标是初始化闪电索引器。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在这个阶段。模型仍然保持着密集注意力的计算方式。也就是说。此时还没有启用稀疏筛选的机制。同时。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_332.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">除了闪电索引器之外。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">模型的其他所有参数都处于冻结状态。不进行更新。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">那为什么要这么做呢？主要是为了让索引器的输出能够和主注意力的分布保持一致。为后续的稀疏训练做好铺垫。具体怎么实现这种对齐呢？对于第t个查询token。研发团队首先会将主注意力的分数在所有注意力头上进行求和。然后沿着序列维度对这个求和结果进行L1归一化。得到一个目标分布p。然后，基于这个目标分布p。团队设置了KL散度损失作为闪电索引器的训练目标。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">具体的损失函数公式是这样的。其中Softmax函数是为了将索引器计算出的索引分数转换为概率分布。以便和目标分布p进行比较。通过最小化两者之间的KL散度。让索引器的输出尽可能接近主注意力的分布。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在这个预热阶段。所使用的学习率是10的负3次方。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">训练步数非常少，只训练了1000步。每一步包含16个序列。每个序列有128K个token。咱们可以算一下。整个密集预热阶段总共训练的token数量是1000步 × 16序列/步 × 128K token/序列。约等于2.1B个token。这样的训练量能够在短时间内完成索引器的初始化。同时避免过度训练导致其他问题。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_411.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">完成了密集预热之后。就进入到了持续预训练的第二个步骤。稀疏训练阶段。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在这个阶段。研发团队引入了前面提到的细粒度token选择机制。开始让模型适应DSA的稀疏模式。并且此时会对模型的所有参数进行优化更新。而不再是只训练索引器。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在这个阶段。仍然需要保持索引器的输出和主注意力分布的对齐。但和密集预热阶段不同的是。此时只考虑经过筛选后的token集合Sₜ。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这个集合St的定义是所有满足It,s属于It,:中前k个分数的s所构成的集合。因此。KL散度损失函数也相应地进行了调整。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">也就是只计算筛选后的token在目标分布和索引器输出分布之间的KL散度。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这里有一个非常关键的技术细节。研发团队将索引器的输入从计算图中分离了出来。进行单独的优化。这意味着什么呢？简单来说。索引器的训练信号只来自于刚才提到的KL散度损失。而主模型的优化则完全依据语言建模损失（language modeling loss）。两者的优化过程相互独立。这样做能够更好地平衡索引器和主模型的训练。避免相互干扰。保证各自训练目标的实现。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_485.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在稀疏训练阶段。所使用的学习率是7.3×10⁻⁶。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">每个查询token会选择2048个键值对token进行后续的注意力计算。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">训练步数比预热阶段多很多。总共训练了15000步。每一步包含480个序列。每个序列同样是128K个token。咱们再来计算一下这个阶段的总训练token量。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">15000步 × 480序列/步 × 128K token/序列。约为943.7B个token。如此大的训练量能够让模型充分适应稀疏模式。保证模型在稀疏注意力机制下的性能。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">持续预训练完成之后。就进入到了后训练阶段。这个阶段的目标是打造最终的DeepSeek-V3.2-Exp模型。需要强调的是，在后训练阶段。模型仍然采用和稀疏持续预训练阶段相同的方式来使用稀疏注意力。这样可以保证训练过程的一致性。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">为了严谨地评估引入DSA之后对模型的影响。研发团队在DeepSeek-V3.2-Exp的后训练过程中。保持了和DeepSeek-V3.1-Terminus完全相同的后训练流水线、算法以及数据。这样一来。两者在后训练阶段的差异就只来源于是否引入了DSA。从而能够更准确地判断DSA对模型性能和效率的影响。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_565.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">后训练阶段主要包括两个核心环节。分别是专家蒸馏（Specialist Distillation）和混合RL训练（Mixed RL Training）。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">先来看专家蒸馏环节。这个环节的思路是。针对每个特定的任务或领域。先开发一个专门的模型。也就是“专家模型”，这些专家模型都只专注于自己所对应的领域。所有的专家模型都是从同一个预训练的DeepSeek-V3.2基础checkpoint进行微调得到的。这样可以保证专家模型之间的基础能力处于同一水平。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">具体涵盖了哪些领域呢？除了常见的写作任务和通用问答任务之外。还包括五个专门的领域。包括数学、竞赛编程（competitive programming）、通用逻辑推理（general logical reasoning）、智能体编程（agentic coding）以及智能体搜索（agentic search）。每个专家模型的训练都投入了大规模的强化学习（RL）计算资源。确保专家模型在各自领域内能够达到较高的性能水平。而且。为了让训练数据更贴合不同的推理和生成需求。研发团队还使用了不同的模型来生成两种类型的训练数据。一种是用于长链式推理（long chain-of-thought reasoning）的训练数据。对应的是“思考模式”（thinking mode）；。另一种是用于直接响应生成（direct response generation）的训练数据。对应的是“非思考模式”（non-thinking mode）。当这些专家模型训练完成之后。它们就会被用来生成特定领域的数据。这些数据会用于最终DeepSeek-V3.2-Exp模型checkpoint的训练。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_643.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">从实验结果来看。使用这些蒸馏后的数据训练出来的模型。性能只比那些专门的领域专家模型略低一点。而且通过后续的RL训练。这个性能差距能够被有效的消除。这就意味着，通过专家蒸馏。DeepSeek-V3.2-Exp能够在多个领域同时具备接近专家模型的能力。而不需要为每个领域单独训练一个模型。大大提升了模型的通用性和效率。接下来呢。是混合RL训练环节。在DeepSeek V3.2 EXP的后训练中。研发团队仍然采用了分组相对策略优化GRPO作为RL训练的算法。不过，和之前的DeepSeek模型相比。这里有一个重要的改进。那就是之前的模型采用的是多阶段强化学习训练。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而DeepSeek-V3.2-Exp则将推理训练、智能体训练以及人类对齐训练这三个原本分开的阶段合并成了一个RL阶段。这种合并带来了两个显著的好处。一方面。它能够在不同的领域之间实现更好的性能平衡。避免某个领域因为训练阶段的侧重不同而导致性能过强或过弱；。另一方面。它成功规避了多阶段训练范式中常见的“灾难性遗忘”问题。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_715.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">所谓灾难性遗忘。就是模型在学习新任务或新领域知识的时候。会忘记之前已经学会的知识。而单阶段训练则很好地缓解了这个问题。保证了模型在各个领域知识的稳定性。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在奖励设计方面。研发团队也考虑得非常细致。针对不同类型的任务设置了不同的奖励机制。对于推理任务和智能体任务。采用了基于规则的结果奖励（rule-based outcome reward）、长度惩罚（length penalty）以及语言一致性奖励（language consistency reward）。基于规则的结果奖励主要是根据任务的客观结果来判断模型输出是否正确。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如推理任务是否得出正确答案。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">智能体任务是否完成指定目标；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">长度惩罚则是为了避免模型生成过长但无意义的内容。鼓励简洁有效的输出；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">语言一致性奖励则是保证模型输出的语言在逻辑、风格等方面保持一致。提升输出质量。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而对于通用任务。研发团队采用了生成式奖励模型（generative reward model）。并且为每个提示词都设置了专门的评估标准（rubrics）。这种奖励模型能够更灵活地评估通用任务中模型输出的质量。因为通用任务的评价标准往往不像推理或智能体任务那样客观明确。需要更细致的评估维度。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_787.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在整个奖励设计过程中。研发团队重点平衡了两个关键的权衡关系。第一个是“长度与准确性”的权衡。也就是既要避免模型为了追求准确性而生成过长的内容。也要防止为了缩短长度而牺牲准确性；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第二个是“语言一致性与准确性”的权衡。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">即保证语言一致性的同时。不影响模型输出的准确性。确保模型在多个评估维度上都能有出色的表现。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">讲完了训练流程。咱们再来看大家最关心的评估结果。毕竟一款模型的好坏。最终还是要靠实际的评估数据来证明。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">DeepSeek-V3.2-Exp的评估主要从三个方面展开。分别是模型能力、推理成本。以及未来的真实场景验证计划。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先是模型能力的评估。研发团队选择了一系列涵盖不同能力的基准测试（benchmark）。将DeepSeek-V3.2-Exp和DeepSeek-V3.1-Terminus进行了全面的对比。具体的对比数据大家可以参考这张表。从整体结果来看。尽管DeepSeek-V3.2-Exp在长序列处理的计算效率上有了显著提升。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_852.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">但是无论是在短上下文任务还是长上下文任务中。它都没有出现明显的性能下降。这一点非常关键。因为很多时候模型效率的提升往往会伴随着性能的损失。而DeepSeek-V3.2-Exp很好地兼顾了效率和性能。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">不过，在个别基准测试中。DeepSeek-V3.2-Exp的性能确实比DeepSeek-V3.1-Terminus略低一些。比如在GPQA-Diamond（Pass@1）、HLE（Pass@1）以及HMMT 2025（Pass@1）这几项测试中。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">那是什么原因导致了这种性能差异呢？研发团队经过分析发现。主要是因为DeepSeek-V3.2-Exp生成的推理token数量更少。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而有趣的是。当使用那些生成的token数量。与DeepSeek-V3.1-Terminus相近的中间checkpoint进行测试时。这种性能差距就消失了。这说明。性能差异并不是因为DSA机制本身导致的。而是和模型生成内容的长度相关。只要调整好生成token的数量。DeepSeek-V3.2-Exp完全能够达到和之前版本相当的性能水平。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_919.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">除了基准测试的静态结果之外。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">研发团队还对比了两款模型的强化学习训练曲线。其中图（a）是BrowseComp任务的训练曲线。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">图（b）是SWE Verified任务的训练曲线。从曲线可以看出，在整个训练过程中。DeepSeek-V3.1-Terminus和DeepSeek-V3.2-Exp在这两项任务上的准确率都在稳步提升。而且两条曲线的走势非常接近。几乎是同步上升的。同时。图中的虚线代表模型输出的平均token数量。两款模型的输出token数量也保持着相似的变化趋势。这种接近的训练曲线充分说明了DSA机制并没有影响模型的训练稳定性。DeepSeek-V3.2-Exp在训练过程中能够像之前的版本一样稳定地学习和提升性能。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">接下来是推理成本的评估。这也是DeepSeek-V3.2-Exp的核心优势所在。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">咱们先从理论层面来看。DSA机制到底是如何降低推理成本的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在传统的密集注意力机制中。主模型的核心注意力计算复杂度是O(L²)，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_981.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">其中L是序列的长度。这意味着当序列长度大幅增加时。计算量会呈平方级增长。这也是长上下文处理效率低的主要原因之一。而引入DSA之后。核心注意力计算复杂度降低到了O(L×k)，其中k是每个查询token选择的键值对token数量。而且k远小于L（k≪L）。这样一来，当处理长序列时。计算量的增长速度就从平方级变成了线性级。大幅降低了计算成本。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">可能有朋友会问。那闪电索引器的计算复杂度呢？确实。闪电索引器的计算复杂度仍然是O(L²)，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">但是它的计算量和DeepSeek-V3.1-Terminus中MLA的计算量相比。要少得多。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">一方面是因为闪电索引器的头数少。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">另一方面是因为它可以用FP8精度实现。这些都使得闪电索引器的实际计算成本非常低。再加上研发团队针对DSA做的优化实现。最终使得DeepSeek-V3.2-Exp在长上下文场景下实现了显著的端到端加速。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_1045.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">为了让大家更直观地感受到这种成本差异。研发团队在H800 GPU集群上部署了实际的服务。并且对两款模型的推理成本进行了 benchmark 测试。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">测试时GPU的租赁价格按照每小时2美元计算。具体的成本对比数据可以参考这张图。其中图（a）是预填充（Prefilling）阶段的成本。图（b）是解码（Decoding）阶段的成本。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">从图中可以清晰地看到。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">随着序列中token位置的增加。比如从0K到128K。DeepSeek-V3.2-Exp的每百万token成本始终低于DeepSeek-V3.1-Terminus。而且在token位置达到128K这种长上下文场景下。成本优势更加明显。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">另外。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">还有一个细节需要跟大家提一下。在处理短序列预填充的时候。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">研发团队专门实现了一种掩码MHA（masked MHA）模式来模拟DSA的效果。这样做的目的是在短上下文条件下。也能让模型达到更高的效率。避免在短序列场景下因为DSA的稀疏机制反而导致效率下降。充分考虑了不同序列长度场景下的效率优化。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_1107.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">最后，关于未来的验证计划。研发团队也表现出了非常严谨的态度。虽然目前的内部评估结果显示DeepSeek-V3.2-Exp的表现很有前景。但是团队并没有就此止步。而是计划在真实世界场景中进行更大规模的测试。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">为什么要这么做呢？因为实验室环境下的基准测试往往无法完全覆盖真实场景中的各种复杂情况。真实场景中可能会遇到各种意想不到的问题。比如不同类型的长上下文数据、不同的推理任务需求等。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">通过大规模的真实场景测试。研发团队希望能够发现稀疏注意力架构可能存在的潜在局限。从而进一步优化模型。让DeepSeek-V3.2-Exp在实际应用中能够有更稳定、更出色的表现。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">到这里。DeepSeek-V3.2-Exp的核心内容就给大家介绍得差不多了。这款模型通过引入DSA稀疏注意力机制。成功在长上下文处理的效率上实现了突破。同时又保持了和之前版本相当的性能水平。为长上下文AI任务的应用提供了一个更高效、更经济的选择。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_1170.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">无论是架构设计中的细节考量。还是训练过程中的严谨优化。再到评估阶段的全面验证。都能看出DeepSeek-AI团队在这款模型上投入的心血和专业能力。当然。AI模型的发展是一个持续迭代的过程。DeepSeek-V3.2-Exp作为一款实验性模型。未来还有很大的优化空间。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如进一步提升稀疏筛选的准确性、在更多真实场景中验证性能等。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">相信随着后续的不断改进。这款模型会在更多领域发挥重要作用。也期待DeepSeek团队能够带来更多像这样兼顾性能和效率的优秀模型。感谢收看本期视频，我们下期再见。</p>

<hr style="clear:both" />
=============
<p><a href="https://www.youtube.com/watch?v=CA2lzW9INrQ">https://www.youtube.com/watch?v=CA2lzW9INrQ</a></p><p>昨天，DeepSeek团队推出了最新模型DeepSeek-V3.2-Exp，在之前 DeepSeek-V3.1-Terminus 的基础上，通过持续训练集成了一种全新的稀疏注意力机制，也就是DeepSeek 稀疏注意力，简称DSA，摇身一变成为了一款实验性的稀疏注意力模型。再搭配上一个高效的闪电索引器，V3.2-Exp在训练和推理两个关键环节都实现了显著的效率提升，尤其是在处理长上下文场景的时候，这种效率优势会更加明显。这期视频我们就从架构、训练、评估这几个核心维度，给大家拆解一下这款模型的细节。</p><p><a href="https://api-docs.deepseek.com/zh-cn/news/news250929">https://api-docs.deepseek.com/zh-cn/news/news250929</a></p><p><a href="https://github.com/deepseek-ai/DeepSeek-V3.2-Exp/blob/main/DeepSeek_V3_2.pdf">https://github.com/deepseek-ai/DeepSeek-V3.2-Exp/blob/main/DeepSeek_V3_2.pdf</a></p><p><a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Exp/tree/main/inference">https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Exp/tree/main/inference</a></p>]]></content:encoded>
      <itunes:image href="https://i.ytimg.com/vi/CA2lzW9INrQ/hqdefault.jpg"/>
      <pubDate>2025-09-30T09:00:18.000Z</pubDate>
    </item></channel>
</rss>