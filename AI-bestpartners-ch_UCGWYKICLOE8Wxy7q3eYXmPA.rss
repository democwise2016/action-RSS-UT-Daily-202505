<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:itunes="http://www.itunes.com/dtds/podcast-1.0.dtd" xmlns:googleplay="http://www.google.com/schemas/play-podcasts/1.0" xmlns:media="http://www.rssboard.org/media-rss" version="2.0">
  <channel>
    <title><![CDATA[AI-最佳拍檔[YT+]]]></title>
    <link>http://www.youtube.com/feeds/videos.xml?channel_id=UCGWYKICLOE8Wxy7q3eYXmPA</link>
    <image>
      <url>https://yt3.googleusercontent.com/GpvF9XbD-stx6HR3BySXKvMqm_AySlczqmJKdkdZsloYQ9-rnoaLdCpOn0irmvqi3QYroccHNg=s900-b50-c-k-c0x008A95A5-no-rj</url>
      <title>AI-最佳拍檔[YT+]</title>
      <link>http://www.youtube.com/feeds/videos.xml?channel_id=UCGWYKICLOE8Wxy7q3eYXmPA</link>
    </image>
    <language>en-us</language>
    <atom:link href="https://www.youtube.com/feeds/videos.xml?channel_id=UCGWYKICLOE8Wxy7q3eYXmPA" rel="self" type="application/rss+xml"/>
    <copyright><![CDATA[AI-最佳拍檔[YT+]]]></copyright>
    <itunes:author><![CDATA[AI-最佳拍檔[YT+]]]></itunes:author>
    <itunes:summary>
      <![CDATA[
      <a href="https://www.youtube.com/channel/UCGWYKICLOE8Wxy7q3eYXmPA" target="_blank">https://www.youtube.com/channel/UCGWYKICLOE8Wxy7q3eYXmPA</a><br />
<br />
<a href="https://www.youtube.com/feeds/videos.xml?channel_id=UCGWYKICLOE8Wxy7q3eYXmPA" target="_blank">https://www.youtube.com/feeds/videos.xml?channel_id=UCGWYKICLOE8Wxy7q3eYXmPA</a>
      ]]>
    </itunes:summary>
    <description>
      <![CDATA[
      <a href="https://www.youtube.com/channel/UCGWYKICLOE8Wxy7q3eYXmPA" target="_blank">https://www.youtube.com/channel/UCGWYKICLOE8Wxy7q3eYXmPA</a><br />
<br />
<a href="https://www.youtube.com/feeds/videos.xml?channel_id=UCGWYKICLOE8Wxy7q3eYXmPA" target="_blank">https://www.youtube.com/feeds/videos.xml?channel_id=UCGWYKICLOE8Wxy7q3eYXmPA</a>
      ]]>
    </description>
    <itunes:owner>
      <itunes:name><![CDATA[AI-最佳拍檔[YT+]]]></itunes:name>
    </itunes:owner>
    <itunes:image href="https://yt3.googleusercontent.com/GpvF9XbD-stx6HR3BySXKvMqm_AySlczqmJKdkdZsloYQ9-rnoaLdCpOn0irmvqi3QYroccHNg=s900-b50-c-k-c0x008A95A5-no-rj"/>
<item>
      <title><![CDATA[【人工智能】OpenAI激进押注基础设施 | Sam Altman A16Z访谈 | 三个战略 | 错误反对垂直整合 | Sora的爆火 | AI科学家 | 规模扩展上限 | 能源和开源]]></title>
      <link>https://www.youtube.com/watch?v=SGYt9Ny2jxw</link>
      <itunes:title><![CDATA[【人工智能】OpenAI激进押注基础设施 | Sam Altman A16Z访谈 | 三个战略 | 错误反对垂直整合 | Sora的爆火 | AI科学家 | 规模扩展上限 | 能源和开源]]></itunes:title>
      <itunes:author><![CDATA[最佳拍档]]></itunes:author>
      <itunes:summary>
        <![CDATA[<hr style="clear:both" />

<p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://img.youtube.com/vi/SGYt9Ny2jxw/maxresdefault.jpg" /></p><p><a href="https://www.youtube.com/watch?v=SGYt9Ny2jxw">https://www.youtube.com/watch?v=SGYt9Ny2jxw</a></p><h1>值得閱讀的理由</h1><ul><li>深入了解OpenAI執行長Sam Altman對公司<strong>核心戰略、AGI發展路徑</strong>及未來願景的獨到見解。</li><li>剖析OpenAI將Sora等看似娛樂性的產品融入其<strong>AGI研究使命</strong>的深層邏輯與考量。</li><li>探索Altman對於未來<strong>人機互動介面、AI科學家</strong>以及<strong>能源瓶頸</strong>等關鍵議題的預測與觀點。</li></ul><hr /><h1>摘要</h1><h2>OpenAI的核心策略與垂直整合</h2><p>影片開頭，大飛介紹了OpenAI近期備受矚目的動態，包括Sora的火熱、顯著的投資合作以及開發者日發布的新品，引發人們對其戰略方向的疑問。OpenAI執行長Sam Altman在a16z的最新訪談中，深入闡述了公司的三大核心目標：成為人們的<strong>個人AI訂閱服務</strong>、建構海量的<strong>基礎設施</strong>，以及最終實現對人類非常有用的<strong>AGI</strong>（通用人工智慧）。奧特曼表示，基礎設施目前主要供內部使用，但未來不排除對外開放。他進一步強調了<strong>研究與基礎設施的緊密關係</strong>，認為這是一個垂直整合的堆疊，研究促成卓越產品，而基礎設施則支持研究。儘管他曾反對垂直整合，但現在認為為了實現使命，OpenAI必須做到比想像中更多，並以iPhone為例，說明其成功的垂直整合模式。</p><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/SGYt9Ny2jxw_95.jpg" /></p><hr /><h2>Sora對於AGI研究的意義</h2><p>針對Sora發布後網友質疑為何將寶貴的GPU資源投入其中，奧特曼給出了多點回應。他認為，如果OpenAI能建立一個「<strong>真正出色的世界模型</strong>」，這對於AGI的重要性將遠超人們想像，就像ChatGPT問世後改變了人們對AGI的態度。他指出，社會與技術必須共同演進，不能等到最終才推出產品。Sora的存在不僅因為製作優秀產品很酷、能讓社會「嘗到即將發生事情的味道」，更有助於社會應對未來令人難以置信的視訊模型，因為視訊比文本具有更多情感共鳴。最重要的是，Sora將<strong>推動OpenAI的AGI研究</strong>。他也透露，投給Sora的計算資源其實只佔一小部分。</p><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/SGYt9Ny2jxw_252.jpg" /></p><hr /><h2>未來的人機互動介面與AI科學家</h2><p>談到未來的互動介面，奧特曼澄清了他之前關於聊天模型飽和的言論，指出那僅限於最基本的對話，而聊天介面能為用戶做的事情遠未飽和。他設想未來的介面將包含兩部分：一是像Sora那樣<strong>即時渲染視訊的世界模型</strong>，二是能夠真正理解上下文並知道何時向用戶展示資訊的<strong>環境感知硬體設備</strong>。當被問及未來幾年模型的能力時，奧特曼最看好「<strong>AI科學家</strong>」，認為圖靈測試的等價物是AI能夠自主進行科學研究。他預測，在兩年內模型將承擔更多的科學工作並做出重要發現，並表示自己最大的驚訝是深度學習技術不斷帶來新的突破，證明了Scaling Law的持續有效性。</p><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/SGYt9Ny2jxw_450.jpg" /></p><hr /><h2>個人化AI與基礎設施佈局</h2><p>奧特曼承認，OpenAI最初認為數十億人會與同一個AI對話的想法過於天真。他堅信「<strong>個人化</strong>」才是終極答案，理想情況下，AI應透過與用戶的短暫交流來了解其偏好並自行推斷。短期內，用戶則可選擇預設的個性。此外，他透露OpenAI決定進行一次「<strong>非常激進的基礎設施押注</strong>」，源於對研究路線圖和模型經濟價值的空前信心。他意識到OpenAI需要整個產業，特別是頂尖企業的支持，並預告未來幾個月將有更多合作。儘管對模型能力發展充滿信心，奧特曼也承認「限制肯定存在」，但強調距離這些限制還非常遙遠。在資源分配上，研究始終享有優先權，幾乎總是優先於產品獲得GPU資源。</p><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/SGYt9Ny2jxw_10.jpg" /></p><hr /><h2>奧特曼與AI的關係及營運哲學</h2><p>奧特曼分享了他從小就是「AI迷」的經歷，以及早期深度學習和Scaling Law不被看好的困境。他坦承自己<strong>天生不適合管理公司</strong>，認為自己更適合投資者的角色。他指出，投資者傾向於理論上的市場效率與「好的感覺」，而營運公司則需處理組織動態、衝突解決等繁瑣細節，常帶來「壞的感覺」。儘管早年營運經驗不足，他仍將管理OpenAI的這幾年視為職業生涯中「最有趣的幾年」，能見證頂尖人才進行的歷史性工作。</p><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/SGYt9Ny2jxw_13.jpg" /></p><hr /><h2>能源、開源與AGI的未來展望</h2><p>最後，奧特曼探討了其他重要議題。他認為<strong>能源是AI最大的瓶頸之一</strong>，AI的指數級增長將依賴於更廉價、更豐富的能源。他預計短期內美國新增能源主要來自天然氣，長期則看好太陽能加儲能以及包括SMR和核聚變在內的整個核能技術棧。他批評西方長期排斥核能是「難以置信的愚蠢決定」。關於開源，他認為是好的，但也擔憂像DeepSeek這樣的中國開源模型主導市場可能帶來的風險。對於AGI的到來，奧特曼持<strong>連續性觀點</strong>，而非類似奇點大爆炸的瞬間。</p><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/SGYt9Ny2jxw_17.jpg" /></p><hr /><hr />================================================</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">大家好，这里是最佳拍档，我是大飞。最近OpenAI的热度不小。一边是火爆出圈的Sora 2。另一边是各种引人注目的投资合作。而且还在几天前的开发者日上发布了一大堆新品。人们不禁要问了。OpenAI这是要干啥呢？背后又有什么样的逻辑呢？关于这些问题。OpenAI CEO Sam Altman在a16z的最新采访中一一做了回应。他不仅详细阐述了OpenAI的三大核心战略。而且也提到了Sora 2、未来人机交互界面等热点话题。甚至还直言自己天生就不是一个适合管理公司的人。那Altman究竟在这次访谈中都说了什么呢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">今天我们就来简单的回顾一下。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">一上来。a16z合伙人埃里克·托伦伯格Erik Torenberg发出了一个疑惑。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">OpenA作为四家公司的组合。包括一家消费技术公司、一家大规模基础设施运营公司、一家研究实验室。以及一家从硬件到应用集成的新公司。背后的考量究竟是什么呢？奥特曼对此表示。一切的核心目标主要有三个。OpenAI希望成为人们的个人AI订阅服务。为了支持这一点。OpenAI还必须构建海量的基础设施。而最终的使命在于构建对人们非常有用的AGI。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/SGYt9Ny2jxw_80.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">当被问到OpenAI的基础设施。是否会卖给其他公司使用时。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">奥特曼表示目前只支持自用。未来就不好说了。紧接着Erik提到了一件早年间的趣事。早期OpenAI被问到商业模式的时候。奥特曼曾经用玩笑般的口吻回复。我们会问AI，它会为我们解决的。虽然这个答案。当时在外界听起来有点可笑。但是后来AI能力的进化确实也让有目共睹。甚至到了现在。奥特曼自曝也会经常问AI一些关于组织运营的问题。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而在这一次的访谈中。他再一次强调了基础设施和研究的紧密关系。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">研究能够让OpenAI能够做出出色的产品。而基础设施能够让OpenAI进行研究。这就像一个垂直的堆栈一样。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">奥特曼表示。自己曾经一直反对垂直整合。但是现在他认为自己错了。他解释说。虽然经济理论倾向于公司只做一件事。但是在OpenAI的案例中。为了实现使命。他们必须做比原先想象中更多的事情。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/SGYt9Ny2jxw_141.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">他还引用了iPhone的例子。称iphone是科技行业最令人难以置信的产品。并且指出它是极其垂直整合的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">随后。谈话的话题转到了最近火出圈的Sora身上。虽然Sora很好玩。但是还是有不少的网友提出了质疑。为什么要把宝贵的GPU投入到Sora上呢？对此，奥特曼是这么回应的。虽然Sora表面上看似乎与AGI不相关。但是他敢打赌。如果他们能够建立一个“真正出色的世界模型”，这对于AGI的重要性将超出人们的想象。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">就像人们或许会认为ChatGPT与AGI的关联不大一样。但是实际情况是。在ChatGPT出现以后。当人们再去谈论AGI的时候。不会再直接说“这不可能发生”，或者“我们不在乎”了。所以说，它对于启迪心智非常的重要。社会和技术必须共同演进。不能等到最后才把东西扔出来。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">整体而言。关于将Sora融入OpenAI战略的理由。奥特曼给出了这样几点说法。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/SGYt9Ny2jxw_202.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">一、制作优秀的产品是很酷的。人们喜欢Sora；。二、为了共同演进。让社会“尝到即将发生的事情的味道”很重要；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">三、世界很快就必须应对令人难以置信的视频模型。并且整个社会将经历一些调整；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">四、让世界迅速了解视频的发展方向非常重要。因为视频比文本有更多的情感共鸣；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">五、Sora将帮助OpenAI推进AGI的研究；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">六、并非所有的事情都只看重效率。沿途也必须有一些乐趣和喜悦。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">不过说归说，奥特曼也还是透露了。整体上投给Sora的计算资源只是一小部分。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">另外值得一提的是。自从Sora 2发布后。国内外的社交网络还掀起了一股。以生成奥特曼为主角的视频的整活儿风潮。对此奥特曼还专门发了一条帖子回应。言语之中不乏苦笑无奈。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">说到视频模型，Erik顺带问到。奥特曼在8月份。曾经说过模型已经在聊天用例方面达到饱和了。那么未来的交互界面会是怎样的呢？</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/SGYt9Ny2jxw_267.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">对于这个问题。奥特曼首先做了一番澄清。他表示当时自己是在非常狭隘的意义上。谈到聊天问题。也就是说。如果你只是想进行最基本的聊天式对话。那么它已经很好了；。但是聊天界面能为你做的事情。这个还远远没有饱和。在他的设想中。未来的界面会包括两部分。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">一是像Sora那样实时渲染视频的世界模型；。二是一些新的环境感知硬件设备。它们能够真正的理解上下文。并且知道什么时候向用户展示信息。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而当被问到“未来几年内。模型将能够做到今天做不到的什么事情？”时，奥特曼兴奋的表示。他自己最看好“AI科学家”。他指出。图灵测试的概念已经“飞快地掠过了”，在他看来，图灵测试的等价物。一直是AI能够自主进行科学研究的时候。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">就像他们第一次在GPT-5上看到的例子一样。并且他还预测，两年内。模型将承担更多的科学工作。并且做出重要的发现。而时至今日。奥特曼表示自己最大的惊讶。就是“发现了很多新东西”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/SGYt9Ny2jxw_334.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">他曾经以为。OpenAI偶然发现了Scaling Law这个巨大的秘密之后。就不会再有这么幸运了。但是深度学习这项技术持续的给予了他们奇迹。他说道。当我们获得推理模型的突破时。我当时认为再也不会有那样的突破了。但是没想到这项技术能如此出色地运作。这似乎太不可思议了。但是。这也许就是当你发现一个重大的科学突破时的感受。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">如果这项突破真的是非常大的。而且是相当基础性的。那么这种感觉会持续有效。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而至于大语言模型能走多远？奥特曼充满自信的表示。我们能够用当前的技术。制造出寻找到下一个突破的东西。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">顺带一提，奥特曼还承认。他们最初认为。数十亿人都想和同一个AI说话的想法。是非常天真的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在谈话中他直接表示。个性化确实是终极答案。理想情况下。AI会通过与用户的短暂交流。来了解他们的喜好并且自行推断。但是在短期内。用户可能只需要选择一个预设的个性就好了。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/SGYt9Ny2jxw_397.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">接下来。奥特曼也回应了OpenAI最近与英伟达、AMD和Oracle等公司进行的合作。他的原话是。我们决定是时候进行一次非常激进的基础设施押注了。因为他对摆在面前的研究路线图。以及使用这些模型将带来的经济价值。从未如此的自信过。基于这个信心。他也意识到了OpenAI需要得到整个行业、或者行业中佼佼者的支持。这涉及到从电子级别到模型分发。以及介于两者之间的所有事情。他还表示，OpenAI将与更多的人合作。让大家期待OpenAI在未来几个月内的更多动作。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">面对信心爆棚的奥特曼。主持人话锋一转。问到了他对规模扩展上限的问题。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">奥特曼随即也稍微冷静了下来。承认“限制肯定是有的”，但是如果他们对模型能力发展的预测是正确的。那么限制离我们今天所处的位置。还非常遥远。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">他继续补充说。即使OpenAI只能拥有如今的模型。他们也会继续扩大规模。但是如果真的只有今天的模型了。那么他们也许不会如此的激进。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/SGYt9Ny2jxw_463.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">另外。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">当被问到OpenAI如何在产品和研究之间分配资源的时候。奥特曼表示，当存在资源限制的时候。几乎总是会优先将GPU提供给研究。而不是支持产品。因为基于构建AGI的这个终极目标。研究总是享有优先权。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">最后，在采访的不同阶段。奥特曼还零零碎碎提到了自己与AI的关系。他表示，其实自己打小就是“AI迷”，AI始终是他想要做的事情。大学一二年级的时候。他曾经在AI实验室工作。并且学习了物理学和计算机科学。不过在那个时候，AI对外界来说。还是个完全不起作用的东西。并且最初当OpenAI团队开始弄明白了深度学习和Scaling Laws的时候。整个领域和投资者都“非常痛恨它”，认为这不是一个吸引人的解决方案。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">另外。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">他还谈到了自己从投资者到CEO的职业角色转变。他坦承自己。天生就不是一个适合管理公司的人。相比管理一家公司。他认为自己还是更适合做投资者。在他看来。投资者更倾向于理论上的市场效率。即每个公司只做一件事。而且通常是一种“好的感觉”；。而运营一家公司。则需要处理组织动态、冲突解决。以及各种繁琐的细节工作。常常是一种“坏的感觉”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/SGYt9Ny2jxw_542.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">包括早期管理OpenAI的时候。他表示自己的“实际运营经验非常少”，他甚至开玩笑的说。简直不敢相信自己还在经营着这家公司。当然了。他也表示那是他职业生涯中“最有趣的几年”，得以看到顶尖人才们进行的、惊人的历史性工作。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">最最后，除了以上这些。奥特曼还提到了一些关于能源和开源的话题。我简单总结一下。首先。他认为能源是AI最大的瓶颈之一。如今，AI与能源已“合二为一”，AI的指数级增长将依赖于更廉价、更丰富的能源。他预计。短期内美国新增的能源将主要来自于天然气。但是从长远来看。他认为主导的能源将是太阳能加储能。以及核能。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">不过，他所指的核能。包括了小型模块化反应堆SMR。以及核聚变在内的整个核能技术栈。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">其次，他批评了西方长期排斥核能。是一个令人难以置信的愚蠢决定。核能的推广速度。取决于它是否具有完全压倒性的经济优势；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/SGYt9Ny2jxw_607.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第三，他认为开源是好的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">但是他也担忧。像DeepSeek这样的中国开源模型。主导市场会带来一定的风险；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">最后，他认为AGI的到来是连续性的。而非一个类似于奇点大爆炸的时刻。好了。以上就是Sam Altman这次访谈的主要内容了。那么大家是怎么看待奥特曼的这些观点呢？欢迎在评论区留言。感谢收看本期视频，我们下期再见。</p>

<hr style="clear:both" />
=============
<p><a href="https://www.youtube.com/watch?v=SGYt9Ny2jxw">https://www.youtube.com/watch?v=SGYt9Ny2jxw</a></p><p>最近OpenAI的热度不小，一边是火爆出圈的Sora 2，另一边是各种引人注目的投资合作，而且还在几天前的开发者日上发布了一大堆新品。人们不禁要问了，OpenAI这是要干啥呢？背后又有什么样的逻辑呢？关于这些问题，OpenAI CEO Sam Altman在a16z的最新采访中一一做了回应。他不仅详细阐述了OpenAI的三大核心战略，而且也提到了Sora 2、未来人机交互界面等热点话题，甚至还直言自己天生就不是一个适合管理公司的人。那Altman究竟在这次访谈中都说了什么呢，今天我们就来简单的回顾一下。</p><p><a href="https://youtu.be/JfE1Wun9xkk?si=rMAy6omir1b6R8Mi">https://youtu.be/JfE1Wun9xkk?si=rMAy6omir1b6R8Mi</a></p>]]>
      </itunes:summary>
      <description>
        <![CDATA[<hr style="clear:both" />

<p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://img.youtube.com/vi/SGYt9Ny2jxw/maxresdefault.jpg" /></p><p><a href="https://www.youtube.com/watch?v=SGYt9Ny2jxw">https://www.youtube.com/watch?v=SGYt9Ny2jxw</a></p><h1>值得閱讀的理由</h1><ul><li>深入了解OpenAI執行長Sam Altman對公司<strong>核心戰略、AGI發展路徑</strong>及未來願景的獨到見解。</li><li>剖析OpenAI將Sora等看似娛樂性的產品融入其<strong>AGI研究使命</strong>的深層邏輯與考量。</li><li>探索Altman對於未來<strong>人機互動介面、AI科學家</strong>以及<strong>能源瓶頸</strong>等關鍵議題的預測與觀點。</li></ul><hr /><h1>摘要</h1><h2>OpenAI的核心策略與垂直整合</h2><p>影片開頭，大飛介紹了OpenAI近期備受矚目的動態，包括Sora的火熱、顯著的投資合作以及開發者日發布的新品，引發人們對其戰略方向的疑問。OpenAI執行長Sam Altman在a16z的最新訪談中，深入闡述了公司的三大核心目標：成為人們的<strong>個人AI訂閱服務</strong>、建構海量的<strong>基礎設施</strong>，以及最終實現對人類非常有用的<strong>AGI</strong>（通用人工智慧）。奧特曼表示，基礎設施目前主要供內部使用，但未來不排除對外開放。他進一步強調了<strong>研究與基礎設施的緊密關係</strong>，認為這是一個垂直整合的堆疊，研究促成卓越產品，而基礎設施則支持研究。儘管他曾反對垂直整合，但現在認為為了實現使命，OpenAI必須做到比想像中更多，並以iPhone為例，說明其成功的垂直整合模式。</p><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/SGYt9Ny2jxw_95.jpg" /></p><hr /><h2>Sora對於AGI研究的意義</h2><p>針對Sora發布後網友質疑為何將寶貴的GPU資源投入其中，奧特曼給出了多點回應。他認為，如果OpenAI能建立一個「<strong>真正出色的世界模型</strong>」，這對於AGI的重要性將遠超人們想像，就像ChatGPT問世後改變了人們對AGI的態度。他指出，社會與技術必須共同演進，不能等到最終才推出產品。Sora的存在不僅因為製作優秀產品很酷、能讓社會「嘗到即將發生事情的味道」，更有助於社會應對未來令人難以置信的視訊模型，因為視訊比文本具有更多情感共鳴。最重要的是，Sora將<strong>推動OpenAI的AGI研究</strong>。他也透露，投給Sora的計算資源其實只佔一小部分。</p><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/SGYt9Ny2jxw_252.jpg" /></p><hr /><h2>未來的人機互動介面與AI科學家</h2><p>談到未來的互動介面，奧特曼澄清了他之前關於聊天模型飽和的言論，指出那僅限於最基本的對話，而聊天介面能為用戶做的事情遠未飽和。他設想未來的介面將包含兩部分：一是像Sora那樣<strong>即時渲染視訊的世界模型</strong>，二是能夠真正理解上下文並知道何時向用戶展示資訊的<strong>環境感知硬體設備</strong>。當被問及未來幾年模型的能力時，奧特曼最看好「<strong>AI科學家</strong>」，認為圖靈測試的等價物是AI能夠自主進行科學研究。他預測，在兩年內模型將承擔更多的科學工作並做出重要發現，並表示自己最大的驚訝是深度學習技術不斷帶來新的突破，證明了Scaling Law的持續有效性。</p><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/SGYt9Ny2jxw_450.jpg" /></p><hr /><h2>個人化AI與基礎設施佈局</h2><p>奧特曼承認，OpenAI最初認為數十億人會與同一個AI對話的想法過於天真。他堅信「<strong>個人化</strong>」才是終極答案，理想情況下，AI應透過與用戶的短暫交流來了解其偏好並自行推斷。短期內，用戶則可選擇預設的個性。此外，他透露OpenAI決定進行一次「<strong>非常激進的基礎設施押注</strong>」，源於對研究路線圖和模型經濟價值的空前信心。他意識到OpenAI需要整個產業，特別是頂尖企業的支持，並預告未來幾個月將有更多合作。儘管對模型能力發展充滿信心，奧特曼也承認「限制肯定存在」，但強調距離這些限制還非常遙遠。在資源分配上，研究始終享有優先權，幾乎總是優先於產品獲得GPU資源。</p><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/SGYt9Ny2jxw_10.jpg" /></p><hr /><h2>奧特曼與AI的關係及營運哲學</h2><p>奧特曼分享了他從小就是「AI迷」的經歷，以及早期深度學習和Scaling Law不被看好的困境。他坦承自己<strong>天生不適合管理公司</strong>，認為自己更適合投資者的角色。他指出，投資者傾向於理論上的市場效率與「好的感覺」，而營運公司則需處理組織動態、衝突解決等繁瑣細節，常帶來「壞的感覺」。儘管早年營運經驗不足，他仍將管理OpenAI的這幾年視為職業生涯中「最有趣的幾年」，能見證頂尖人才進行的歷史性工作。</p><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/SGYt9Ny2jxw_13.jpg" /></p><hr /><h2>能源、開源與AGI的未來展望</h2><p>最後，奧特曼探討了其他重要議題。他認為<strong>能源是AI最大的瓶頸之一</strong>，AI的指數級增長將依賴於更廉價、更豐富的能源。他預計短期內美國新增能源主要來自天然氣，長期則看好太陽能加儲能以及包括SMR和核聚變在內的整個核能技術棧。他批評西方長期排斥核能是「難以置信的愚蠢決定」。關於開源，他認為是好的，但也擔憂像DeepSeek這樣的中國開源模型主導市場可能帶來的風險。對於AGI的到來，奧特曼持<strong>連續性觀點</strong>，而非類似奇點大爆炸的瞬間。</p><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/SGYt9Ny2jxw_17.jpg" /></p><hr /><hr />================================================</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">大家好，这里是最佳拍档，我是大飞。最近OpenAI的热度不小。一边是火爆出圈的Sora 2。另一边是各种引人注目的投资合作。而且还在几天前的开发者日上发布了一大堆新品。人们不禁要问了。OpenAI这是要干啥呢？背后又有什么样的逻辑呢？关于这些问题。OpenAI CEO Sam Altman在a16z的最新采访中一一做了回应。他不仅详细阐述了OpenAI的三大核心战略。而且也提到了Sora 2、未来人机交互界面等热点话题。甚至还直言自己天生就不是一个适合管理公司的人。那Altman究竟在这次访谈中都说了什么呢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">今天我们就来简单的回顾一下。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">一上来。a16z合伙人埃里克·托伦伯格Erik Torenberg发出了一个疑惑。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">OpenA作为四家公司的组合。包括一家消费技术公司、一家大规模基础设施运营公司、一家研究实验室。以及一家从硬件到应用集成的新公司。背后的考量究竟是什么呢？奥特曼对此表示。一切的核心目标主要有三个。OpenAI希望成为人们的个人AI订阅服务。为了支持这一点。OpenAI还必须构建海量的基础设施。而最终的使命在于构建对人们非常有用的AGI。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/SGYt9Ny2jxw_80.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">当被问到OpenAI的基础设施。是否会卖给其他公司使用时。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">奥特曼表示目前只支持自用。未来就不好说了。紧接着Erik提到了一件早年间的趣事。早期OpenAI被问到商业模式的时候。奥特曼曾经用玩笑般的口吻回复。我们会问AI，它会为我们解决的。虽然这个答案。当时在外界听起来有点可笑。但是后来AI能力的进化确实也让有目共睹。甚至到了现在。奥特曼自曝也会经常问AI一些关于组织运营的问题。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而在这一次的访谈中。他再一次强调了基础设施和研究的紧密关系。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">研究能够让OpenAI能够做出出色的产品。而基础设施能够让OpenAI进行研究。这就像一个垂直的堆栈一样。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">奥特曼表示。自己曾经一直反对垂直整合。但是现在他认为自己错了。他解释说。虽然经济理论倾向于公司只做一件事。但是在OpenAI的案例中。为了实现使命。他们必须做比原先想象中更多的事情。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/SGYt9Ny2jxw_141.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">他还引用了iPhone的例子。称iphone是科技行业最令人难以置信的产品。并且指出它是极其垂直整合的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">随后。谈话的话题转到了最近火出圈的Sora身上。虽然Sora很好玩。但是还是有不少的网友提出了质疑。为什么要把宝贵的GPU投入到Sora上呢？对此，奥特曼是这么回应的。虽然Sora表面上看似乎与AGI不相关。但是他敢打赌。如果他们能够建立一个“真正出色的世界模型”，这对于AGI的重要性将超出人们的想象。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">就像人们或许会认为ChatGPT与AGI的关联不大一样。但是实际情况是。在ChatGPT出现以后。当人们再去谈论AGI的时候。不会再直接说“这不可能发生”，或者“我们不在乎”了。所以说，它对于启迪心智非常的重要。社会和技术必须共同演进。不能等到最后才把东西扔出来。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">整体而言。关于将Sora融入OpenAI战略的理由。奥特曼给出了这样几点说法。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/SGYt9Ny2jxw_202.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">一、制作优秀的产品是很酷的。人们喜欢Sora；。二、为了共同演进。让社会“尝到即将发生的事情的味道”很重要；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">三、世界很快就必须应对令人难以置信的视频模型。并且整个社会将经历一些调整；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">四、让世界迅速了解视频的发展方向非常重要。因为视频比文本有更多的情感共鸣；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">五、Sora将帮助OpenAI推进AGI的研究；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">六、并非所有的事情都只看重效率。沿途也必须有一些乐趣和喜悦。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">不过说归说，奥特曼也还是透露了。整体上投给Sora的计算资源只是一小部分。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">另外值得一提的是。自从Sora 2发布后。国内外的社交网络还掀起了一股。以生成奥特曼为主角的视频的整活儿风潮。对此奥特曼还专门发了一条帖子回应。言语之中不乏苦笑无奈。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">说到视频模型，Erik顺带问到。奥特曼在8月份。曾经说过模型已经在聊天用例方面达到饱和了。那么未来的交互界面会是怎样的呢？</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/SGYt9Ny2jxw_267.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">对于这个问题。奥特曼首先做了一番澄清。他表示当时自己是在非常狭隘的意义上。谈到聊天问题。也就是说。如果你只是想进行最基本的聊天式对话。那么它已经很好了；。但是聊天界面能为你做的事情。这个还远远没有饱和。在他的设想中。未来的界面会包括两部分。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">一是像Sora那样实时渲染视频的世界模型；。二是一些新的环境感知硬件设备。它们能够真正的理解上下文。并且知道什么时候向用户展示信息。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而当被问到“未来几年内。模型将能够做到今天做不到的什么事情？”时，奥特曼兴奋的表示。他自己最看好“AI科学家”。他指出。图灵测试的概念已经“飞快地掠过了”，在他看来，图灵测试的等价物。一直是AI能够自主进行科学研究的时候。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">就像他们第一次在GPT-5上看到的例子一样。并且他还预测，两年内。模型将承担更多的科学工作。并且做出重要的发现。而时至今日。奥特曼表示自己最大的惊讶。就是“发现了很多新东西”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/SGYt9Ny2jxw_334.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">他曾经以为。OpenAI偶然发现了Scaling Law这个巨大的秘密之后。就不会再有这么幸运了。但是深度学习这项技术持续的给予了他们奇迹。他说道。当我们获得推理模型的突破时。我当时认为再也不会有那样的突破了。但是没想到这项技术能如此出色地运作。这似乎太不可思议了。但是。这也许就是当你发现一个重大的科学突破时的感受。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">如果这项突破真的是非常大的。而且是相当基础性的。那么这种感觉会持续有效。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而至于大语言模型能走多远？奥特曼充满自信的表示。我们能够用当前的技术。制造出寻找到下一个突破的东西。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">顺带一提，奥特曼还承认。他们最初认为。数十亿人都想和同一个AI说话的想法。是非常天真的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在谈话中他直接表示。个性化确实是终极答案。理想情况下。AI会通过与用户的短暂交流。来了解他们的喜好并且自行推断。但是在短期内。用户可能只需要选择一个预设的个性就好了。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/SGYt9Ny2jxw_397.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">接下来。奥特曼也回应了OpenAI最近与英伟达、AMD和Oracle等公司进行的合作。他的原话是。我们决定是时候进行一次非常激进的基础设施押注了。因为他对摆在面前的研究路线图。以及使用这些模型将带来的经济价值。从未如此的自信过。基于这个信心。他也意识到了OpenAI需要得到整个行业、或者行业中佼佼者的支持。这涉及到从电子级别到模型分发。以及介于两者之间的所有事情。他还表示，OpenAI将与更多的人合作。让大家期待OpenAI在未来几个月内的更多动作。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">面对信心爆棚的奥特曼。主持人话锋一转。问到了他对规模扩展上限的问题。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">奥特曼随即也稍微冷静了下来。承认“限制肯定是有的”，但是如果他们对模型能力发展的预测是正确的。那么限制离我们今天所处的位置。还非常遥远。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">他继续补充说。即使OpenAI只能拥有如今的模型。他们也会继续扩大规模。但是如果真的只有今天的模型了。那么他们也许不会如此的激进。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/SGYt9Ny2jxw_463.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">另外。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">当被问到OpenAI如何在产品和研究之间分配资源的时候。奥特曼表示，当存在资源限制的时候。几乎总是会优先将GPU提供给研究。而不是支持产品。因为基于构建AGI的这个终极目标。研究总是享有优先权。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">最后，在采访的不同阶段。奥特曼还零零碎碎提到了自己与AI的关系。他表示，其实自己打小就是“AI迷”，AI始终是他想要做的事情。大学一二年级的时候。他曾经在AI实验室工作。并且学习了物理学和计算机科学。不过在那个时候，AI对外界来说。还是个完全不起作用的东西。并且最初当OpenAI团队开始弄明白了深度学习和Scaling Laws的时候。整个领域和投资者都“非常痛恨它”，认为这不是一个吸引人的解决方案。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">另外。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">他还谈到了自己从投资者到CEO的职业角色转变。他坦承自己。天生就不是一个适合管理公司的人。相比管理一家公司。他认为自己还是更适合做投资者。在他看来。投资者更倾向于理论上的市场效率。即每个公司只做一件事。而且通常是一种“好的感觉”；。而运营一家公司。则需要处理组织动态、冲突解决。以及各种繁琐的细节工作。常常是一种“坏的感觉”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/SGYt9Ny2jxw_542.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">包括早期管理OpenAI的时候。他表示自己的“实际运营经验非常少”，他甚至开玩笑的说。简直不敢相信自己还在经营着这家公司。当然了。他也表示那是他职业生涯中“最有趣的几年”，得以看到顶尖人才们进行的、惊人的历史性工作。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">最最后，除了以上这些。奥特曼还提到了一些关于能源和开源的话题。我简单总结一下。首先。他认为能源是AI最大的瓶颈之一。如今，AI与能源已“合二为一”，AI的指数级增长将依赖于更廉价、更丰富的能源。他预计。短期内美国新增的能源将主要来自于天然气。但是从长远来看。他认为主导的能源将是太阳能加储能。以及核能。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">不过，他所指的核能。包括了小型模块化反应堆SMR。以及核聚变在内的整个核能技术栈。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">其次，他批评了西方长期排斥核能。是一个令人难以置信的愚蠢决定。核能的推广速度。取决于它是否具有完全压倒性的经济优势；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/SGYt9Ny2jxw_607.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第三，他认为开源是好的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">但是他也担忧。像DeepSeek这样的中国开源模型。主导市场会带来一定的风险；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">最后，他认为AGI的到来是连续性的。而非一个类似于奇点大爆炸的时刻。好了。以上就是Sam Altman这次访谈的主要内容了。那么大家是怎么看待奥特曼的这些观点呢？欢迎在评论区留言。感谢收看本期视频，我们下期再见。</p>

<hr style="clear:both" />
=============
<p><a href="https://www.youtube.com/watch?v=SGYt9Ny2jxw">https://www.youtube.com/watch?v=SGYt9Ny2jxw</a></p><p>最近OpenAI的热度不小，一边是火爆出圈的Sora 2，另一边是各种引人注目的投资合作，而且还在几天前的开发者日上发布了一大堆新品。人们不禁要问了，OpenAI这是要干啥呢？背后又有什么样的逻辑呢？关于这些问题，OpenAI CEO Sam Altman在a16z的最新采访中一一做了回应。他不仅详细阐述了OpenAI的三大核心战略，而且也提到了Sora 2、未来人机交互界面等热点话题，甚至还直言自己天生就不是一个适合管理公司的人。那Altman究竟在这次访谈中都说了什么呢，今天我们就来简单的回顾一下。</p><p><a href="https://youtu.be/JfE1Wun9xkk?si=rMAy6omir1b6R8Mi">https://youtu.be/JfE1Wun9xkk?si=rMAy6omir1b6R8Mi</a></p>]]>
      </description>
      <content:encoded><![CDATA[<hr style="clear:both" />

<p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://img.youtube.com/vi/SGYt9Ny2jxw/maxresdefault.jpg" /></p><p><a href="https://www.youtube.com/watch?v=SGYt9Ny2jxw">https://www.youtube.com/watch?v=SGYt9Ny2jxw</a></p><h1>值得閱讀的理由</h1><ul><li>深入了解OpenAI執行長Sam Altman對公司<strong>核心戰略、AGI發展路徑</strong>及未來願景的獨到見解。</li><li>剖析OpenAI將Sora等看似娛樂性的產品融入其<strong>AGI研究使命</strong>的深層邏輯與考量。</li><li>探索Altman對於未來<strong>人機互動介面、AI科學家</strong>以及<strong>能源瓶頸</strong>等關鍵議題的預測與觀點。</li></ul><hr /><h1>摘要</h1><h2>OpenAI的核心策略與垂直整合</h2><p>影片開頭，大飛介紹了OpenAI近期備受矚目的動態，包括Sora的火熱、顯著的投資合作以及開發者日發布的新品，引發人們對其戰略方向的疑問。OpenAI執行長Sam Altman在a16z的最新訪談中，深入闡述了公司的三大核心目標：成為人們的<strong>個人AI訂閱服務</strong>、建構海量的<strong>基礎設施</strong>，以及最終實現對人類非常有用的<strong>AGI</strong>（通用人工智慧）。奧特曼表示，基礎設施目前主要供內部使用，但未來不排除對外開放。他進一步強調了<strong>研究與基礎設施的緊密關係</strong>，認為這是一個垂直整合的堆疊，研究促成卓越產品，而基礎設施則支持研究。儘管他曾反對垂直整合，但現在認為為了實現使命，OpenAI必須做到比想像中更多，並以iPhone為例，說明其成功的垂直整合模式。</p><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/SGYt9Ny2jxw_95.jpg" /></p><hr /><h2>Sora對於AGI研究的意義</h2><p>針對Sora發布後網友質疑為何將寶貴的GPU資源投入其中，奧特曼給出了多點回應。他認為，如果OpenAI能建立一個「<strong>真正出色的世界模型</strong>」，這對於AGI的重要性將遠超人們想像，就像ChatGPT問世後改變了人們對AGI的態度。他指出，社會與技術必須共同演進，不能等到最終才推出產品。Sora的存在不僅因為製作優秀產品很酷、能讓社會「嘗到即將發生事情的味道」，更有助於社會應對未來令人難以置信的視訊模型，因為視訊比文本具有更多情感共鳴。最重要的是，Sora將<strong>推動OpenAI的AGI研究</strong>。他也透露，投給Sora的計算資源其實只佔一小部分。</p><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/SGYt9Ny2jxw_252.jpg" /></p><hr /><h2>未來的人機互動介面與AI科學家</h2><p>談到未來的互動介面，奧特曼澄清了他之前關於聊天模型飽和的言論，指出那僅限於最基本的對話，而聊天介面能為用戶做的事情遠未飽和。他設想未來的介面將包含兩部分：一是像Sora那樣<strong>即時渲染視訊的世界模型</strong>，二是能夠真正理解上下文並知道何時向用戶展示資訊的<strong>環境感知硬體設備</strong>。當被問及未來幾年模型的能力時，奧特曼最看好「<strong>AI科學家</strong>」，認為圖靈測試的等價物是AI能夠自主進行科學研究。他預測，在兩年內模型將承擔更多的科學工作並做出重要發現，並表示自己最大的驚訝是深度學習技術不斷帶來新的突破，證明了Scaling Law的持續有效性。</p><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/SGYt9Ny2jxw_450.jpg" /></p><hr /><h2>個人化AI與基礎設施佈局</h2><p>奧特曼承認，OpenAI最初認為數十億人會與同一個AI對話的想法過於天真。他堅信「<strong>個人化</strong>」才是終極答案，理想情況下，AI應透過與用戶的短暫交流來了解其偏好並自行推斷。短期內，用戶則可選擇預設的個性。此外，他透露OpenAI決定進行一次「<strong>非常激進的基礎設施押注</strong>」，源於對研究路線圖和模型經濟價值的空前信心。他意識到OpenAI需要整個產業，特別是頂尖企業的支持，並預告未來幾個月將有更多合作。儘管對模型能力發展充滿信心，奧特曼也承認「限制肯定存在」，但強調距離這些限制還非常遙遠。在資源分配上，研究始終享有優先權，幾乎總是優先於產品獲得GPU資源。</p><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/SGYt9Ny2jxw_10.jpg" /></p><hr /><h2>奧特曼與AI的關係及營運哲學</h2><p>奧特曼分享了他從小就是「AI迷」的經歷，以及早期深度學習和Scaling Law不被看好的困境。他坦承自己<strong>天生不適合管理公司</strong>，認為自己更適合投資者的角色。他指出，投資者傾向於理論上的市場效率與「好的感覺」，而營運公司則需處理組織動態、衝突解決等繁瑣細節，常帶來「壞的感覺」。儘管早年營運經驗不足，他仍將管理OpenAI的這幾年視為職業生涯中「最有趣的幾年」，能見證頂尖人才進行的歷史性工作。</p><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/SGYt9Ny2jxw_13.jpg" /></p><hr /><h2>能源、開源與AGI的未來展望</h2><p>最後，奧特曼探討了其他重要議題。他認為<strong>能源是AI最大的瓶頸之一</strong>，AI的指數級增長將依賴於更廉價、更豐富的能源。他預計短期內美國新增能源主要來自天然氣，長期則看好太陽能加儲能以及包括SMR和核聚變在內的整個核能技術棧。他批評西方長期排斥核能是「難以置信的愚蠢決定」。關於開源，他認為是好的，但也擔憂像DeepSeek這樣的中國開源模型主導市場可能帶來的風險。對於AGI的到來，奧特曼持<strong>連續性觀點</strong>，而非類似奇點大爆炸的瞬間。</p><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/SGYt9Ny2jxw_17.jpg" /></p><hr /><hr />================================================</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">大家好，这里是最佳拍档，我是大飞。最近OpenAI的热度不小。一边是火爆出圈的Sora 2。另一边是各种引人注目的投资合作。而且还在几天前的开发者日上发布了一大堆新品。人们不禁要问了。OpenAI这是要干啥呢？背后又有什么样的逻辑呢？关于这些问题。OpenAI CEO Sam Altman在a16z的最新采访中一一做了回应。他不仅详细阐述了OpenAI的三大核心战略。而且也提到了Sora 2、未来人机交互界面等热点话题。甚至还直言自己天生就不是一个适合管理公司的人。那Altman究竟在这次访谈中都说了什么呢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">今天我们就来简单的回顾一下。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">一上来。a16z合伙人埃里克·托伦伯格Erik Torenberg发出了一个疑惑。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">OpenA作为四家公司的组合。包括一家消费技术公司、一家大规模基础设施运营公司、一家研究实验室。以及一家从硬件到应用集成的新公司。背后的考量究竟是什么呢？奥特曼对此表示。一切的核心目标主要有三个。OpenAI希望成为人们的个人AI订阅服务。为了支持这一点。OpenAI还必须构建海量的基础设施。而最终的使命在于构建对人们非常有用的AGI。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/SGYt9Ny2jxw_80.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">当被问到OpenAI的基础设施。是否会卖给其他公司使用时。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">奥特曼表示目前只支持自用。未来就不好说了。紧接着Erik提到了一件早年间的趣事。早期OpenAI被问到商业模式的时候。奥特曼曾经用玩笑般的口吻回复。我们会问AI，它会为我们解决的。虽然这个答案。当时在外界听起来有点可笑。但是后来AI能力的进化确实也让有目共睹。甚至到了现在。奥特曼自曝也会经常问AI一些关于组织运营的问题。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而在这一次的访谈中。他再一次强调了基础设施和研究的紧密关系。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">研究能够让OpenAI能够做出出色的产品。而基础设施能够让OpenAI进行研究。这就像一个垂直的堆栈一样。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">奥特曼表示。自己曾经一直反对垂直整合。但是现在他认为自己错了。他解释说。虽然经济理论倾向于公司只做一件事。但是在OpenAI的案例中。为了实现使命。他们必须做比原先想象中更多的事情。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/SGYt9Ny2jxw_141.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">他还引用了iPhone的例子。称iphone是科技行业最令人难以置信的产品。并且指出它是极其垂直整合的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">随后。谈话的话题转到了最近火出圈的Sora身上。虽然Sora很好玩。但是还是有不少的网友提出了质疑。为什么要把宝贵的GPU投入到Sora上呢？对此，奥特曼是这么回应的。虽然Sora表面上看似乎与AGI不相关。但是他敢打赌。如果他们能够建立一个“真正出色的世界模型”，这对于AGI的重要性将超出人们的想象。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">就像人们或许会认为ChatGPT与AGI的关联不大一样。但是实际情况是。在ChatGPT出现以后。当人们再去谈论AGI的时候。不会再直接说“这不可能发生”，或者“我们不在乎”了。所以说，它对于启迪心智非常的重要。社会和技术必须共同演进。不能等到最后才把东西扔出来。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">整体而言。关于将Sora融入OpenAI战略的理由。奥特曼给出了这样几点说法。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/SGYt9Ny2jxw_202.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">一、制作优秀的产品是很酷的。人们喜欢Sora；。二、为了共同演进。让社会“尝到即将发生的事情的味道”很重要；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">三、世界很快就必须应对令人难以置信的视频模型。并且整个社会将经历一些调整；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">四、让世界迅速了解视频的发展方向非常重要。因为视频比文本有更多的情感共鸣；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">五、Sora将帮助OpenAI推进AGI的研究；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">六、并非所有的事情都只看重效率。沿途也必须有一些乐趣和喜悦。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">不过说归说，奥特曼也还是透露了。整体上投给Sora的计算资源只是一小部分。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">另外值得一提的是。自从Sora 2发布后。国内外的社交网络还掀起了一股。以生成奥特曼为主角的视频的整活儿风潮。对此奥特曼还专门发了一条帖子回应。言语之中不乏苦笑无奈。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">说到视频模型，Erik顺带问到。奥特曼在8月份。曾经说过模型已经在聊天用例方面达到饱和了。那么未来的交互界面会是怎样的呢？</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/SGYt9Ny2jxw_267.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">对于这个问题。奥特曼首先做了一番澄清。他表示当时自己是在非常狭隘的意义上。谈到聊天问题。也就是说。如果你只是想进行最基本的聊天式对话。那么它已经很好了；。但是聊天界面能为你做的事情。这个还远远没有饱和。在他的设想中。未来的界面会包括两部分。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">一是像Sora那样实时渲染视频的世界模型；。二是一些新的环境感知硬件设备。它们能够真正的理解上下文。并且知道什么时候向用户展示信息。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而当被问到“未来几年内。模型将能够做到今天做不到的什么事情？”时，奥特曼兴奋的表示。他自己最看好“AI科学家”。他指出。图灵测试的概念已经“飞快地掠过了”，在他看来，图灵测试的等价物。一直是AI能够自主进行科学研究的时候。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">就像他们第一次在GPT-5上看到的例子一样。并且他还预测，两年内。模型将承担更多的科学工作。并且做出重要的发现。而时至今日。奥特曼表示自己最大的惊讶。就是“发现了很多新东西”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/SGYt9Ny2jxw_334.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">他曾经以为。OpenAI偶然发现了Scaling Law这个巨大的秘密之后。就不会再有这么幸运了。但是深度学习这项技术持续的给予了他们奇迹。他说道。当我们获得推理模型的突破时。我当时认为再也不会有那样的突破了。但是没想到这项技术能如此出色地运作。这似乎太不可思议了。但是。这也许就是当你发现一个重大的科学突破时的感受。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">如果这项突破真的是非常大的。而且是相当基础性的。那么这种感觉会持续有效。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而至于大语言模型能走多远？奥特曼充满自信的表示。我们能够用当前的技术。制造出寻找到下一个突破的东西。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">顺带一提，奥特曼还承认。他们最初认为。数十亿人都想和同一个AI说话的想法。是非常天真的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在谈话中他直接表示。个性化确实是终极答案。理想情况下。AI会通过与用户的短暂交流。来了解他们的喜好并且自行推断。但是在短期内。用户可能只需要选择一个预设的个性就好了。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/SGYt9Ny2jxw_397.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">接下来。奥特曼也回应了OpenAI最近与英伟达、AMD和Oracle等公司进行的合作。他的原话是。我们决定是时候进行一次非常激进的基础设施押注了。因为他对摆在面前的研究路线图。以及使用这些模型将带来的经济价值。从未如此的自信过。基于这个信心。他也意识到了OpenAI需要得到整个行业、或者行业中佼佼者的支持。这涉及到从电子级别到模型分发。以及介于两者之间的所有事情。他还表示，OpenAI将与更多的人合作。让大家期待OpenAI在未来几个月内的更多动作。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">面对信心爆棚的奥特曼。主持人话锋一转。问到了他对规模扩展上限的问题。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">奥特曼随即也稍微冷静了下来。承认“限制肯定是有的”，但是如果他们对模型能力发展的预测是正确的。那么限制离我们今天所处的位置。还非常遥远。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">他继续补充说。即使OpenAI只能拥有如今的模型。他们也会继续扩大规模。但是如果真的只有今天的模型了。那么他们也许不会如此的激进。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/SGYt9Ny2jxw_463.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">另外。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">当被问到OpenAI如何在产品和研究之间分配资源的时候。奥特曼表示，当存在资源限制的时候。几乎总是会优先将GPU提供给研究。而不是支持产品。因为基于构建AGI的这个终极目标。研究总是享有优先权。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">最后，在采访的不同阶段。奥特曼还零零碎碎提到了自己与AI的关系。他表示，其实自己打小就是“AI迷”，AI始终是他想要做的事情。大学一二年级的时候。他曾经在AI实验室工作。并且学习了物理学和计算机科学。不过在那个时候，AI对外界来说。还是个完全不起作用的东西。并且最初当OpenAI团队开始弄明白了深度学习和Scaling Laws的时候。整个领域和投资者都“非常痛恨它”，认为这不是一个吸引人的解决方案。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">另外。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">他还谈到了自己从投资者到CEO的职业角色转变。他坦承自己。天生就不是一个适合管理公司的人。相比管理一家公司。他认为自己还是更适合做投资者。在他看来。投资者更倾向于理论上的市场效率。即每个公司只做一件事。而且通常是一种“好的感觉”；。而运营一家公司。则需要处理组织动态、冲突解决。以及各种繁琐的细节工作。常常是一种“坏的感觉”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/SGYt9Ny2jxw_542.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">包括早期管理OpenAI的时候。他表示自己的“实际运营经验非常少”，他甚至开玩笑的说。简直不敢相信自己还在经营着这家公司。当然了。他也表示那是他职业生涯中“最有趣的几年”，得以看到顶尖人才们进行的、惊人的历史性工作。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">最最后，除了以上这些。奥特曼还提到了一些关于能源和开源的话题。我简单总结一下。首先。他认为能源是AI最大的瓶颈之一。如今，AI与能源已“合二为一”，AI的指数级增长将依赖于更廉价、更丰富的能源。他预计。短期内美国新增的能源将主要来自于天然气。但是从长远来看。他认为主导的能源将是太阳能加储能。以及核能。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">不过，他所指的核能。包括了小型模块化反应堆SMR。以及核聚变在内的整个核能技术栈。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">其次，他批评了西方长期排斥核能。是一个令人难以置信的愚蠢决定。核能的推广速度。取决于它是否具有完全压倒性的经济优势；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/SGYt9Ny2jxw_607.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第三，他认为开源是好的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">但是他也担忧。像DeepSeek这样的中国开源模型。主导市场会带来一定的风险；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">最后，他认为AGI的到来是连续性的。而非一个类似于奇点大爆炸的时刻。好了。以上就是Sam Altman这次访谈的主要内容了。那么大家是怎么看待奥特曼的这些观点呢？欢迎在评论区留言。感谢收看本期视频，我们下期再见。</p>

<hr style="clear:both" />
=============
<p><a href="https://www.youtube.com/watch?v=SGYt9Ny2jxw">https://www.youtube.com/watch?v=SGYt9Ny2jxw</a></p><p>最近OpenAI的热度不小，一边是火爆出圈的Sora 2，另一边是各种引人注目的投资合作，而且还在几天前的开发者日上发布了一大堆新品。人们不禁要问了，OpenAI这是要干啥呢？背后又有什么样的逻辑呢？关于这些问题，OpenAI CEO Sam Altman在a16z的最新采访中一一做了回应。他不仅详细阐述了OpenAI的三大核心战略，而且也提到了Sora 2、未来人机交互界面等热点话题，甚至还直言自己天生就不是一个适合管理公司的人。那Altman究竟在这次访谈中都说了什么呢，今天我们就来简单的回顾一下。</p><p><a href="https://youtu.be/JfE1Wun9xkk?si=rMAy6omir1b6R8Mi">https://youtu.be/JfE1Wun9xkk?si=rMAy6omir1b6R8Mi</a></p>]]></content:encoded>
      <itunes:image href="https://i.ytimg.com/vi/SGYt9Ny2jxw/hqdefault.jpg"/>
      <pubDate>2025-10-09T23:00:34.000Z</pubDate>
    </item><item>
      <title><![CDATA[【人工智能】OpenAI DevDay 2025 | Apps SDK | AgentKit | Codex | GPT-5 Pro API | Real-time mini | Sora 2 API]]></title>
      <link>https://www.youtube.com/watch?v=N0X8-fsQuNY</link>
      <itunes:title><![CDATA[【人工智能】OpenAI DevDay 2025 | Apps SDK | AgentKit | Codex | GPT-5 Pro API | Real-time mini | Sora 2 API]]></itunes:title>
      <itunes:author><![CDATA[最佳拍档]]></itunes:author>
      <itunes:summary>
        <![CDATA[<hr style="clear:both" />

<p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://img.youtube.com/vi/N0X8-fsQuNY/maxresdefault.jpg" /></p><p><a href="https://www.youtube.com/watch?v=N0X8-fsQuNY">https://www.youtube.com/watch?v=N0X8-fsQuNY</a></p><h1>值得閱讀的理由</h1> <ul><li>深入了解OpenAI如何從技術展示轉向構建以AI為核心的<strong>全新軟體生態系統</strong>，掌握其商業藍圖。</li><li>學習Apps SDK、AgentKit和Codex這三大核心工具如何徹底改變軟體開發和使用者互動模式，體驗<strong>「無代碼編程」</strong>和<strong>「對話即應用」</strong>的未來。</li><li>分析OpenAI策略轉變的深層影響，思考其在追求商業落地與AGI（通用人工智慧）願景之間，如何平衡與取捨，以及這對整個AI行業的意義。</li></ul><hr /><h1>摘要</h1> <h2>OpenAI戰略藍圖：構建AI核心軟體生態</h2> <p>近期，OpenAI在AI領域動作頻頻，繼Sora 2震撼影片生成能力後，其DevDay發布會更揭示了OpenAI的宏大<strong>戰略藍圖</strong>。影片中說故事的人指出，這場發布會的目標是構建一個以AI為絕對核心的全新軟體生態體系，其核心內容涵蓋了三大板塊：<strong>Apps SDK</strong>、<strong>AgentKit</strong>和<strong>Codex</strong>，這三者共同支撐起OpenAI未來的商業帝國布局。作者強調，OpenAI不再僅是展示尖端技術，而是將AI技術演進整合為一套系統化、平台化的生態方案，旨在全面掌控未來的軟體世界。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/N0X8-fsQuNY_75.jpg" /></p> <p>首先登場的是<strong>Apps SDK</strong>，它旨在讓<strong>「大語言模型成為所有軟體唯一的入口」</strong>。作者解釋，Apps SDK是一套完整的開發堆棧，允許開發者直接在ChatGPT內部構建真實、可交互的應用程式，透過連接數據、觸發操作甚至渲染用戶界面。這與過去僅以文本形式返回結果的工具截然不同，它讓用戶能像在電腦上使用App一樣，在大語言模型中<strong>「完成操作」</strong>。影片中生動的Canva和Zillow案例展示了Apps SDK如何讓用戶在不切換軟體的情況下，透過對話完成從海報設計到房產篩選的複雜任務，實現了工具之間的<strong>「協作流」</strong>。支撐這一切的是OpenAI的MCP體系和GPT本身的圖像識別能力，以及關鍵的<strong>上下文記憶能力</strong>，使應用程式成為能隨時喚醒、理解對話背景的「插件」。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/N0X8-fsQuNY_330.jpg" /></p> <h2>AgentKit：推動Agent時代的可視化開發</h2> <p>接著，OpenAI推出了<strong>AgentKit</strong>，旨在降低開發門檻，<strong>「推動Agent時代」</strong>。作者提到，AgentKit的核心是<strong>「Agent Builder」</strong>，一個可視化畫布，讓開發者無需編寫代碼，只需透過拖拽和連接功能節點，就能直觀設計複雜業務流程。它還包含<strong>「ChatKit」</strong>（可嵌入的聊天組件）和<strong>「Connector」</strong>（連接Agent與企業內部數據工具），大幅簡化了Agent的開發與整合。影片中產品經理僅用8分鐘就為DevDay官網搭建並上線智能問答Agent的演示，證明了AgentKit的<strong>「精簡」</strong>和<strong>「易用性」</strong>。此外，AgentKit提供兩個「殺手鐧」：一是<strong>RFT定制</strong>，允許開發者深入模型內部，優化GPT-5的推理模型，並訓練其在最恰當的時機調用工具；二是<strong>Evals評估板塊</strong>，提供數據集構建、跟踪評估和自動化提示優化能力，幫助開發者精準定位並修復Agent工作流中的問題。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/N0X8-fsQuNY_660.jpg" /></p> <h2>Codex：實現「無代碼編程」的開發助手</h2> <p>在底層編程方面，發布會的第三個核心是可能威脅Claude地位的<strong>Codex</strong>。作者指出，Codex已是OpenAI增長最快的產品之一，受到程式設計師社群的廣泛好評，認為其生成的代碼更貼合實際需求。Codex正式版專注於對企業和工程團隊的深度支持，提供了三大功能：一是<strong>Slack集成</strong>，讓團隊成員直接在Slack頻道內調用Codex，回答技術問題、生成代碼片段甚至檢查bug，顯著提升溝通效率；二是全新的<strong>Codex SDK</strong>，使企業能將Codex能力作為「模塊」集成到內部開發工作流，實現自動化代碼審查或API文檔生成，將Codex轉變為<strong>「整個團隊的開發助手」</strong>；三是新的<strong>後台管理與報告工具</strong>，讓管理者能對Codex使用情況進行環境控制、實時監控與分析。最令人印象深刻的演示是，僅透過對話，語音助手就成功調用Codex SDK，實時修改前端代碼，完成滾動開發者名單的功能更新，預示著OpenAI所設想的<strong>「無代碼編程」</strong>未來，即AI能直接根據需求修改和進化軟體，開發者只需負責提出需求。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/N0X8-fsQuNY_915.jpg" /></p> <h2>API更新：降低開發門檻與生態共享</h2> <p>除了上述三大核心，DevDay還帶來了多項傳統但重要的API更新，旨在降低開發者使用OpenAI模型的門檻。作者強調，首先是<strong>GPT-5 Pro的API開放</strong>，讓所有開發者都能利用OpenAI目前最強大的模型，實現更複雜的自然語言理解與多模態處理。其次是新的語音模型<strong>「GPT Real-time mini」</strong>，其成本降低了70%，同時保持了出色的音質與情感表現力，大幅降低了語音應用的開發門檻。最關鍵的是<strong>Sora 2 API的開放</strong>，這意味著開發者能夠將Sora 2的頂級視頻生成能力集成到自己的產品中，無論是內容創作、電商宣傳還是教育動畫，都將看到Sora 2的廣泛應用，標誌著OpenAI技術從「自用」走向<strong>「生態共享」</strong>。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/N0X8-fsQuNY_1065.jpg" /></p> <h2>OpenAI的商業帝國輪廓與AGI願景之辯</h2> <p>在總結部分，作者跳脫技術細節，探討了OpenAI這次DevDay背後的<strong>「野心」</strong>。作者認為，OpenAI的商業帝國輪廓已日益清晰，甚至隱約蓋過了AGI的遠景圖景。他指出，GPT-5 Pro的迭代更像是穩健優化，而非GPT-4那樣的「跨時代驚艷感」；Sora 2的成功也更多是「商業嗅覺」的勝利，而非底層技術革命。作者強調，Apps SDK、AgentKit和Codex這三者，本質上都是在<strong>「搭建生態」</strong>，目標是建立一個以OpenAI大模型為核心的<strong>封閉且強掌控力的軟體生態系統</strong>，讓所有數據和流量都留在OpenAI體系內。作者感慨，如今的OpenAI更像一家<strong>「成熟的商業公司」</strong>，追求的是「生態掌控力」和「商業落地」，而非早期那種為AGI而探索未知的「神秘感」。他提出疑問：當OpenAI將更多精力放在「掌控生態」上，它還能像以前那樣為AGI的突破投入足夠資源嗎？AGI的遠景會不會慢慢變成「商業帝國」的附屬品？雖然這些問題需要時間來回答，但這次DevDay明確指出AI的下一個階段是<strong>「生態整合」</strong>，誰能將應用、Agent、編程工具整合在自己的體系內，誰就能在未來競爭中佔據優勢。對開發者而言是機遇與挑戰，對用戶而言則可能是對話框解決所有問題，但也可能陷入「生態壟斷」的時代。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/N0X8-fsQuNY_22.jpg" /></p><hr /><hr />================================================</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">大家好，这里是最佳拍档，我是大飞。相信最近关注AI圈的朋友。都被OpenAI的动静刷屏了。前几天，Sora 2刚登顶App榜。那种细腻到发丝、流畅到自然的视频生成效果。让不少人直呼“AI视频时代真的来了”。但是如果说Sora 2是OpenAI在“秀技术肌肉”，那美国时间10月6日举办的OpenAI DevDay。就是它真正亮出“战略蓝图”的时刻。这场发布会没有太多颠覆性的“黑科技”，却把过去两年AI技术的演进。整合成了一套系统化、平台化的生态方案。目标很明确。那就是要构建一个以AI为绝对核心的全新软件生态体系。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">今天咱们就来回顾一下这场发布会的内容。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">看看OpenAI的“商业帝国”到底在如何布局。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先。咱们得先明确这场发布会的整体框架。除了常规的API更新。核心内容其实是三个板块。Apps SDK、AgentKit、Codex. 这三者共同撑起了OpenAI的未来生态。而第一个要聊的。就是能让“大语言模型成为所有软件唯一入口”的Apps SDK。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/N0X8-fsQuNY_71.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">可能有朋友还记得，2024年的时候。“把大语言模型当软件入口”，还只是个模糊的概念。大家觉得AI能帮着调用工具。但总觉得差点意思。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">可到了2025年。这个概念已经成了行业共识。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而OpenAI的Apps SDK。就是把这个共识落地的关键工具。它不是一个简单的“插件集合”，而是一套完整的开发堆栈。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">开发者用它。能在ChatGPT内部直接构建真实、可交互的应用程序。通过连接自己的数据、触发具体的操作。甚至可以渲染出交互式的用户界面。这和之前的工具不一样的地方在于。像Claude和GitHub Copilot这些。它们虽然也能调用外部服务。但是最终只能把结果以文本的形式返回。缺乏真正的上下文理解和自然的交互界面。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">简单来说。过去的工具是“告诉你结果”，而Apps SDK是“帮你完成操作”，让用户在大语言模型里像在电脑上一样来使用App。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/N0X8-fsQuNY_133.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">发布会现场有个非常生动的案例。我给大家还原一下。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">假设你正在为宠物狗业务做头脑风暴。聊到一半想做海报。这时候你不用退出ChatGPT。直接“@”出Canva。重点来了。这个Canva能完全理解你之前和ChatGPT聊的所有点子。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如你说，要色彩丰富、异想天开。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">它不用你再重复解释。直接就能生成一系列符合要求的精美海报。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">如果这时候你灵感来了，说。请把这张海报扩展成商业计划书。它也能无缝衔接。直接帮你把商业计划书的框架搭好。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">等你想扩张业务，需要找场地的时候。ChatGPT还会根据你之前聊的“宠物狗业务”上下文。主动建议你考虑“匹兹堡”这个城市。然后直接唤起房产平台Zillow。全屏展示。你在ChatGPT里说“要带院子的三居室”，它就会帮你进行筛选。筛选完你再问“这个房子离狗狗公园有多远”，它还能基于Zillow的数据直接回答。整个过程没有一次软件切换。所有操作都在ChatGPT的对话框里完成。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/N0X8-fsQuNY_202.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">工具在最需要的时候自动出现。还能协同工作。这背后就是Apps SDK的核心能力。打破应用之间的壁垒。让不同工具在统一的入口里形成“协作流”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而支撑这一切的，是OpenAI的MCP体系。开发者依靠MCP能设计应用的逻辑和界面。再结合GPT本身的图像识别能力。让App不是“硬塞进”对话。而是“自然的融入”对话。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">更关键的是上下文记忆能力。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">发布会里还有个细节。ChatGPT在另一个新对话里。还能延续上一个对话的宠物狗业务话题。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这种跨对话的记忆。正是大语言模型能够作为软件入口的核心竞争力。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">以前的App是“功能孤岛”，你打开才能用；。现在有了Apps SDK。App成了“可随时唤醒的插件”，还能理解你的对话背景。真正做到了万物在大语言模型内。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">聊完Apps SDK，我们自然要问。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">怎么让这些App更好地适应用户需求。和对话更紧密地连接呢？</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/N0X8-fsQuNY_266.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这就轮到发布会的第二个核心。AgentKit了。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">行业里其实早就把2025年称作“Agent元年”，但是直到10月。市场上都没出现足够引发行业震荡的现象级Agent产品。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">显然，大家都觉得Agent是未来。但是开发门槛太高、落地太难。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而OpenAI这次推出AgentKit。就是想“推一把”Agent时代。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">它号称是“最简便、快捷”的Agent开发工具包。核心思路就是让一切回归可视化。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">AgentKit的核心是“Agent Builder”，这是一个可视化的画布。以前开发者做Agent，得从零写代码。逻辑错一点就得从头改。而现在用Agent Builder，不用写代码。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">通过直接拖拽、连接不同的功能节点。就能直观地设计复杂的业务流程。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">除了画布，它还有两个关键的功能。一个是“ChatKit”，这是一个可嵌入的聊天组件。开发者能轻松把带品牌定制能力的聊天界面。集成到自己的应用里。不用自己再来做界面设计了；。另一个是“Connector”，能够直接把AgentKit里构建的Agent工具。和企业内部的数据、工具连起来。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/N0X8-fsQuNY_339.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如企业的客户数据库、内部办公系统等等。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这样就不用再去做复杂的接口开发了。直接和企业已有的系统打通。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">发布会现场。产品经理克里斯蒂娜（Christina）做了个演示。从零开始，只用了8分钟。就为一个静态的DevDay活动官网。搭建并且上线了一个智能问答Agent。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">从演示里能看到。AgentKit的功能节点特别精简。只有三个核心。分别是Agent、End和Note。不同Agent之间的逻辑关系。也只用三个点就能控制。分别是条件触发。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如用户问时间就调用A Agent。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">问地点就调用B Agent，还有同时进行。让多个Agent并行处理任务。以及用户许可。也就是需要用户确认后才继续。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">每个Agent内部，还能添加三个工具。包括文件搜索、安全防护和MCP应用。当然。你也能把这些工具作为独立的功能点。加在流程外部。整个设计逻辑十分清晰。哪怕是没做过Agent开发的人。看一眼也能明白个大概。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/N0X8-fsQuNY_401.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">不过，客观地说，单从设计逻辑上看。AgentKit并没有比现在市场上的Dify、Coze这些工具领先太多。但是它胜在“精简”和“易用”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">为了让习惯了Dify的用户转用AgentKit。OpenAI还提供了两个“杀手锏”。第一个是RFT定制。现在第三方工具用GPT。都只能把它当做“黑箱”一样来调用API。你不知道模型内部是怎么推理的。也没法优化。但是AgentKit能深入模型的内部。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">目前OpenAI已经在实验GPT-5的RFT功能。开发者通过RFT。不仅能定制GPT-5的推理模型。还能专门训练模型在最恰当的时机、用最优的方式来调用工具。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如你想做一个客服Agent。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">用RFT能让模型学会。用户问售后问题的时候。先调用订单数据库查询信息。再调用售后流程工具提交申请。而不是乱调用工具。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这对想基于GPT来开发Agent的企业来说。吸引力非常的大。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/N0X8-fsQuNY_462.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第二个是AgentKit的“Evals评估板块”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">做Agent开发。最头疼的就是“不知道问题在哪”。比如流程跑不通，到底是节点错了。还是工具调用时机不对呢？Evals就直接解决了这个问题。它提供了“数据集构建”、“跟踪评估”、“自动化提示优化”这些能力。能让开发者对Agent工作流进行端到端的评估。精准定位和修复问题。这对需要快速迭代Agent的团队来说。确实能省不少事。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">不过这里我也得客观说一句。看到AgentKit。不禁让人容易想到当年OpenAI发布的GPT Store。那时候的模型主要靠上下文。没法有效的调用工具和数据。应用场景特别窄。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">现在AgentKit虽然解决了开发门槛问题。但是Agent落地的核心难题。比如复杂任务的逻辑拆解、多工具协同的容错性。以及真实场景的数据安全等等。这套框架能不能解决，目前还不好说。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">不过，至少OpenAI已经开始行动了。也许就会有找到解法的可能。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/N0X8-fsQuNY_524.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">聊完Agent，咱们再往底层看。不管是开发Agent。还是部署App，最基础的支撑都是编程。而发布会的第三个核心。就是可能会让Claude“编程王者”地位受到威胁的Codex。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">根据Sam Altman在发布会上的介绍。Codex从今年8月上线到现在。已经处理了40T的token数量。成了OpenAI增长最快的产品之一。从程序员社群的反馈来看。Codex的出现确实冲击了Anthropic Claude。现在有越来越多的程序员开始用Codex。认为它生成的代码更贴合实际的开发需求。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这次发布会。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">Codex正式从“研究预览版”转为了“正式版”，更新的重点是对企业和工程团队的深度支持。具体分为三个方面。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第一个是Slack集成。这是程序员社群呼吁了很久的功能。以前团队用Codex。得在Codex界面和Slack之间来回切。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">现在不用了。直接可以在Slack频道里调用Codex。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">它能在对话流里直接回答相关的技术问题。比如“Python怎么实现批量处理Excel”，或者直接生成代码片段。甚至能帮忙查看代码里的bug。整个流程都在团队日常沟通的场景里完成。不用再切换应用，效率提升十分明显。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/N0X8-fsQuNY_601.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第二个是全新的Codex SDK。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">以前企业用Codex，大多是“单点使用”，比如团队里的某个程序员用它来写代码；。现在有了Codex SDK。企业能够把Codex的能力作为一个“模块”，集成到自己内部的开发工作流里。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">拿企业的代码审查系统举例。现在就可以通过Codex SDK来调用Codex的能力。自动检查代码规范；。或者让内部的开发文档工具。通过Codex来自动生成API文档。这样一来。Codex就不再是“单个程序员的工具”了。而是“整个团队的开发助手”，能更好地融入企业现有的开发体系。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第三个是新的后台管理与报告工具。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">对于企业管理者来说，以前用Codex。不知道团队里谁在用、用它做了什么、有没有安全风险。现在有了这套工具，能做“环境控制”，比如限制某些团队只能使用特定的代码库、“实时监控”，查看Codex的调用频率、处理时长。以及“分析仪表盘”，统计Codex帮团队节省了多少开发时间、减少了多少bug。这些功能能让管理者清晰地掌握Codex在企业内的使用情况。也能更好地管控风险。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/N0X8-fsQuNY_670.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">但是这些更新。都不如发布会最后的那段演示令人印象深刻。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">演示者拉曼（Raman）当时说。我们来试试。只用对话，让语音助手调用Codex SDK。做一个滚动的开发者名单。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">然后他对着语音助手说。我需要在当前的前端应用里。加一个滚动展示的开发者名单。名单数据用DevDay的参会开发者信息。滚动速度要平缓。样式和现有界面保持一致。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">话音刚落。后台的Codex就开始实时修改前端的React代码。屏幕上能看到代码一行行的变化。没过几秒。页面上就出现了一个流畅滚动的开发者名单。和拉曼描述的完全一致。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这个场景。就是OpenAI预想的“未来软件开发”，你不用打开代码编辑器。不用手动敲一行代码，只用和AI对话。说出你的需求。软件就能在后台实时修改代码、自我迭代、完成功能更新。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">以前咱们说“代码生成”，主要是AI帮你写代码。而现在OpenAI想做的。是“无代码编程”。让AI直接帮你完成软件的修改和进化。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/N0X8-fsQuNY_736.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">开发者只需要负责提出需求就行了。这一步现在来看虽然还很初级。但是方向已经很明确了，未来的编程。可能真的会从写代码变成聊需求。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">除了APPS SDK AgentKit Codex这三个生态核心。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">发布会还有第四部分的内容。那就是相对传统的API更新。不过它们的重要性依然不容小觑。因为它直接降低了开发者使用OpenAI模型的门槛。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先是GPT-5 Pro的API开放。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">GPT-5 Pro是OpenAI目前最强大的模型。之前只对少数企业开放。这次正式向所有开发者开放API。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这意味着不管是小团队还是个人开发者。都能用GPT-5 Pro的能力来做产品。实现更复杂的自然语言理解、更精准的多模态处理等等。这对整个开发者生态来说。这无疑是个重大利好。其次是新的语音模型“GPT Real-time mini”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">其实OpenAI做语音模型也有段时间了。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">但是之前的模型成本太高。很多开发者想做一些语音应用。都因为成本问题望而却步。而这次的GPT Real-time mini。成本比之前的版本降低了70%，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/N0X8-fsQuNY_807.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">但是音质和情感表现力没打折扣。比如它能听出用户说话时的情绪。用对应的语气回应。也能清晰识别带有口音的语音。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">成本降了，能力没减。这就会大大降低语音应用的开发门槛。未来我们可能也会在更多场景里看到OpenAI的语音技术。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而最重要的，是Sora 2 API的开放。之前Sora 2是作为独立App上线的。大家只能在App里生成视频。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">现在开放了API。意味着开发者能够把Sora 2的视频生成能力。集成到自己的产品里。比如内容创作平台。能让用户直接在平台里生成AI视频素材；。电商平台。能让商家用Sora 2快速生成产品的宣传视频；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">甚至对于教育平台来说。也能让老师用Sora 2来生成教学动画。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">可以说Sora 2 API的开放。标志着OpenAI的顶级视频生成技术。正式从“自用”转向“生态共享”，未来我们在各种应用里。可能都能够看到用Sora 2生成的视频。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/N0X8-fsQuNY_871.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">聊完所有技术细节，我们再跳出来。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">看看OpenAI这次DevDay背后的“野心”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">它的商业帝国轮廓。其实已经越来越清晰。甚至隐隐盖住了AGI的远景图景。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">先看模型迭代，今年的GPT-5 Pro。确实强大。但是已经没有GPT-4发布时那种“跨时代的惊艳感”了。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">GPT-4当时让大家觉得。AI居然能做到这个程度。而GPT-5 Pro更像是在GPT-4的基础上。做了稳健的优化。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如处理速度更快、上下文窗口更大、多模态能力更协调。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">但是没有突破大家的预期。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">Sora 2也是一样，它的惊艳之处。不是视频生成技术比更早它发布的Veo 3强多少。而是产品团队精准抓住了社交媒体的爆点。把真实人物无缝的融入到了AI生成的视频中。这是个天才的商业构想。能够快速吸引用户、占领市场。但是它更多是“商业嗅觉”的胜利。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而非底层技术的革命。再看这次DevDay的核心。Apps SDK、AgentKit、Codex. 这三者其实都是在“搭建生态”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/N0X8-fsQuNY_933.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">Apps SDK把所有应用拉进ChatGPT。让ChatGPT成为唯一入口；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">AgentKit统一了Agent的开发标准。让开发者都来用OpenAI的工具做Agent；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">Codex深入到企业的开发流程中。让企业离不开OpenAI的编程支持。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">OpenAI的整个逻辑很清晰。那就是想要以自家大模型为核心。搭建一个封闭而且具有强掌控力的软件生态。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">开发者们在这个生态里开发。用户们在这个生态里使用。所有数据和流量都留在OpenAI的体系内。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这时候咱们再回过头来看看OpenAI早年的气质。那时候它带着一种“神秘感”，总在探索未知领域。比如最早的GPT-3、DALL-E。每次发布都让大家觉得。离AGI又近了一步。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">那种对技术边界的突破。能让人肾上腺素飙升。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">但是现在的OpenAI。更像一个“成熟的商业公司”，它不再追求“惊艳感”，而是追求“生态掌控力”；。不再强调“探索AGI”，而是强调“商业落地”。它的商业帝国轮廓也越来越清晰。但是曾经那种“为了AGI而探索未知”的气质。却在慢慢的褪色。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/N0X8-fsQuNY_1003.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">当然，这并不是说OpenAI做错了。企业要发展，必然要考虑商业落地。要搭建生态壁垒。但是作为关注AI行业的人。我们难免会有些感慨。当OpenAI把更多精力放在“掌控生态”上。它还能像以前那样。为AGI的突破投入足够的资源吗？</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">AGI的远景。会不会慢慢变成“商业帝国”的附属品？</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这个问题，可能需要时间来回答。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">不过不管怎么说。这次DevDay都给整个AI行业指明了一个方向。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">AI的下一个阶段。不再是“单点技术突破”，而是“生态整合”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">谁能把应用、Agent、编程工具都整合在自己的体系内。谁就能在未来的竞争中占据优势。对开发者来说，这是机遇，也是挑战；。对用户来说。未来我们可能真的会进入。一个对话框解决所有问题的时代。但同时也要接受所有需求都在一个生态里完成的现状。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">最后，我想问问。大家觉得OpenAI的这个生态蓝图。会让AI变得更加实用。还是会让行业陷入到“生态垄断”中呢？你会用Apps SDK来做自己的应用。还是继续用独立工具呢？欢迎在评论区留言交流。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/N0X8-fsQuNY_1074.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">好了。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">今天对OpenAI DevDay的解析就到这里。感谢收看，我们下期再见。</p>

<hr style="clear:both" />
=============
<p><a href="https://www.youtube.com/watch?v=N0X8-fsQuNY">https://www.youtube.com/watch?v=N0X8-fsQuNY</a></p><p>美国时间10月6日举办的OpenAI DevDay，就是它真正亮出“战略蓝图”的时刻。这场发布会没有太多颠覆性的“黑科技”，却把过去两年AI技术的演进，整合成了一套系统化、平台化的生态方案，目标很明确，那就是要构建一个以AI为绝对核心的全新软件生态体系。今天咱们就来回顾一下这场发布会的内容，看看OpenAI的“商业帝国”到底在如何布局。</p><p><a href="https://www.youtube.com/live/hS1YqcewH0c?si=pfOTZjGf2Q_jKR6P">https://www.youtube.com/live/hS1YqcewH0c?si=pfOTZjGf2Q_jKR6P</a></p>]]>
      </itunes:summary>
      <description>
        <![CDATA[<hr style="clear:both" />

<p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://img.youtube.com/vi/N0X8-fsQuNY/maxresdefault.jpg" /></p><p><a href="https://www.youtube.com/watch?v=N0X8-fsQuNY">https://www.youtube.com/watch?v=N0X8-fsQuNY</a></p><h1>值得閱讀的理由</h1> <ul><li>深入了解OpenAI如何從技術展示轉向構建以AI為核心的<strong>全新軟體生態系統</strong>，掌握其商業藍圖。</li><li>學習Apps SDK、AgentKit和Codex這三大核心工具如何徹底改變軟體開發和使用者互動模式，體驗<strong>「無代碼編程」</strong>和<strong>「對話即應用」</strong>的未來。</li><li>分析OpenAI策略轉變的深層影響，思考其在追求商業落地與AGI（通用人工智慧）願景之間，如何平衡與取捨，以及這對整個AI行業的意義。</li></ul><hr /><h1>摘要</h1> <h2>OpenAI戰略藍圖：構建AI核心軟體生態</h2> <p>近期，OpenAI在AI領域動作頻頻，繼Sora 2震撼影片生成能力後，其DevDay發布會更揭示了OpenAI的宏大<strong>戰略藍圖</strong>。影片中說故事的人指出，這場發布會的目標是構建一個以AI為絕對核心的全新軟體生態體系，其核心內容涵蓋了三大板塊：<strong>Apps SDK</strong>、<strong>AgentKit</strong>和<strong>Codex</strong>，這三者共同支撐起OpenAI未來的商業帝國布局。作者強調，OpenAI不再僅是展示尖端技術，而是將AI技術演進整合為一套系統化、平台化的生態方案，旨在全面掌控未來的軟體世界。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/N0X8-fsQuNY_75.jpg" /></p> <p>首先登場的是<strong>Apps SDK</strong>，它旨在讓<strong>「大語言模型成為所有軟體唯一的入口」</strong>。作者解釋，Apps SDK是一套完整的開發堆棧，允許開發者直接在ChatGPT內部構建真實、可交互的應用程式，透過連接數據、觸發操作甚至渲染用戶界面。這與過去僅以文本形式返回結果的工具截然不同，它讓用戶能像在電腦上使用App一樣，在大語言模型中<strong>「完成操作」</strong>。影片中生動的Canva和Zillow案例展示了Apps SDK如何讓用戶在不切換軟體的情況下，透過對話完成從海報設計到房產篩選的複雜任務，實現了工具之間的<strong>「協作流」</strong>。支撐這一切的是OpenAI的MCP體系和GPT本身的圖像識別能力，以及關鍵的<strong>上下文記憶能力</strong>，使應用程式成為能隨時喚醒、理解對話背景的「插件」。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/N0X8-fsQuNY_330.jpg" /></p> <h2>AgentKit：推動Agent時代的可視化開發</h2> <p>接著，OpenAI推出了<strong>AgentKit</strong>，旨在降低開發門檻，<strong>「推動Agent時代」</strong>。作者提到，AgentKit的核心是<strong>「Agent Builder」</strong>，一個可視化畫布，讓開發者無需編寫代碼，只需透過拖拽和連接功能節點，就能直觀設計複雜業務流程。它還包含<strong>「ChatKit」</strong>（可嵌入的聊天組件）和<strong>「Connector」</strong>（連接Agent與企業內部數據工具），大幅簡化了Agent的開發與整合。影片中產品經理僅用8分鐘就為DevDay官網搭建並上線智能問答Agent的演示，證明了AgentKit的<strong>「精簡」</strong>和<strong>「易用性」</strong>。此外，AgentKit提供兩個「殺手鐧」：一是<strong>RFT定制</strong>，允許開發者深入模型內部，優化GPT-5的推理模型，並訓練其在最恰當的時機調用工具；二是<strong>Evals評估板塊</strong>，提供數據集構建、跟踪評估和自動化提示優化能力，幫助開發者精準定位並修復Agent工作流中的問題。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/N0X8-fsQuNY_660.jpg" /></p> <h2>Codex：實現「無代碼編程」的開發助手</h2> <p>在底層編程方面，發布會的第三個核心是可能威脅Claude地位的<strong>Codex</strong>。作者指出，Codex已是OpenAI增長最快的產品之一，受到程式設計師社群的廣泛好評，認為其生成的代碼更貼合實際需求。Codex正式版專注於對企業和工程團隊的深度支持，提供了三大功能：一是<strong>Slack集成</strong>，讓團隊成員直接在Slack頻道內調用Codex，回答技術問題、生成代碼片段甚至檢查bug，顯著提升溝通效率；二是全新的<strong>Codex SDK</strong>，使企業能將Codex能力作為「模塊」集成到內部開發工作流，實現自動化代碼審查或API文檔生成，將Codex轉變為<strong>「整個團隊的開發助手」</strong>；三是新的<strong>後台管理與報告工具</strong>，讓管理者能對Codex使用情況進行環境控制、實時監控與分析。最令人印象深刻的演示是，僅透過對話，語音助手就成功調用Codex SDK，實時修改前端代碼，完成滾動開發者名單的功能更新，預示著OpenAI所設想的<strong>「無代碼編程」</strong>未來，即AI能直接根據需求修改和進化軟體，開發者只需負責提出需求。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/N0X8-fsQuNY_915.jpg" /></p> <h2>API更新：降低開發門檻與生態共享</h2> <p>除了上述三大核心，DevDay還帶來了多項傳統但重要的API更新，旨在降低開發者使用OpenAI模型的門檻。作者強調，首先是<strong>GPT-5 Pro的API開放</strong>，讓所有開發者都能利用OpenAI目前最強大的模型，實現更複雜的自然語言理解與多模態處理。其次是新的語音模型<strong>「GPT Real-time mini」</strong>，其成本降低了70%，同時保持了出色的音質與情感表現力，大幅降低了語音應用的開發門檻。最關鍵的是<strong>Sora 2 API的開放</strong>，這意味著開發者能夠將Sora 2的頂級視頻生成能力集成到自己的產品中，無論是內容創作、電商宣傳還是教育動畫，都將看到Sora 2的廣泛應用，標誌著OpenAI技術從「自用」走向<strong>「生態共享」</strong>。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/N0X8-fsQuNY_1065.jpg" /></p> <h2>OpenAI的商業帝國輪廓與AGI願景之辯</h2> <p>在總結部分，作者跳脫技術細節，探討了OpenAI這次DevDay背後的<strong>「野心」</strong>。作者認為，OpenAI的商業帝國輪廓已日益清晰，甚至隱約蓋過了AGI的遠景圖景。他指出，GPT-5 Pro的迭代更像是穩健優化，而非GPT-4那樣的「跨時代驚艷感」；Sora 2的成功也更多是「商業嗅覺」的勝利，而非底層技術革命。作者強調，Apps SDK、AgentKit和Codex這三者，本質上都是在<strong>「搭建生態」</strong>，目標是建立一個以OpenAI大模型為核心的<strong>封閉且強掌控力的軟體生態系統</strong>，讓所有數據和流量都留在OpenAI體系內。作者感慨，如今的OpenAI更像一家<strong>「成熟的商業公司」</strong>，追求的是「生態掌控力」和「商業落地」，而非早期那種為AGI而探索未知的「神秘感」。他提出疑問：當OpenAI將更多精力放在「掌控生態」上，它還能像以前那樣為AGI的突破投入足夠資源嗎？AGI的遠景會不會慢慢變成「商業帝國」的附屬品？雖然這些問題需要時間來回答，但這次DevDay明確指出AI的下一個階段是<strong>「生態整合」</strong>，誰能將應用、Agent、編程工具整合在自己的體系內，誰就能在未來競爭中佔據優勢。對開發者而言是機遇與挑戰，對用戶而言則可能是對話框解決所有問題，但也可能陷入「生態壟斷」的時代。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/N0X8-fsQuNY_22.jpg" /></p><hr /><hr />================================================</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">大家好，这里是最佳拍档，我是大飞。相信最近关注AI圈的朋友。都被OpenAI的动静刷屏了。前几天，Sora 2刚登顶App榜。那种细腻到发丝、流畅到自然的视频生成效果。让不少人直呼“AI视频时代真的来了”。但是如果说Sora 2是OpenAI在“秀技术肌肉”，那美国时间10月6日举办的OpenAI DevDay。就是它真正亮出“战略蓝图”的时刻。这场发布会没有太多颠覆性的“黑科技”，却把过去两年AI技术的演进。整合成了一套系统化、平台化的生态方案。目标很明确。那就是要构建一个以AI为绝对核心的全新软件生态体系。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">今天咱们就来回顾一下这场发布会的内容。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">看看OpenAI的“商业帝国”到底在如何布局。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先。咱们得先明确这场发布会的整体框架。除了常规的API更新。核心内容其实是三个板块。Apps SDK、AgentKit、Codex. 这三者共同撑起了OpenAI的未来生态。而第一个要聊的。就是能让“大语言模型成为所有软件唯一入口”的Apps SDK。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/N0X8-fsQuNY_71.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">可能有朋友还记得，2024年的时候。“把大语言模型当软件入口”，还只是个模糊的概念。大家觉得AI能帮着调用工具。但总觉得差点意思。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">可到了2025年。这个概念已经成了行业共识。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而OpenAI的Apps SDK。就是把这个共识落地的关键工具。它不是一个简单的“插件集合”，而是一套完整的开发堆栈。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">开发者用它。能在ChatGPT内部直接构建真实、可交互的应用程序。通过连接自己的数据、触发具体的操作。甚至可以渲染出交互式的用户界面。这和之前的工具不一样的地方在于。像Claude和GitHub Copilot这些。它们虽然也能调用外部服务。但是最终只能把结果以文本的形式返回。缺乏真正的上下文理解和自然的交互界面。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">简单来说。过去的工具是“告诉你结果”，而Apps SDK是“帮你完成操作”，让用户在大语言模型里像在电脑上一样来使用App。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/N0X8-fsQuNY_133.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">发布会现场有个非常生动的案例。我给大家还原一下。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">假设你正在为宠物狗业务做头脑风暴。聊到一半想做海报。这时候你不用退出ChatGPT。直接“@”出Canva。重点来了。这个Canva能完全理解你之前和ChatGPT聊的所有点子。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如你说，要色彩丰富、异想天开。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">它不用你再重复解释。直接就能生成一系列符合要求的精美海报。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">如果这时候你灵感来了，说。请把这张海报扩展成商业计划书。它也能无缝衔接。直接帮你把商业计划书的框架搭好。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">等你想扩张业务，需要找场地的时候。ChatGPT还会根据你之前聊的“宠物狗业务”上下文。主动建议你考虑“匹兹堡”这个城市。然后直接唤起房产平台Zillow。全屏展示。你在ChatGPT里说“要带院子的三居室”，它就会帮你进行筛选。筛选完你再问“这个房子离狗狗公园有多远”，它还能基于Zillow的数据直接回答。整个过程没有一次软件切换。所有操作都在ChatGPT的对话框里完成。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/N0X8-fsQuNY_202.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">工具在最需要的时候自动出现。还能协同工作。这背后就是Apps SDK的核心能力。打破应用之间的壁垒。让不同工具在统一的入口里形成“协作流”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而支撑这一切的，是OpenAI的MCP体系。开发者依靠MCP能设计应用的逻辑和界面。再结合GPT本身的图像识别能力。让App不是“硬塞进”对话。而是“自然的融入”对话。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">更关键的是上下文记忆能力。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">发布会里还有个细节。ChatGPT在另一个新对话里。还能延续上一个对话的宠物狗业务话题。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这种跨对话的记忆。正是大语言模型能够作为软件入口的核心竞争力。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">以前的App是“功能孤岛”，你打开才能用；。现在有了Apps SDK。App成了“可随时唤醒的插件”，还能理解你的对话背景。真正做到了万物在大语言模型内。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">聊完Apps SDK，我们自然要问。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">怎么让这些App更好地适应用户需求。和对话更紧密地连接呢？</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/N0X8-fsQuNY_266.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这就轮到发布会的第二个核心。AgentKit了。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">行业里其实早就把2025年称作“Agent元年”，但是直到10月。市场上都没出现足够引发行业震荡的现象级Agent产品。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">显然，大家都觉得Agent是未来。但是开发门槛太高、落地太难。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而OpenAI这次推出AgentKit。就是想“推一把”Agent时代。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">它号称是“最简便、快捷”的Agent开发工具包。核心思路就是让一切回归可视化。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">AgentKit的核心是“Agent Builder”，这是一个可视化的画布。以前开发者做Agent，得从零写代码。逻辑错一点就得从头改。而现在用Agent Builder，不用写代码。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">通过直接拖拽、连接不同的功能节点。就能直观地设计复杂的业务流程。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">除了画布，它还有两个关键的功能。一个是“ChatKit”，这是一个可嵌入的聊天组件。开发者能轻松把带品牌定制能力的聊天界面。集成到自己的应用里。不用自己再来做界面设计了；。另一个是“Connector”，能够直接把AgentKit里构建的Agent工具。和企业内部的数据、工具连起来。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/N0X8-fsQuNY_339.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如企业的客户数据库、内部办公系统等等。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这样就不用再去做复杂的接口开发了。直接和企业已有的系统打通。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">发布会现场。产品经理克里斯蒂娜（Christina）做了个演示。从零开始，只用了8分钟。就为一个静态的DevDay活动官网。搭建并且上线了一个智能问答Agent。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">从演示里能看到。AgentKit的功能节点特别精简。只有三个核心。分别是Agent、End和Note。不同Agent之间的逻辑关系。也只用三个点就能控制。分别是条件触发。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如用户问时间就调用A Agent。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">问地点就调用B Agent，还有同时进行。让多个Agent并行处理任务。以及用户许可。也就是需要用户确认后才继续。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">每个Agent内部，还能添加三个工具。包括文件搜索、安全防护和MCP应用。当然。你也能把这些工具作为独立的功能点。加在流程外部。整个设计逻辑十分清晰。哪怕是没做过Agent开发的人。看一眼也能明白个大概。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/N0X8-fsQuNY_401.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">不过，客观地说，单从设计逻辑上看。AgentKit并没有比现在市场上的Dify、Coze这些工具领先太多。但是它胜在“精简”和“易用”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">为了让习惯了Dify的用户转用AgentKit。OpenAI还提供了两个“杀手锏”。第一个是RFT定制。现在第三方工具用GPT。都只能把它当做“黑箱”一样来调用API。你不知道模型内部是怎么推理的。也没法优化。但是AgentKit能深入模型的内部。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">目前OpenAI已经在实验GPT-5的RFT功能。开发者通过RFT。不仅能定制GPT-5的推理模型。还能专门训练模型在最恰当的时机、用最优的方式来调用工具。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如你想做一个客服Agent。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">用RFT能让模型学会。用户问售后问题的时候。先调用订单数据库查询信息。再调用售后流程工具提交申请。而不是乱调用工具。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这对想基于GPT来开发Agent的企业来说。吸引力非常的大。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/N0X8-fsQuNY_462.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第二个是AgentKit的“Evals评估板块”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">做Agent开发。最头疼的就是“不知道问题在哪”。比如流程跑不通，到底是节点错了。还是工具调用时机不对呢？Evals就直接解决了这个问题。它提供了“数据集构建”、“跟踪评估”、“自动化提示优化”这些能力。能让开发者对Agent工作流进行端到端的评估。精准定位和修复问题。这对需要快速迭代Agent的团队来说。确实能省不少事。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">不过这里我也得客观说一句。看到AgentKit。不禁让人容易想到当年OpenAI发布的GPT Store。那时候的模型主要靠上下文。没法有效的调用工具和数据。应用场景特别窄。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">现在AgentKit虽然解决了开发门槛问题。但是Agent落地的核心难题。比如复杂任务的逻辑拆解、多工具协同的容错性。以及真实场景的数据安全等等。这套框架能不能解决，目前还不好说。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">不过，至少OpenAI已经开始行动了。也许就会有找到解法的可能。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/N0X8-fsQuNY_524.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">聊完Agent，咱们再往底层看。不管是开发Agent。还是部署App，最基础的支撑都是编程。而发布会的第三个核心。就是可能会让Claude“编程王者”地位受到威胁的Codex。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">根据Sam Altman在发布会上的介绍。Codex从今年8月上线到现在。已经处理了40T的token数量。成了OpenAI增长最快的产品之一。从程序员社群的反馈来看。Codex的出现确实冲击了Anthropic Claude。现在有越来越多的程序员开始用Codex。认为它生成的代码更贴合实际的开发需求。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这次发布会。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">Codex正式从“研究预览版”转为了“正式版”，更新的重点是对企业和工程团队的深度支持。具体分为三个方面。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第一个是Slack集成。这是程序员社群呼吁了很久的功能。以前团队用Codex。得在Codex界面和Slack之间来回切。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">现在不用了。直接可以在Slack频道里调用Codex。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">它能在对话流里直接回答相关的技术问题。比如“Python怎么实现批量处理Excel”，或者直接生成代码片段。甚至能帮忙查看代码里的bug。整个流程都在团队日常沟通的场景里完成。不用再切换应用，效率提升十分明显。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/N0X8-fsQuNY_601.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第二个是全新的Codex SDK。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">以前企业用Codex，大多是“单点使用”，比如团队里的某个程序员用它来写代码；。现在有了Codex SDK。企业能够把Codex的能力作为一个“模块”，集成到自己内部的开发工作流里。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">拿企业的代码审查系统举例。现在就可以通过Codex SDK来调用Codex的能力。自动检查代码规范；。或者让内部的开发文档工具。通过Codex来自动生成API文档。这样一来。Codex就不再是“单个程序员的工具”了。而是“整个团队的开发助手”，能更好地融入企业现有的开发体系。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第三个是新的后台管理与报告工具。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">对于企业管理者来说，以前用Codex。不知道团队里谁在用、用它做了什么、有没有安全风险。现在有了这套工具，能做“环境控制”，比如限制某些团队只能使用特定的代码库、“实时监控”，查看Codex的调用频率、处理时长。以及“分析仪表盘”，统计Codex帮团队节省了多少开发时间、减少了多少bug。这些功能能让管理者清晰地掌握Codex在企业内的使用情况。也能更好地管控风险。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/N0X8-fsQuNY_670.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">但是这些更新。都不如发布会最后的那段演示令人印象深刻。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">演示者拉曼（Raman）当时说。我们来试试。只用对话，让语音助手调用Codex SDK。做一个滚动的开发者名单。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">然后他对着语音助手说。我需要在当前的前端应用里。加一个滚动展示的开发者名单。名单数据用DevDay的参会开发者信息。滚动速度要平缓。样式和现有界面保持一致。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">话音刚落。后台的Codex就开始实时修改前端的React代码。屏幕上能看到代码一行行的变化。没过几秒。页面上就出现了一个流畅滚动的开发者名单。和拉曼描述的完全一致。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这个场景。就是OpenAI预想的“未来软件开发”，你不用打开代码编辑器。不用手动敲一行代码，只用和AI对话。说出你的需求。软件就能在后台实时修改代码、自我迭代、完成功能更新。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">以前咱们说“代码生成”，主要是AI帮你写代码。而现在OpenAI想做的。是“无代码编程”。让AI直接帮你完成软件的修改和进化。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/N0X8-fsQuNY_736.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">开发者只需要负责提出需求就行了。这一步现在来看虽然还很初级。但是方向已经很明确了，未来的编程。可能真的会从写代码变成聊需求。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">除了APPS SDK AgentKit Codex这三个生态核心。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">发布会还有第四部分的内容。那就是相对传统的API更新。不过它们的重要性依然不容小觑。因为它直接降低了开发者使用OpenAI模型的门槛。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先是GPT-5 Pro的API开放。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">GPT-5 Pro是OpenAI目前最强大的模型。之前只对少数企业开放。这次正式向所有开发者开放API。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这意味着不管是小团队还是个人开发者。都能用GPT-5 Pro的能力来做产品。实现更复杂的自然语言理解、更精准的多模态处理等等。这对整个开发者生态来说。这无疑是个重大利好。其次是新的语音模型“GPT Real-time mini”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">其实OpenAI做语音模型也有段时间了。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">但是之前的模型成本太高。很多开发者想做一些语音应用。都因为成本问题望而却步。而这次的GPT Real-time mini。成本比之前的版本降低了70%，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/N0X8-fsQuNY_807.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">但是音质和情感表现力没打折扣。比如它能听出用户说话时的情绪。用对应的语气回应。也能清晰识别带有口音的语音。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">成本降了，能力没减。这就会大大降低语音应用的开发门槛。未来我们可能也会在更多场景里看到OpenAI的语音技术。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而最重要的，是Sora 2 API的开放。之前Sora 2是作为独立App上线的。大家只能在App里生成视频。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">现在开放了API。意味着开发者能够把Sora 2的视频生成能力。集成到自己的产品里。比如内容创作平台。能让用户直接在平台里生成AI视频素材；。电商平台。能让商家用Sora 2快速生成产品的宣传视频；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">甚至对于教育平台来说。也能让老师用Sora 2来生成教学动画。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">可以说Sora 2 API的开放。标志着OpenAI的顶级视频生成技术。正式从“自用”转向“生态共享”，未来我们在各种应用里。可能都能够看到用Sora 2生成的视频。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/N0X8-fsQuNY_871.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">聊完所有技术细节，我们再跳出来。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">看看OpenAI这次DevDay背后的“野心”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">它的商业帝国轮廓。其实已经越来越清晰。甚至隐隐盖住了AGI的远景图景。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">先看模型迭代，今年的GPT-5 Pro。确实强大。但是已经没有GPT-4发布时那种“跨时代的惊艳感”了。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">GPT-4当时让大家觉得。AI居然能做到这个程度。而GPT-5 Pro更像是在GPT-4的基础上。做了稳健的优化。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如处理速度更快、上下文窗口更大、多模态能力更协调。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">但是没有突破大家的预期。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">Sora 2也是一样，它的惊艳之处。不是视频生成技术比更早它发布的Veo 3强多少。而是产品团队精准抓住了社交媒体的爆点。把真实人物无缝的融入到了AI生成的视频中。这是个天才的商业构想。能够快速吸引用户、占领市场。但是它更多是“商业嗅觉”的胜利。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而非底层技术的革命。再看这次DevDay的核心。Apps SDK、AgentKit、Codex. 这三者其实都是在“搭建生态”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/N0X8-fsQuNY_933.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">Apps SDK把所有应用拉进ChatGPT。让ChatGPT成为唯一入口；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">AgentKit统一了Agent的开发标准。让开发者都来用OpenAI的工具做Agent；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">Codex深入到企业的开发流程中。让企业离不开OpenAI的编程支持。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">OpenAI的整个逻辑很清晰。那就是想要以自家大模型为核心。搭建一个封闭而且具有强掌控力的软件生态。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">开发者们在这个生态里开发。用户们在这个生态里使用。所有数据和流量都留在OpenAI的体系内。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这时候咱们再回过头来看看OpenAI早年的气质。那时候它带着一种“神秘感”，总在探索未知领域。比如最早的GPT-3、DALL-E。每次发布都让大家觉得。离AGI又近了一步。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">那种对技术边界的突破。能让人肾上腺素飙升。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">但是现在的OpenAI。更像一个“成熟的商业公司”，它不再追求“惊艳感”，而是追求“生态掌控力”；。不再强调“探索AGI”，而是强调“商业落地”。它的商业帝国轮廓也越来越清晰。但是曾经那种“为了AGI而探索未知”的气质。却在慢慢的褪色。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/N0X8-fsQuNY_1003.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">当然，这并不是说OpenAI做错了。企业要发展，必然要考虑商业落地。要搭建生态壁垒。但是作为关注AI行业的人。我们难免会有些感慨。当OpenAI把更多精力放在“掌控生态”上。它还能像以前那样。为AGI的突破投入足够的资源吗？</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">AGI的远景。会不会慢慢变成“商业帝国”的附属品？</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这个问题，可能需要时间来回答。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">不过不管怎么说。这次DevDay都给整个AI行业指明了一个方向。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">AI的下一个阶段。不再是“单点技术突破”，而是“生态整合”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">谁能把应用、Agent、编程工具都整合在自己的体系内。谁就能在未来的竞争中占据优势。对开发者来说，这是机遇，也是挑战；。对用户来说。未来我们可能真的会进入。一个对话框解决所有问题的时代。但同时也要接受所有需求都在一个生态里完成的现状。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">最后，我想问问。大家觉得OpenAI的这个生态蓝图。会让AI变得更加实用。还是会让行业陷入到“生态垄断”中呢？你会用Apps SDK来做自己的应用。还是继续用独立工具呢？欢迎在评论区留言交流。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/N0X8-fsQuNY_1074.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">好了。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">今天对OpenAI DevDay的解析就到这里。感谢收看，我们下期再见。</p>

<hr style="clear:both" />
=============
<p><a href="https://www.youtube.com/watch?v=N0X8-fsQuNY">https://www.youtube.com/watch?v=N0X8-fsQuNY</a></p><p>美国时间10月6日举办的OpenAI DevDay，就是它真正亮出“战略蓝图”的时刻。这场发布会没有太多颠覆性的“黑科技”，却把过去两年AI技术的演进，整合成了一套系统化、平台化的生态方案，目标很明确，那就是要构建一个以AI为绝对核心的全新软件生态体系。今天咱们就来回顾一下这场发布会的内容，看看OpenAI的“商业帝国”到底在如何布局。</p><p><a href="https://www.youtube.com/live/hS1YqcewH0c?si=pfOTZjGf2Q_jKR6P">https://www.youtube.com/live/hS1YqcewH0c?si=pfOTZjGf2Q_jKR6P</a></p>]]>
      </description>
      <content:encoded><![CDATA[<hr style="clear:both" />

<p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://img.youtube.com/vi/N0X8-fsQuNY/maxresdefault.jpg" /></p><p><a href="https://www.youtube.com/watch?v=N0X8-fsQuNY">https://www.youtube.com/watch?v=N0X8-fsQuNY</a></p><h1>值得閱讀的理由</h1> <ul><li>深入了解OpenAI如何從技術展示轉向構建以AI為核心的<strong>全新軟體生態系統</strong>，掌握其商業藍圖。</li><li>學習Apps SDK、AgentKit和Codex這三大核心工具如何徹底改變軟體開發和使用者互動模式，體驗<strong>「無代碼編程」</strong>和<strong>「對話即應用」</strong>的未來。</li><li>分析OpenAI策略轉變的深層影響，思考其在追求商業落地與AGI（通用人工智慧）願景之間，如何平衡與取捨，以及這對整個AI行業的意義。</li></ul><hr /><h1>摘要</h1> <h2>OpenAI戰略藍圖：構建AI核心軟體生態</h2> <p>近期，OpenAI在AI領域動作頻頻，繼Sora 2震撼影片生成能力後，其DevDay發布會更揭示了OpenAI的宏大<strong>戰略藍圖</strong>。影片中說故事的人指出，這場發布會的目標是構建一個以AI為絕對核心的全新軟體生態體系，其核心內容涵蓋了三大板塊：<strong>Apps SDK</strong>、<strong>AgentKit</strong>和<strong>Codex</strong>，這三者共同支撐起OpenAI未來的商業帝國布局。作者強調，OpenAI不再僅是展示尖端技術，而是將AI技術演進整合為一套系統化、平台化的生態方案，旨在全面掌控未來的軟體世界。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/N0X8-fsQuNY_75.jpg" /></p> <p>首先登場的是<strong>Apps SDK</strong>，它旨在讓<strong>「大語言模型成為所有軟體唯一的入口」</strong>。作者解釋，Apps SDK是一套完整的開發堆棧，允許開發者直接在ChatGPT內部構建真實、可交互的應用程式，透過連接數據、觸發操作甚至渲染用戶界面。這與過去僅以文本形式返回結果的工具截然不同，它讓用戶能像在電腦上使用App一樣，在大語言模型中<strong>「完成操作」</strong>。影片中生動的Canva和Zillow案例展示了Apps SDK如何讓用戶在不切換軟體的情況下，透過對話完成從海報設計到房產篩選的複雜任務，實現了工具之間的<strong>「協作流」</strong>。支撐這一切的是OpenAI的MCP體系和GPT本身的圖像識別能力，以及關鍵的<strong>上下文記憶能力</strong>，使應用程式成為能隨時喚醒、理解對話背景的「插件」。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/N0X8-fsQuNY_330.jpg" /></p> <h2>AgentKit：推動Agent時代的可視化開發</h2> <p>接著，OpenAI推出了<strong>AgentKit</strong>，旨在降低開發門檻，<strong>「推動Agent時代」</strong>。作者提到，AgentKit的核心是<strong>「Agent Builder」</strong>，一個可視化畫布，讓開發者無需編寫代碼，只需透過拖拽和連接功能節點，就能直觀設計複雜業務流程。它還包含<strong>「ChatKit」</strong>（可嵌入的聊天組件）和<strong>「Connector」</strong>（連接Agent與企業內部數據工具），大幅簡化了Agent的開發與整合。影片中產品經理僅用8分鐘就為DevDay官網搭建並上線智能問答Agent的演示，證明了AgentKit的<strong>「精簡」</strong>和<strong>「易用性」</strong>。此外，AgentKit提供兩個「殺手鐧」：一是<strong>RFT定制</strong>，允許開發者深入模型內部，優化GPT-5的推理模型，並訓練其在最恰當的時機調用工具；二是<strong>Evals評估板塊</strong>，提供數據集構建、跟踪評估和自動化提示優化能力，幫助開發者精準定位並修復Agent工作流中的問題。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/N0X8-fsQuNY_660.jpg" /></p> <h2>Codex：實現「無代碼編程」的開發助手</h2> <p>在底層編程方面，發布會的第三個核心是可能威脅Claude地位的<strong>Codex</strong>。作者指出，Codex已是OpenAI增長最快的產品之一，受到程式設計師社群的廣泛好評，認為其生成的代碼更貼合實際需求。Codex正式版專注於對企業和工程團隊的深度支持，提供了三大功能：一是<strong>Slack集成</strong>，讓團隊成員直接在Slack頻道內調用Codex，回答技術問題、生成代碼片段甚至檢查bug，顯著提升溝通效率；二是全新的<strong>Codex SDK</strong>，使企業能將Codex能力作為「模塊」集成到內部開發工作流，實現自動化代碼審查或API文檔生成，將Codex轉變為<strong>「整個團隊的開發助手」</strong>；三是新的<strong>後台管理與報告工具</strong>，讓管理者能對Codex使用情況進行環境控制、實時監控與分析。最令人印象深刻的演示是，僅透過對話，語音助手就成功調用Codex SDK，實時修改前端代碼，完成滾動開發者名單的功能更新，預示著OpenAI所設想的<strong>「無代碼編程」</strong>未來，即AI能直接根據需求修改和進化軟體，開發者只需負責提出需求。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/N0X8-fsQuNY_915.jpg" /></p> <h2>API更新：降低開發門檻與生態共享</h2> <p>除了上述三大核心，DevDay還帶來了多項傳統但重要的API更新，旨在降低開發者使用OpenAI模型的門檻。作者強調，首先是<strong>GPT-5 Pro的API開放</strong>，讓所有開發者都能利用OpenAI目前最強大的模型，實現更複雜的自然語言理解與多模態處理。其次是新的語音模型<strong>「GPT Real-time mini」</strong>，其成本降低了70%，同時保持了出色的音質與情感表現力，大幅降低了語音應用的開發門檻。最關鍵的是<strong>Sora 2 API的開放</strong>，這意味著開發者能夠將Sora 2的頂級視頻生成能力集成到自己的產品中，無論是內容創作、電商宣傳還是教育動畫，都將看到Sora 2的廣泛應用，標誌著OpenAI技術從「自用」走向<strong>「生態共享」</strong>。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/N0X8-fsQuNY_1065.jpg" /></p> <h2>OpenAI的商業帝國輪廓與AGI願景之辯</h2> <p>在總結部分，作者跳脫技術細節，探討了OpenAI這次DevDay背後的<strong>「野心」</strong>。作者認為，OpenAI的商業帝國輪廓已日益清晰，甚至隱約蓋過了AGI的遠景圖景。他指出，GPT-5 Pro的迭代更像是穩健優化，而非GPT-4那樣的「跨時代驚艷感」；Sora 2的成功也更多是「商業嗅覺」的勝利，而非底層技術革命。作者強調，Apps SDK、AgentKit和Codex這三者，本質上都是在<strong>「搭建生態」</strong>，目標是建立一個以OpenAI大模型為核心的<strong>封閉且強掌控力的軟體生態系統</strong>，讓所有數據和流量都留在OpenAI體系內。作者感慨，如今的OpenAI更像一家<strong>「成熟的商業公司」</strong>，追求的是「生態掌控力」和「商業落地」，而非早期那種為AGI而探索未知的「神秘感」。他提出疑問：當OpenAI將更多精力放在「掌控生態」上，它還能像以前那樣為AGI的突破投入足夠資源嗎？AGI的遠景會不會慢慢變成「商業帝國」的附屬品？雖然這些問題需要時間來回答，但這次DevDay明確指出AI的下一個階段是<strong>「生態整合」</strong>，誰能將應用、Agent、編程工具整合在自己的體系內，誰就能在未來競爭中佔據優勢。對開發者而言是機遇與挑戰，對用戶而言則可能是對話框解決所有問題，但也可能陷入「生態壟斷」的時代。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/N0X8-fsQuNY_22.jpg" /></p><hr /><hr />================================================</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">大家好，这里是最佳拍档，我是大飞。相信最近关注AI圈的朋友。都被OpenAI的动静刷屏了。前几天，Sora 2刚登顶App榜。那种细腻到发丝、流畅到自然的视频生成效果。让不少人直呼“AI视频时代真的来了”。但是如果说Sora 2是OpenAI在“秀技术肌肉”，那美国时间10月6日举办的OpenAI DevDay。就是它真正亮出“战略蓝图”的时刻。这场发布会没有太多颠覆性的“黑科技”，却把过去两年AI技术的演进。整合成了一套系统化、平台化的生态方案。目标很明确。那就是要构建一个以AI为绝对核心的全新软件生态体系。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">今天咱们就来回顾一下这场发布会的内容。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">看看OpenAI的“商业帝国”到底在如何布局。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先。咱们得先明确这场发布会的整体框架。除了常规的API更新。核心内容其实是三个板块。Apps SDK、AgentKit、Codex. 这三者共同撑起了OpenAI的未来生态。而第一个要聊的。就是能让“大语言模型成为所有软件唯一入口”的Apps SDK。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/N0X8-fsQuNY_71.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">可能有朋友还记得，2024年的时候。“把大语言模型当软件入口”，还只是个模糊的概念。大家觉得AI能帮着调用工具。但总觉得差点意思。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">可到了2025年。这个概念已经成了行业共识。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而OpenAI的Apps SDK。就是把这个共识落地的关键工具。它不是一个简单的“插件集合”，而是一套完整的开发堆栈。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">开发者用它。能在ChatGPT内部直接构建真实、可交互的应用程序。通过连接自己的数据、触发具体的操作。甚至可以渲染出交互式的用户界面。这和之前的工具不一样的地方在于。像Claude和GitHub Copilot这些。它们虽然也能调用外部服务。但是最终只能把结果以文本的形式返回。缺乏真正的上下文理解和自然的交互界面。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">简单来说。过去的工具是“告诉你结果”，而Apps SDK是“帮你完成操作”，让用户在大语言模型里像在电脑上一样来使用App。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/N0X8-fsQuNY_133.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">发布会现场有个非常生动的案例。我给大家还原一下。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">假设你正在为宠物狗业务做头脑风暴。聊到一半想做海报。这时候你不用退出ChatGPT。直接“@”出Canva。重点来了。这个Canva能完全理解你之前和ChatGPT聊的所有点子。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如你说，要色彩丰富、异想天开。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">它不用你再重复解释。直接就能生成一系列符合要求的精美海报。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">如果这时候你灵感来了，说。请把这张海报扩展成商业计划书。它也能无缝衔接。直接帮你把商业计划书的框架搭好。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">等你想扩张业务，需要找场地的时候。ChatGPT还会根据你之前聊的“宠物狗业务”上下文。主动建议你考虑“匹兹堡”这个城市。然后直接唤起房产平台Zillow。全屏展示。你在ChatGPT里说“要带院子的三居室”，它就会帮你进行筛选。筛选完你再问“这个房子离狗狗公园有多远”，它还能基于Zillow的数据直接回答。整个过程没有一次软件切换。所有操作都在ChatGPT的对话框里完成。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/N0X8-fsQuNY_202.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">工具在最需要的时候自动出现。还能协同工作。这背后就是Apps SDK的核心能力。打破应用之间的壁垒。让不同工具在统一的入口里形成“协作流”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而支撑这一切的，是OpenAI的MCP体系。开发者依靠MCP能设计应用的逻辑和界面。再结合GPT本身的图像识别能力。让App不是“硬塞进”对话。而是“自然的融入”对话。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">更关键的是上下文记忆能力。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">发布会里还有个细节。ChatGPT在另一个新对话里。还能延续上一个对话的宠物狗业务话题。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这种跨对话的记忆。正是大语言模型能够作为软件入口的核心竞争力。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">以前的App是“功能孤岛”，你打开才能用；。现在有了Apps SDK。App成了“可随时唤醒的插件”，还能理解你的对话背景。真正做到了万物在大语言模型内。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">聊完Apps SDK，我们自然要问。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">怎么让这些App更好地适应用户需求。和对话更紧密地连接呢？</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/N0X8-fsQuNY_266.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这就轮到发布会的第二个核心。AgentKit了。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">行业里其实早就把2025年称作“Agent元年”，但是直到10月。市场上都没出现足够引发行业震荡的现象级Agent产品。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">显然，大家都觉得Agent是未来。但是开发门槛太高、落地太难。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而OpenAI这次推出AgentKit。就是想“推一把”Agent时代。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">它号称是“最简便、快捷”的Agent开发工具包。核心思路就是让一切回归可视化。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">AgentKit的核心是“Agent Builder”，这是一个可视化的画布。以前开发者做Agent，得从零写代码。逻辑错一点就得从头改。而现在用Agent Builder，不用写代码。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">通过直接拖拽、连接不同的功能节点。就能直观地设计复杂的业务流程。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">除了画布，它还有两个关键的功能。一个是“ChatKit”，这是一个可嵌入的聊天组件。开发者能轻松把带品牌定制能力的聊天界面。集成到自己的应用里。不用自己再来做界面设计了；。另一个是“Connector”，能够直接把AgentKit里构建的Agent工具。和企业内部的数据、工具连起来。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/N0X8-fsQuNY_339.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如企业的客户数据库、内部办公系统等等。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这样就不用再去做复杂的接口开发了。直接和企业已有的系统打通。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">发布会现场。产品经理克里斯蒂娜（Christina）做了个演示。从零开始，只用了8分钟。就为一个静态的DevDay活动官网。搭建并且上线了一个智能问答Agent。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">从演示里能看到。AgentKit的功能节点特别精简。只有三个核心。分别是Agent、End和Note。不同Agent之间的逻辑关系。也只用三个点就能控制。分别是条件触发。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如用户问时间就调用A Agent。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">问地点就调用B Agent，还有同时进行。让多个Agent并行处理任务。以及用户许可。也就是需要用户确认后才继续。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">每个Agent内部，还能添加三个工具。包括文件搜索、安全防护和MCP应用。当然。你也能把这些工具作为独立的功能点。加在流程外部。整个设计逻辑十分清晰。哪怕是没做过Agent开发的人。看一眼也能明白个大概。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/N0X8-fsQuNY_401.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">不过，客观地说，单从设计逻辑上看。AgentKit并没有比现在市场上的Dify、Coze这些工具领先太多。但是它胜在“精简”和“易用”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">为了让习惯了Dify的用户转用AgentKit。OpenAI还提供了两个“杀手锏”。第一个是RFT定制。现在第三方工具用GPT。都只能把它当做“黑箱”一样来调用API。你不知道模型内部是怎么推理的。也没法优化。但是AgentKit能深入模型的内部。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">目前OpenAI已经在实验GPT-5的RFT功能。开发者通过RFT。不仅能定制GPT-5的推理模型。还能专门训练模型在最恰当的时机、用最优的方式来调用工具。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如你想做一个客服Agent。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">用RFT能让模型学会。用户问售后问题的时候。先调用订单数据库查询信息。再调用售后流程工具提交申请。而不是乱调用工具。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这对想基于GPT来开发Agent的企业来说。吸引力非常的大。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/N0X8-fsQuNY_462.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第二个是AgentKit的“Evals评估板块”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">做Agent开发。最头疼的就是“不知道问题在哪”。比如流程跑不通，到底是节点错了。还是工具调用时机不对呢？Evals就直接解决了这个问题。它提供了“数据集构建”、“跟踪评估”、“自动化提示优化”这些能力。能让开发者对Agent工作流进行端到端的评估。精准定位和修复问题。这对需要快速迭代Agent的团队来说。确实能省不少事。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">不过这里我也得客观说一句。看到AgentKit。不禁让人容易想到当年OpenAI发布的GPT Store。那时候的模型主要靠上下文。没法有效的调用工具和数据。应用场景特别窄。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">现在AgentKit虽然解决了开发门槛问题。但是Agent落地的核心难题。比如复杂任务的逻辑拆解、多工具协同的容错性。以及真实场景的数据安全等等。这套框架能不能解决，目前还不好说。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">不过，至少OpenAI已经开始行动了。也许就会有找到解法的可能。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/N0X8-fsQuNY_524.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">聊完Agent，咱们再往底层看。不管是开发Agent。还是部署App，最基础的支撑都是编程。而发布会的第三个核心。就是可能会让Claude“编程王者”地位受到威胁的Codex。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">根据Sam Altman在发布会上的介绍。Codex从今年8月上线到现在。已经处理了40T的token数量。成了OpenAI增长最快的产品之一。从程序员社群的反馈来看。Codex的出现确实冲击了Anthropic Claude。现在有越来越多的程序员开始用Codex。认为它生成的代码更贴合实际的开发需求。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这次发布会。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">Codex正式从“研究预览版”转为了“正式版”，更新的重点是对企业和工程团队的深度支持。具体分为三个方面。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第一个是Slack集成。这是程序员社群呼吁了很久的功能。以前团队用Codex。得在Codex界面和Slack之间来回切。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">现在不用了。直接可以在Slack频道里调用Codex。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">它能在对话流里直接回答相关的技术问题。比如“Python怎么实现批量处理Excel”，或者直接生成代码片段。甚至能帮忙查看代码里的bug。整个流程都在团队日常沟通的场景里完成。不用再切换应用，效率提升十分明显。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/N0X8-fsQuNY_601.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第二个是全新的Codex SDK。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">以前企业用Codex，大多是“单点使用”，比如团队里的某个程序员用它来写代码；。现在有了Codex SDK。企业能够把Codex的能力作为一个“模块”，集成到自己内部的开发工作流里。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">拿企业的代码审查系统举例。现在就可以通过Codex SDK来调用Codex的能力。自动检查代码规范；。或者让内部的开发文档工具。通过Codex来自动生成API文档。这样一来。Codex就不再是“单个程序员的工具”了。而是“整个团队的开发助手”，能更好地融入企业现有的开发体系。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第三个是新的后台管理与报告工具。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">对于企业管理者来说，以前用Codex。不知道团队里谁在用、用它做了什么、有没有安全风险。现在有了这套工具，能做“环境控制”，比如限制某些团队只能使用特定的代码库、“实时监控”，查看Codex的调用频率、处理时长。以及“分析仪表盘”，统计Codex帮团队节省了多少开发时间、减少了多少bug。这些功能能让管理者清晰地掌握Codex在企业内的使用情况。也能更好地管控风险。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/N0X8-fsQuNY_670.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">但是这些更新。都不如发布会最后的那段演示令人印象深刻。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">演示者拉曼（Raman）当时说。我们来试试。只用对话，让语音助手调用Codex SDK。做一个滚动的开发者名单。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">然后他对着语音助手说。我需要在当前的前端应用里。加一个滚动展示的开发者名单。名单数据用DevDay的参会开发者信息。滚动速度要平缓。样式和现有界面保持一致。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">话音刚落。后台的Codex就开始实时修改前端的React代码。屏幕上能看到代码一行行的变化。没过几秒。页面上就出现了一个流畅滚动的开发者名单。和拉曼描述的完全一致。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这个场景。就是OpenAI预想的“未来软件开发”，你不用打开代码编辑器。不用手动敲一行代码，只用和AI对话。说出你的需求。软件就能在后台实时修改代码、自我迭代、完成功能更新。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">以前咱们说“代码生成”，主要是AI帮你写代码。而现在OpenAI想做的。是“无代码编程”。让AI直接帮你完成软件的修改和进化。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/N0X8-fsQuNY_736.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">开发者只需要负责提出需求就行了。这一步现在来看虽然还很初级。但是方向已经很明确了，未来的编程。可能真的会从写代码变成聊需求。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">除了APPS SDK AgentKit Codex这三个生态核心。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">发布会还有第四部分的内容。那就是相对传统的API更新。不过它们的重要性依然不容小觑。因为它直接降低了开发者使用OpenAI模型的门槛。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先是GPT-5 Pro的API开放。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">GPT-5 Pro是OpenAI目前最强大的模型。之前只对少数企业开放。这次正式向所有开发者开放API。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这意味着不管是小团队还是个人开发者。都能用GPT-5 Pro的能力来做产品。实现更复杂的自然语言理解、更精准的多模态处理等等。这对整个开发者生态来说。这无疑是个重大利好。其次是新的语音模型“GPT Real-time mini”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">其实OpenAI做语音模型也有段时间了。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">但是之前的模型成本太高。很多开发者想做一些语音应用。都因为成本问题望而却步。而这次的GPT Real-time mini。成本比之前的版本降低了70%，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/N0X8-fsQuNY_807.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">但是音质和情感表现力没打折扣。比如它能听出用户说话时的情绪。用对应的语气回应。也能清晰识别带有口音的语音。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">成本降了，能力没减。这就会大大降低语音应用的开发门槛。未来我们可能也会在更多场景里看到OpenAI的语音技术。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而最重要的，是Sora 2 API的开放。之前Sora 2是作为独立App上线的。大家只能在App里生成视频。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">现在开放了API。意味着开发者能够把Sora 2的视频生成能力。集成到自己的产品里。比如内容创作平台。能让用户直接在平台里生成AI视频素材；。电商平台。能让商家用Sora 2快速生成产品的宣传视频；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">甚至对于教育平台来说。也能让老师用Sora 2来生成教学动画。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">可以说Sora 2 API的开放。标志着OpenAI的顶级视频生成技术。正式从“自用”转向“生态共享”，未来我们在各种应用里。可能都能够看到用Sora 2生成的视频。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/N0X8-fsQuNY_871.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">聊完所有技术细节，我们再跳出来。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">看看OpenAI这次DevDay背后的“野心”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">它的商业帝国轮廓。其实已经越来越清晰。甚至隐隐盖住了AGI的远景图景。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">先看模型迭代，今年的GPT-5 Pro。确实强大。但是已经没有GPT-4发布时那种“跨时代的惊艳感”了。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">GPT-4当时让大家觉得。AI居然能做到这个程度。而GPT-5 Pro更像是在GPT-4的基础上。做了稳健的优化。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如处理速度更快、上下文窗口更大、多模态能力更协调。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">但是没有突破大家的预期。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">Sora 2也是一样，它的惊艳之处。不是视频生成技术比更早它发布的Veo 3强多少。而是产品团队精准抓住了社交媒体的爆点。把真实人物无缝的融入到了AI生成的视频中。这是个天才的商业构想。能够快速吸引用户、占领市场。但是它更多是“商业嗅觉”的胜利。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而非底层技术的革命。再看这次DevDay的核心。Apps SDK、AgentKit、Codex. 这三者其实都是在“搭建生态”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/N0X8-fsQuNY_933.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">Apps SDK把所有应用拉进ChatGPT。让ChatGPT成为唯一入口；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">AgentKit统一了Agent的开发标准。让开发者都来用OpenAI的工具做Agent；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">Codex深入到企业的开发流程中。让企业离不开OpenAI的编程支持。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">OpenAI的整个逻辑很清晰。那就是想要以自家大模型为核心。搭建一个封闭而且具有强掌控力的软件生态。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">开发者们在这个生态里开发。用户们在这个生态里使用。所有数据和流量都留在OpenAI的体系内。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这时候咱们再回过头来看看OpenAI早年的气质。那时候它带着一种“神秘感”，总在探索未知领域。比如最早的GPT-3、DALL-E。每次发布都让大家觉得。离AGI又近了一步。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">那种对技术边界的突破。能让人肾上腺素飙升。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">但是现在的OpenAI。更像一个“成熟的商业公司”，它不再追求“惊艳感”，而是追求“生态掌控力”；。不再强调“探索AGI”，而是强调“商业落地”。它的商业帝国轮廓也越来越清晰。但是曾经那种“为了AGI而探索未知”的气质。却在慢慢的褪色。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/N0X8-fsQuNY_1003.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">当然，这并不是说OpenAI做错了。企业要发展，必然要考虑商业落地。要搭建生态壁垒。但是作为关注AI行业的人。我们难免会有些感慨。当OpenAI把更多精力放在“掌控生态”上。它还能像以前那样。为AGI的突破投入足够的资源吗？</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">AGI的远景。会不会慢慢变成“商业帝国”的附属品？</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这个问题，可能需要时间来回答。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">不过不管怎么说。这次DevDay都给整个AI行业指明了一个方向。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">AI的下一个阶段。不再是“单点技术突破”，而是“生态整合”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">谁能把应用、Agent、编程工具都整合在自己的体系内。谁就能在未来的竞争中占据优势。对开发者来说，这是机遇，也是挑战；。对用户来说。未来我们可能真的会进入。一个对话框解决所有问题的时代。但同时也要接受所有需求都在一个生态里完成的现状。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">最后，我想问问。大家觉得OpenAI的这个生态蓝图。会让AI变得更加实用。还是会让行业陷入到“生态垄断”中呢？你会用Apps SDK来做自己的应用。还是继续用独立工具呢？欢迎在评论区留言交流。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/N0X8-fsQuNY_1074.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">好了。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">今天对OpenAI DevDay的解析就到这里。感谢收看，我们下期再见。</p>

<hr style="clear:both" />
=============
<p><a href="https://www.youtube.com/watch?v=N0X8-fsQuNY">https://www.youtube.com/watch?v=N0X8-fsQuNY</a></p><p>美国时间10月6日举办的OpenAI DevDay，就是它真正亮出“战略蓝图”的时刻。这场发布会没有太多颠覆性的“黑科技”，却把过去两年AI技术的演进，整合成了一套系统化、平台化的生态方案，目标很明确，那就是要构建一个以AI为绝对核心的全新软件生态体系。今天咱们就来回顾一下这场发布会的内容，看看OpenAI的“商业帝国”到底在如何布局。</p><p><a href="https://www.youtube.com/live/hS1YqcewH0c?si=pfOTZjGf2Q_jKR6P">https://www.youtube.com/live/hS1YqcewH0c?si=pfOTZjGf2Q_jKR6P</a></p>]]></content:encoded>
      <itunes:image href="https://i.ytimg.com/vi/N0X8-fsQuNY/hqdefault.jpg"/>
      <pubDate>2025-10-09T09:00:29.000Z</pubDate>
    </item><item>
      <title><![CDATA[【人工智能】软件正在吞噬生产力市场 | A16Z合伙人Alex Rampell | 软件的70年变革 | SaaS定价的“星巴克”模式 | 催生市场扩张 | 新的业务可能性 | 全球性机会]]></title>
      <link>https://www.youtube.com/watch?v=uiZEWwN5EKU</link>
      <itunes:title><![CDATA[【人工智能】软件正在吞噬生产力市场 | A16Z合伙人Alex Rampell | 软件的70年变革 | SaaS定价的“星巴克”模式 | 催生市场扩张 | 新的业务可能性 | 全球性机会]]></itunes:title>
      <itunes:author><![CDATA[最佳拍档]]></itunes:author>
      <itunes:summary>
        <![CDATA[<hr style="clear:both" />

<p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://img.youtube.com/vi/uiZEWwN5EKU/maxresdefault.jpg" /></p><p><a href="https://www.youtube.com/watch?v=uiZEWwN5EKU">https://www.youtube.com/watch?v=uiZEWwN5EKU</a></p><h1>值得閱讀的理由</h1> <ul> <li>深入理解軟體產業的未來趨勢：本摘要將揭示軟體如何從單純的資訊工具，轉變為直接執行勞動的實體，這對於所有軟體開發者、投資人及企業決策者都至關重要。</li> <li>洞悉AI對商業模式的顛覆性影響：摘要詳細闡述了SaaS產業現有的「按座席收費」模式所面臨的挑戰，並探討了基於「成果」的計費方式，幫助讀者預見新的營收增長點與策略。</li> <li>掌握AI解決勞動力結構性問題的獨特優勢：從應對間歇性需求到處理繁瑣工作，從確保合規性到克服語言障礙，AI不僅能降低成本，更能以人類無法比擬的方式解決企業面臨的實際難題。</li> </ul> <hr /> <h1>摘要</h1> <p>這段影片由「最佳拍檔」的<a href="#author-intro">大飛</a>主講，深入探討了軟體行業的未來方向。作者引述了a16z合夥人亞歷克斯·蘭佩爾（Alex Rampell）在2025年LP峰會上的演講，提出了軟體產業的最終目標並非僅止於用戶數或市場份額，而是整個龐大的勞動力市場。全球SaaS市場規模約為3000億美元，而單是美國的勞動力市場便高達13兆美元，AI技術正迅速縮小這巨大的差距。演講的核心觀點是，軟體不再只是記錄資訊的工具，而是開始真正執行勞動本身，這將徹底改變軟體行業的本質及未來趨勢。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/uiZEWwN5EKU_80.jpg" /></p> <h2>軟體發展史：從文件櫃到數據庫</h2> <p>亞歷克斯·蘭佩爾首先回顧了軟體發展的歷史，指出過去70年來，幾乎所有軟體公司都在將實體文件櫃轉換為數字<strong>數據庫</strong>。無論是航空訂票系統Sabre Systems，還是客戶關係管理系統（CRM）如Siebel Systems和Salesforce，抑或是製造業和會計領域的SAP和QuickBooks，其本質都是將紙質文件數位化。然而，儘管介質發生了變化，但操作流程並未實質改變，最終仍需<strong>人工</strong>來讀取、理解和操作這些資訊，因此效率提升是有限的。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/uiZEWwN5EKU_300.jpg" /></p> <h2>AI帶來的典範轉移：軟體執行勞動</h2> <p>作者指出，AI技術正在顛覆這一遊戲規則。軟體不再只是儲存和展示資訊，而是開始<strong>理解資訊並採取行動</strong>。這標誌著軟體功能的一個質的飛躍，從被動的記錄工具轉變為主動的勞動執行者，從根本上改變了資訊處理的方式與效率。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/uiZEWwN5EKU_370.jpg" /></p> <h2>SaaS商業模式的衝擊：「星巴克模式」的終結</h2> <p>亞歷克斯·蘭佩爾以「星巴克模式」（中杯、大杯、超大杯）來形容目前SaaS公司常見的<strong>按「座席數」</strong>定價方式，並以客服軟體Zendesk為例進行說明。他指出，假設一家公司有1000名客服人員，每年的人力成本高達7500萬美元，而軟體成本僅約140萬美元。當AI能大幅提高客服生產力，甚至能處理絕大部分客戶問題時，對「座席」的需求將趨近於零。這將對Zendesk這類公司的營收構成巨大挑戰，但同時也存在轉機：如果Zendesk能提供以<strong>成果為導向</strong>的服務（例如每年500萬美元，幫助客戶節省7000萬美元人力成本），其收入反而可能大幅增長。這凸顯了SaaS公司商業模式面臨的巨大十字路口。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/uiZEWwN5EKU_570.jpg" /></p> <h2>軟體的新戰場：勞動力市場的潛力</h2> <p>亞歷克斯·蘭佩爾提出了一個令人震驚的數據：僅美國註冊護士一個職業，每年的總收入就達到6500億美元，這個單一勞動力市場的規模甚至比整個全球軟體市場還要大。這旨在強調，軟體真正的競爭池子是整個勞動力市場。軟體不再只是將文件櫃數位化，而是開始在這些數據上執行<strong>操作</strong>，將角色從「提供資訊」轉變為「<strong>完成工作</strong>」，這也是軟體產業瞄準勞動力市場的核心原因。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/uiZEWwN5EKU_615.jpg" /></p> <h2>案例分析：從眼科診所看市場擴張邏輯</h2> <p>作者分享了一個有趣的案例：一家小型眼科診所招聘前台接待員，年薪4.5萬美元，但職位已懸空六個月。傳統上，這類小型企業每年軟體開支可能只有數百美元，被認為是「不夠大」的軟體市場。然而，當AI公司「應聘」這個職位，聲稱能承擔大部分職責（除了開關門），並以每年2萬美元的價格提供服務時，情況就完全不同了。這不僅大大低於招聘不到人的成本，也讓過去不具吸引力的小型市場，因軟體提供<strong>勞動力替代方案</strong>而變得極具潛力，從根本上擴大了軟體市場的定義與規模。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/uiZEWwN5EKU_850.jpg" /></p> <h2>AI親自上陣：語音通話實例</h2> <p>亞歷克斯·蘭佩爾在演講中播放了兩段真實的AI語音通話錄音，將抽象概念轉化為具體現實。第一個案例來自投資公司Happy Robot，AI在貨運和卡車運輸行業中與潛在客戶自然地<strong>洽談運費</strong>，甚至包含了討價還價。第二個案例來自Salient公司，其AI能以西班牙語進行汽車貸款<strong>催收</strong>，並能支援幾十種語言。這些錄音展現了AI在複雜對話和情緒敏感場景中的高度擬真能力，堪稱是新的圖靈測試。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/uiZEWwN5EKU_945.jpg" /></p> <h2>AI取代人力的獨特優勢</h2> <p>作者進一步闡釋了AI取代人類工作不僅是成本問題，更是因為AI擁有許多人類無法比擬的獨特優勢，解決了許多結構性問題：首先是<strong>間歇性需求</strong>，如黑色星期五期間零售商對客服的暴增需求，AI能隨時擴展。其次是<strong>令人沮喪的工作</strong>，如催收，AI不會感到疲憊或受負面情緒影響。第三是<strong>監管確定性</strong>，AI能嚴格遵守法規，降低人類可能因情緒失控而帶來的違規風險。第四是<strong>語言能力</strong>，AI能即時處理幾十種不同語言的溝通，這對任何個人來說都是不可能實現的。因此，AI在處理間歇性、沮喪性、高度受監管且多語言的工作方面表現卓越。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/uiZEWwN5EKU_1180.jpg" /></p> <h2>AI如何拓展新業務領域</h2> <p>因為AI的出現，許多過去被認為不可行或市場太小的業務，現在也變得可行。例如，合規官曾是美國增長最快的工作之一，但因為需要大量人力而非軟體，因此沒有公司專門開發合規軟體。但現在，AI可以提供<strong>端到端</strong>的合規解決方案。此外，許多因客戶獲取成本（CAC）或銷售成本（COGS）過高而無法啟動的業務（例如自行車版Airbnb），現在也能通過AI銷售代表和客服降低成本，從而大大<strong>擴展市場規模</strong>。AI通過「vibe coding」方式，能快速啟動這些潛在業務，使沉寂的想法重新煥發生命力。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/uiZEWwN5EKU_21.jpg" /></p> <h2>大飛的總結與反思</h2> <p>大飛在影片結尾分享了他對演講的總結與感受。他認為，軟體行業在過去70年從「文件櫃」到「數據庫」的轉變，創造了巨大的價值，但效率提升有限。而AI的革命性意義在於，它使軟體能夠「執行工作」，將軟體從<strong>記錄工具</strong>轉變為<strong>勞動力本身</strong>，市場規模從數千億美元擴大到數十兆美元，甚至更大。這意味著所有SaaS公司都需重新思考其商業模式，從「按座席」轉向更具挑戰的「<strong>按結果</strong>」收費。勞動力市場也將面臨雙重影響：一方面，客服、催收等傳統工作將被取代；另一方面，訓練、監督、設計和優化AI將創造新的工作機會。</p> <p>「軟體正在吞噬勞動力」的觀點呼應了馬克·安德森（Marc Andreessen）多年前提出的「軟體正在吞噬世界」。過去的軟體革命主要圍繞「資訊」，而現在的AI革命則聚焦於「<strong>行動</strong>」。軟體不再只是提供資訊，而是直接完成工作，這徹底改變了軟體從「工具」到「勞動力」的性質。大飛也提到了亞歷克斯·蘭佩爾用卡爾·馬克思的《資本論》來比喻，指出資本家投資資本，購買GPU，僱傭工程師利用GPU產出能執行勞動的軟體，這就像新的質能等價公式，E=MC²一般，具備深遠的影響。最後，大飛邀請觀眾分享對「軟體吞噬勞動力」這一觀點的看法。</p> <hr /><hr />================================================</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">大家好，这里是最佳拍档，我是大飞。不知道大家有没有想过。软件行业真正追逐的目标是什么呢？如果有人说，软件行业真正的目标。既不是更多的用户。也不是更高的市场份额。而是整个劳动力市场。你会不会觉得这听起来像是天方夜谭呢？但是当我们看到一组数字的时候。也许就会明白。全球 SaaS市场的规模大约为 3000 亿美元。而仅美国的劳动力市场就高达 13 万亿美元。而这个巨大的差距。正在被 AI 技术快速的缩小。最近。我看了 a16z 合伙人亚历克斯·兰佩尔 Alex Rampell 在2025 年 LP 峰会上的演讲。他用自己独特的视角。解释了这场正在发生的变革。那就是软件不再只是记录信息的工具。而是开始真正执行劳动本身。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">今天大飞就来带大家回顾一下这期演讲。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">看看软件行业究竟在发生什么样的变革。以及未来的趋势将会怎样演化。Alex 首先从软件发展的历史说起。他认为过去 70 年里。几乎所有软件公司做的事情都是把文件柜变成数据库。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/uiZEWwN5EKU_74.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如从航空订票系统到 CRM。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">从 ERP 到电子健康记录。本质上都是在把纸质文件数字化。但是关键的问题是，虽然介质变了。但是操作流程并没有发生本质的改变。过去看纸质文件的人。现在看电脑屏幕。仅此而已。因此，这种效率提升是有限的。因为最终还是需要人来读取、理解和操作这些信息。而现在，AI 正在改变这个游戏规则。软件不再只是存储和展示信息。而是开始理解信息并采取行动。而这将是一个质的飞跃。Alex举的第一个例子是 Sabre Systems。这是美国航空和 IBM 在 1959 年共同开发的航空订票系统。在有这个系统之前。航空公司是怎么管理订票的呢？想象一下，有无数个文件柜。里面塞满了表格。假设有一位叫Betty的乘客。打电话来说她想要 4A 座位。工作人员在纸上写下来。然后她又改主意说要取消。工作人员擦掉。她再改口说要 2C 座位。工作人员又擦掉重写。这个过程效率极低。而且信息无法在不同办公室之间共享。因为所有数据都锁在一个物理的文件柜里。Sabre 把这一切搬到了 IBM 大型机上。通过分布在全球各地的终端。让旅行社可以访问同一个系统。这彻底改变了旅游行业的运作方式。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/uiZEWwN5EKU_162.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">同样的故事在各个行业重复上演。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在销售领域。Alex 提到了电影《拜金一族》里那些业务员。为了获得好的客户名单而拼命竞争。而实际上，那些名单就是一张张纸。1980 年代出现了 ACT Systems。1990 年有 GoldMine。1993 年 Tom Siebel 创立了 Siebel Systems。这些都是 CRM。也就是客户关系管理系统的先驱。然后 Salesforce 在 1999 年把这一切搬到了云端。但是本质上。在1950 年代电影里那个翻查纸质文件的销售人员。和 2010 年打开 Salesforce 记录的销售人员。做的是同样的事情。只是介质从纸变成了屏幕。制造业和库存管理也是如此。作为产品制造商。你需要知道自己有多少库存。销售情况如何。IBM 再次走在前沿。但是也有其他公司跟进了。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如 1972 年成立的 SAP、JD Edwards、Epicor、Sage 等等。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这些都是把老式纸质记录数字化的公司。法律行业也一样。Alex 说他在 1980 年代去律所的时候。发现大部分空间都被文件柜占据了。像 PC Law、LexisNexis 和 Reuters 这样的公司。很大一部分收入来自为律所提供的数字化服务。把那些本来要占据昂贵办公空间的文件数字化。会计行业也是如此。Intuit 推出了 QuickBooks。把财务报表数字化了。还有 Peachtree、MYOB 等公司。都在做同样的事情。Alex花了很多时间来讲软件的历史。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/uiZEWwN5EKU_259.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">其实主要想说明的是。尽管技术在进步。但是效率提升是很有限的。因为文件柜是人在看。数字记录也还是人在看。那个坐在电脑前帮客户解决问题的客服人员。和以前翻纸质文件的客服人员。本质上没有区别。理解了这一点。才能理解为什么整个软件行业的商业模式必须改变。Alex 用了一个非常形象的比喻来描述当前的 SaaS 定价模式。他称之为"星巴克模式"，Tall、Grande、Venti. 俗称中杯、大杯、超大杯。如果你去任何一家 SaaS 公司的网站。他们的定价页面可能都长这样。他举了 Zendesk 的例子。这是一家现在被私有化的公司。年收入 20 亿美元，销售的是座席数。他们最受欢迎的套餐是 Suite Professional。每月 115 美元。但是问题来了。如果 AI 能够非常好地回答客户的问题。那么当每个客服人员的生产力都提高了9000 倍的时候。你还需要多少座席呢？让我们来看看具体的数字。假设一家公司有 1000 个客服人员在呼叫中心工作。每人的总成本是7.5 万美元一年。那就是每年 7500 万美元的人力成本。软件成本是多少呢？1000 个座席乘以 115 美元再乘以 12 个月。大约是每年 140 万美元。显然，人力成本远远高于软件成本。如果每个人工客服一年回答 2000 个问题。那么每个问题的成本大约是 37 美元的人力成本。加上 69 美分的软件成本。总共大约 38 美元。如果现在 AI 可以回答所有问题。那么会发生什么呢？这可能会朝两个方向发展。一种可能是，如果 AI 能处理一切。那你还需要多少座席呢？答案是零，一个都不需要。如果Zendesk还按座席收费的方式。那么他们的收入就会从 140 万美元降到零。这对 Zendesk 来说是灾难性的。但是另一种可能是。也许 Zendesk 可以收费 500 万美元一年。然后对客户说，嘿。你不要再花 7500 万美元在客服上了。改成花 500 万美元。全付给我们。只需要500 万，你就省下了 7000 万。所以说。Zendesk 现在真的是处在一个十字路口。他们的收入可能归零。也可能增长三倍。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">但是他们自己也不知道答案会是什么。Alex 说他一直在和 Zendesk 的 CEO 交流。他们现在正在新西兰试点基于结果的定价模式。结果如何，我们可以拭目以待。接下来。Alex 给出了一个让人震撼的数据对比。他说仅看护士这一个职业。美国的注册护士每年的收入总计大约 是6500 亿美元。大约有 450 万注册护士。这个单一职业的市场规模。就比整个全球软件市场还大。当然。这不意味着护理软件市场会有 6500 亿美元。只是为了说明软件真正竞争的池子有多大。这里Alex其实想表达的核心观点是。软件不再只是数字化文件柜。而是开始在文件柜上执行操作。这意味着什么呢？拿旅行订票来说。如果软件拥有了旅行数据。它可以帮你重新预订航班。或者你不需要和旅行代理沟通。可以直接和美联航的 AI 对话。让它来帮你搞定一切。销售领域更明显。Salesforce 是按座席收费的。但是它真正应该做的是直接帮客户销售。客户其实并不想付那 1000 个座席的钱。而是想为获得的客户付费。制造业、法律业、医疗健康领域、甚至连HR和薪酬系统也应该是如此。这背后的意义是。以前软件公司坐拥海量的数据。结果只是把数据展示给了人类。让人类来做决策和采取行动。而现在软件可以直接理解数据并且采取行动。完成从"提供信息"到"完成工作"的根本转变。这也正是为什么软件要瞄上劳动力市场的原因。Alex还分享了一个特别有意思的案例。他说因为跟腱受伤。不能跑步和骑自行车了。所以有大把时间。于是他就开始在 Craigslist 上浏览招聘信息。当然他不是为了自己找工作。而是为了观察市场。他找到了一个真实的招聘广告。Plaza Lane Optometry 这家眼科诊所。在招聘前台接待员。这个职位已经挂了 6 个月了。年薪是 4.5 万美元。显然，这个职位的薪酬并不高。如果他们挂个年薪 10 万美元。这个职位也许早就填上了，但是显然。他们只能支付 4.5 万美元的成本。如果我们仔细去看职位的职责说明。会发现第一条是开门关门上锁。这个AI 确实做不了。但是其他的很多职责 AI 就完全可以做了。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/uiZEWwN5EKU_547.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如和保险公司争论。在预约前一天给病人打电话防止爽约等等。如果我们从软件市场的角度来看眼科诊所。会断定这不是一个好的软件市场。Plaza Lane Optometry 可能每年只会花 500 美元在软件上。大概会买一个 Microsoft Office 许可证。或者可能用 Squarespace 或者Wix 建了个网站。就没了。所以他们每年软件支出大概也就是 500 美元。但在现在。Alex 看到很多公司开始这么干了。他们浏览 Craigslist，寻找招聘信息。然后说，嘿。Plaza Lane Optometry. 我想申请这个职位。眼科医生看到就会问，好啊。说说你的资质，你以前在哪工作？AI 然后回答，其实我是一家软件公司。我不能开门关门。但是我可以做其他八件事。要不要看个演示呢？一开始眼科医生可能会说不。但是后来可能会说。好吧，试试看吧。而且软件的价格是每年 2 万美元。远远低于招不到人的 4.5 万美元。Alex想用这个例子来说明市场扩张的逻辑。对于这些小型企业来说。软件的支出很小。但是劳动力的支出很高。当软件开始承担劳动力的工作的时候。它就进入了一个大得多的市场。这些以前被认为是不够大的行业。现在突然变得很有吸引力。因为软件不再只是卖给他们 500 美元的工具。而是在卖给他们 2 万美元的劳动力替代方案。Alex 在演讲中还播放了两段真实的 AI 语音通话录音。让抽象的概念变成了具体的现实。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/uiZEWwN5EKU_644.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第一个案例来自他们投资的公司 Happy Robot。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">服务货运和卡车运输行业。录音中。AI 正在和一个潜在的客户洽谈运费。我们先来听一下。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">整个对话我听了几遍，非常自然。有讨价还价。有妥协，完全不像是在和机器说话。不知道大家能不能听出来。谁是机器人。谁是人类么？可以说，这就是新的图灵测试。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/uiZEWwN5EKU_711.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第二个案例来自一家叫 Salient 的公司。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">专门做汽车贷款的催收业务。在录音中。AI 用西班牙语和一个客户通话。您的账户目前逾期 51 天。金额 825.35 美元。您今天能还款吗？Alex 特别指出。Salient 可以说几十种语言。包括他加禄语、越南语、普通话等等。通过这个例子，Alex 想要解释。为什么说AI取代人类的工作。不仅仅是成本问题。而是因为AI具有人类无法比拟的独特优势。解决了很多人类劳动力无法解决的结构性问题。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先是间歇性需求。想象一下黑色星期五的零售商。销售额在黑色星期五期间会大幅增长。所以需要雇佣大量的收银员。或者如果是在线零售商。则需要雇佣大量的客服人员来回答问题。实际上因为还需要培训。所以9月份就应该开始招聘了。但是过了黑五怎么办呢？把他们都解雇然后 11个月以后再重新雇佣吗？所以这个问题就很棘手。同样。还有很多其他行业也有间歇性需求的问题。但是 AI 在这方面非常擅长。其次是令人沮丧的工作。什么是令人沮丧的工作？催收就是其中的一种。因为你打电话给别人说。嘿，你逾期了。大概率对方会直接爆粗口。人类会对这样的工作感到疲惫。因为这不是一份愉快的工作。但是 AI 不会被这些困扰。所以对于这种会令人沮丧的工作。AI非常非常适合。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/uiZEWwN5EKU_799.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第三呢是监管确定性。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">Alex提到。他联合创办过一家叫做Affirm的公司。他们每个季度都要接受 UDAAP。也就是不公平、欺骗性和滥用性行为的培训。有很多法律规定你可以对客户说什么、不能说什么。想象一下你打电话给客户说，嘿。你欠钱了。客户爆了粗口。然后万一你那天心情不太好。也回了一句Fuck。这可能就会让你陷入麻烦。但是如果让一个经过编程的AI机器人。从头到尾来进行整个通话。你就会有更多的确定性。远比人类更加可靠。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第四是语言能力。Alex 说他花了太多时间。来学习俄语、日语和一点西班牙语。但是如果客户只会说波斯语怎么办呢？你在斯坦福能找到会说波斯语的护士吗？那如果万一客户只会说蒙古语呢？现在，有了AI之后。无论是护士、催收员、谈判员。所有这些工作都可以立刻用几十种不同的语言完成。显然，我们不可能在爱荷华州。找到一个会说塞尔维亚语、能够按需去做令人沮丧的工作。还能够经常处理间歇性需求的人。但是AI在这些方面都很出色。在解决了人类劳动力的结构性问题之后。Alex 特别强调了AI对劳动力市场的扩大效应。这也是为什么他要从文件柜的故事开始讲起。根据美国劳工统计局的数据。美国增长最快的工作是美甲师美容师。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/uiZEWwN5EKU_887.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第二快的就是合规官Compliance Officer。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">但是合规官不需要软件。对于银行来说。需要的只是更多的人。所以，没有公司专门做合规软件。因为这个软件市场太小了。不过，现在有了AI之后。你可以对银行说。我可以提供端到端的合规解决方案。每年只需要1000 万美元。我的软件产品可以追踪一切的合规问题。所以银行就不需要那么多人了。Alex还提出了另一个有趣的观点。那就是因为 AI。很多过去不可行的业务。现在也变得可行了。Alex 因为受伤不能骑自行车。他的车库里有很多闲置的自行车。他想问。为什么没有人做自行车的 Airbnb呢？</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">其实没人做这个的原因很简单。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">因为这是个很糟糕的生意。为什么说它糟糕呢？其中一个核心原则就是。不管有没有 AI。如果你的客户获取成本（CAC）加上销售成本（COGS）。大于客户的生命周期价值（LTV）。那就不要做，因为这不是一个生意。但是现在有了 AI，情况不同了。想象一下。如果你要做一个自行车的Airbnb。该怎么找到那些车库里有闲置自行车的人呢？你难道要雇佣一群昂贵的斯坦福学生。给他们提供 20 种不同口味的椰子水。满足他们千禧一代的各种需求。然后让他们在帕洛阿尔托的销售运营团队工作吗？不，正确的做法应该是。让 AI 销售代表给每个人打电话。它每年的成本只有几百美元。既不是 10 万美元，也不需要椰子水。如果有紧急的情况怎么办呢？那就直接让AI 代表来接听客服热线就好了。当然，如何对用户进行筛选。做背景调查。甚至判断自行车是不是好的。是不是偷来的。所有这些事情AI 都可以做。所以在Alex看来。实际上有一大类的业务。本来是可行的。但就是因为客户获取成本或者销售成本太高了。而没做成。现在有了 AI。你可以用 vibe coding 的方式快速启动这些业务。这会大大扩展市场的规模。因为 CAC 下降了，COGS 下降了。当然。每个公司最终都会开始使用这些工具。所以就像美国著名棒球教练尤吉·贝拉（Yogi Berra）的那句名言一样。太拥挤了，没人去那里了。但是现在很多公司。还尝试5 年前根本行不通的那些老想法。最后，Alex 在演讲结尾指出。这是一个全球性机会。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">虽然美国劳动力市场很大。一年13 万亿美元。但是全球劳动力市场要大得多。作为风险投资人。他们的工作是找到最好的公司。让软件市场看起来很小。接下来说说大飞我的感受。整个演讲听下来。Alex 其实想要描绘一个完整的转变图景。从文件柜到数据库，花了 70 年。这 70 年软件行业创造了 2.2 万亿美元的市值。以及每年 3000 亿美元的收入。但是这只是把记录数字化了。效率的提升终究有限。而现在 AI 让软件能够真正的执行工作。这意味着软件行业可以触及的市场。从 3000 亿美元扩大到 13 万亿美元甚至更多。简单来说，就是软件的角色。从记录工具变成了劳动力本身。这个转变的深远影响。可能还没有被大多数人所充分的理解。从全球视角来看，这个机会应该更大。发展中国家的劳动力成本虽然较低。但是仍然是软件成本的很多倍。而且很多发展中国家面临着熟练劳动力短缺的问题。AI 可以提供一致的、高质量的服务。不受地理位置限制。不受语言障碍的限制。这对全球经济发展的影响可能也会是革命性的。从商业模式角度来看。这意味着几乎所有 SaaS 公司都需要重新思考他们的定价策略。按座席收费在 AI 时代估计是行不通的。因为座席数会趋近于零。但是按结果收费又会带来新的挑战。如何定义结果？如何衡量价值？如何定价呢？一个帮你催收到 100 万美元的 AI 究竟值多少钱呢？帮你获得 10 个新客户的 AI 又值多少钱？这些都是需要重新思考的问题。从劳动力市场角度看。这场变革的影响应该说是双重的。一方面，很多传统工作确实会被取代。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/uiZEWwN5EKU_1142.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如客服、催收员、初级的销售人员、前台接待等等。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这些工作中的大部分任务可以由 AI 完成。但是另一方面。这也会创造新的工作机会。需要有人训练 AI，监督 AI。处理 AI 无法处理的复杂情况。需要有人设计 AI 的对话流程。优化 AI 的表现。确保 AI 符合法规的要求。这些都是新的工作类型。回到 Alex 演讲的标题。软件正在吞噬劳动力（Software is Eating Labor）。这个标题其实呼应了Marc Andreessen 十多年前的文章。软件正在吞噬世界（Software is Eating the World）。当然。劳动力市场确实是世界的一部分。但是这次也可能有些不同。过去的软件革命。大多都是关于信息的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如我们如何更好地记录、存储、检索和展示信息。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这创造了巨大的价值。但是软件本质上还是一个工具。需要人来使用。而现在的 AI 革命是关于行动的。软件不再只是提供信息。而是直接完成工作。这彻底改变了软件的性质。让它从工具变成了劳动力。回想到在演讲的开头。Alex 展示的那张卡尔·马克思的照片。他说。如果你读过《资本论》就会知道。书中的核心观点是资本和劳动。资本在剥削劳动。但是现在发生的是。投资人给公司投资资本。公司用这些资本来购买或者租用 GPU。然后雇佣工程师，给他们用GPU。然后产出可以完成劳动工作的软件。这就像是新的质能等价公式，E=MC²。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/uiZEWwN5EKU_1228.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">好了。以上就是这场演讲的主要内容了。那大家是如何看待Alex所提出。软件将会吞噬掉劳动力市场的观点呢？这其中是否蕴藏着大量的机会呢？还是觉得这可能就是个噱头。AI根本就无法代替大多数劳动力工作呢？欢迎在评论区发表你的看法。感谢收看本期视频。我们下期再见。</p>

<hr style="clear:both" />
=============
<p><a href="https://www.youtube.com/watch?v=uiZEWwN5EKU">https://www.youtube.com/watch?v=uiZEWwN5EKU</a></p><p>最近，我看了 a16z 合伙人亚历克斯·兰佩尔 Alex Rampell 在2025 年 LP 峰会上的演讲，他用自己独特的视角，解释了软件领域正在发生的变革，那就是软件不再只是记录信息的工具，而是开始真正执行劳动本身。今天大飞就来带大家回顾一下这期演讲，看看软件行业究竟在发生什么样的变革，以及未来的趋势将会怎样演化。</p><p></p><p><a href="https://youtu.be/dhyhR4Bzc0I?si=XIHic9m-o9tg6knW">https://youtu.be/dhyhR4Bzc0I?si=XIHic9m-o9tg6knW</a></p>]]>
      </itunes:summary>
      <description>
        <![CDATA[<hr style="clear:both" />

<p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://img.youtube.com/vi/uiZEWwN5EKU/maxresdefault.jpg" /></p><p><a href="https://www.youtube.com/watch?v=uiZEWwN5EKU">https://www.youtube.com/watch?v=uiZEWwN5EKU</a></p><h1>值得閱讀的理由</h1> <ul> <li>深入理解軟體產業的未來趨勢：本摘要將揭示軟體如何從單純的資訊工具，轉變為直接執行勞動的實體，這對於所有軟體開發者、投資人及企業決策者都至關重要。</li> <li>洞悉AI對商業模式的顛覆性影響：摘要詳細闡述了SaaS產業現有的「按座席收費」模式所面臨的挑戰，並探討了基於「成果」的計費方式，幫助讀者預見新的營收增長點與策略。</li> <li>掌握AI解決勞動力結構性問題的獨特優勢：從應對間歇性需求到處理繁瑣工作，從確保合規性到克服語言障礙，AI不僅能降低成本，更能以人類無法比擬的方式解決企業面臨的實際難題。</li> </ul> <hr /> <h1>摘要</h1> <p>這段影片由「最佳拍檔」的<a href="#author-intro">大飛</a>主講，深入探討了軟體行業的未來方向。作者引述了a16z合夥人亞歷克斯·蘭佩爾（Alex Rampell）在2025年LP峰會上的演講，提出了軟體產業的最終目標並非僅止於用戶數或市場份額，而是整個龐大的勞動力市場。全球SaaS市場規模約為3000億美元，而單是美國的勞動力市場便高達13兆美元，AI技術正迅速縮小這巨大的差距。演講的核心觀點是，軟體不再只是記錄資訊的工具，而是開始真正執行勞動本身，這將徹底改變軟體行業的本質及未來趨勢。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/uiZEWwN5EKU_80.jpg" /></p> <h2>軟體發展史：從文件櫃到數據庫</h2> <p>亞歷克斯·蘭佩爾首先回顧了軟體發展的歷史，指出過去70年來，幾乎所有軟體公司都在將實體文件櫃轉換為數字<strong>數據庫</strong>。無論是航空訂票系統Sabre Systems，還是客戶關係管理系統（CRM）如Siebel Systems和Salesforce，抑或是製造業和會計領域的SAP和QuickBooks，其本質都是將紙質文件數位化。然而，儘管介質發生了變化，但操作流程並未實質改變，最終仍需<strong>人工</strong>來讀取、理解和操作這些資訊，因此效率提升是有限的。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/uiZEWwN5EKU_300.jpg" /></p> <h2>AI帶來的典範轉移：軟體執行勞動</h2> <p>作者指出，AI技術正在顛覆這一遊戲規則。軟體不再只是儲存和展示資訊，而是開始<strong>理解資訊並採取行動</strong>。這標誌著軟體功能的一個質的飛躍，從被動的記錄工具轉變為主動的勞動執行者，從根本上改變了資訊處理的方式與效率。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/uiZEWwN5EKU_370.jpg" /></p> <h2>SaaS商業模式的衝擊：「星巴克模式」的終結</h2> <p>亞歷克斯·蘭佩爾以「星巴克模式」（中杯、大杯、超大杯）來形容目前SaaS公司常見的<strong>按「座席數」</strong>定價方式，並以客服軟體Zendesk為例進行說明。他指出，假設一家公司有1000名客服人員，每年的人力成本高達7500萬美元，而軟體成本僅約140萬美元。當AI能大幅提高客服生產力，甚至能處理絕大部分客戶問題時，對「座席」的需求將趨近於零。這將對Zendesk這類公司的營收構成巨大挑戰，但同時也存在轉機：如果Zendesk能提供以<strong>成果為導向</strong>的服務（例如每年500萬美元，幫助客戶節省7000萬美元人力成本），其收入反而可能大幅增長。這凸顯了SaaS公司商業模式面臨的巨大十字路口。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/uiZEWwN5EKU_570.jpg" /></p> <h2>軟體的新戰場：勞動力市場的潛力</h2> <p>亞歷克斯·蘭佩爾提出了一個令人震驚的數據：僅美國註冊護士一個職業，每年的總收入就達到6500億美元，這個單一勞動力市場的規模甚至比整個全球軟體市場還要大。這旨在強調，軟體真正的競爭池子是整個勞動力市場。軟體不再只是將文件櫃數位化，而是開始在這些數據上執行<strong>操作</strong>，將角色從「提供資訊」轉變為「<strong>完成工作</strong>」，這也是軟體產業瞄準勞動力市場的核心原因。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/uiZEWwN5EKU_615.jpg" /></p> <h2>案例分析：從眼科診所看市場擴張邏輯</h2> <p>作者分享了一個有趣的案例：一家小型眼科診所招聘前台接待員，年薪4.5萬美元，但職位已懸空六個月。傳統上，這類小型企業每年軟體開支可能只有數百美元，被認為是「不夠大」的軟體市場。然而，當AI公司「應聘」這個職位，聲稱能承擔大部分職責（除了開關門），並以每年2萬美元的價格提供服務時，情況就完全不同了。這不僅大大低於招聘不到人的成本，也讓過去不具吸引力的小型市場，因軟體提供<strong>勞動力替代方案</strong>而變得極具潛力，從根本上擴大了軟體市場的定義與規模。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/uiZEWwN5EKU_850.jpg" /></p> <h2>AI親自上陣：語音通話實例</h2> <p>亞歷克斯·蘭佩爾在演講中播放了兩段真實的AI語音通話錄音，將抽象概念轉化為具體現實。第一個案例來自投資公司Happy Robot，AI在貨運和卡車運輸行業中與潛在客戶自然地<strong>洽談運費</strong>，甚至包含了討價還價。第二個案例來自Salient公司，其AI能以西班牙語進行汽車貸款<strong>催收</strong>，並能支援幾十種語言。這些錄音展現了AI在複雜對話和情緒敏感場景中的高度擬真能力，堪稱是新的圖靈測試。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/uiZEWwN5EKU_945.jpg" /></p> <h2>AI取代人力的獨特優勢</h2> <p>作者進一步闡釋了AI取代人類工作不僅是成本問題，更是因為AI擁有許多人類無法比擬的獨特優勢，解決了許多結構性問題：首先是<strong>間歇性需求</strong>，如黑色星期五期間零售商對客服的暴增需求，AI能隨時擴展。其次是<strong>令人沮喪的工作</strong>，如催收，AI不會感到疲憊或受負面情緒影響。第三是<strong>監管確定性</strong>，AI能嚴格遵守法規，降低人類可能因情緒失控而帶來的違規風險。第四是<strong>語言能力</strong>，AI能即時處理幾十種不同語言的溝通，這對任何個人來說都是不可能實現的。因此，AI在處理間歇性、沮喪性、高度受監管且多語言的工作方面表現卓越。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/uiZEWwN5EKU_1180.jpg" /></p> <h2>AI如何拓展新業務領域</h2> <p>因為AI的出現，許多過去被認為不可行或市場太小的業務，現在也變得可行。例如，合規官曾是美國增長最快的工作之一，但因為需要大量人力而非軟體，因此沒有公司專門開發合規軟體。但現在，AI可以提供<strong>端到端</strong>的合規解決方案。此外，許多因客戶獲取成本（CAC）或銷售成本（COGS）過高而無法啟動的業務（例如自行車版Airbnb），現在也能通過AI銷售代表和客服降低成本，從而大大<strong>擴展市場規模</strong>。AI通過「vibe coding」方式，能快速啟動這些潛在業務，使沉寂的想法重新煥發生命力。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/uiZEWwN5EKU_21.jpg" /></p> <h2>大飛的總結與反思</h2> <p>大飛在影片結尾分享了他對演講的總結與感受。他認為，軟體行業在過去70年從「文件櫃」到「數據庫」的轉變，創造了巨大的價值，但效率提升有限。而AI的革命性意義在於，它使軟體能夠「執行工作」，將軟體從<strong>記錄工具</strong>轉變為<strong>勞動力本身</strong>，市場規模從數千億美元擴大到數十兆美元，甚至更大。這意味著所有SaaS公司都需重新思考其商業模式，從「按座席」轉向更具挑戰的「<strong>按結果</strong>」收費。勞動力市場也將面臨雙重影響：一方面，客服、催收等傳統工作將被取代；另一方面，訓練、監督、設計和優化AI將創造新的工作機會。</p> <p>「軟體正在吞噬勞動力」的觀點呼應了馬克·安德森（Marc Andreessen）多年前提出的「軟體正在吞噬世界」。過去的軟體革命主要圍繞「資訊」，而現在的AI革命則聚焦於「<strong>行動</strong>」。軟體不再只是提供資訊，而是直接完成工作，這徹底改變了軟體從「工具」到「勞動力」的性質。大飛也提到了亞歷克斯·蘭佩爾用卡爾·馬克思的《資本論》來比喻，指出資本家投資資本，購買GPU，僱傭工程師利用GPU產出能執行勞動的軟體，這就像新的質能等價公式，E=MC²一般，具備深遠的影響。最後，大飛邀請觀眾分享對「軟體吞噬勞動力」這一觀點的看法。</p> <hr /><hr />================================================</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">大家好，这里是最佳拍档，我是大飞。不知道大家有没有想过。软件行业真正追逐的目标是什么呢？如果有人说，软件行业真正的目标。既不是更多的用户。也不是更高的市场份额。而是整个劳动力市场。你会不会觉得这听起来像是天方夜谭呢？但是当我们看到一组数字的时候。也许就会明白。全球 SaaS市场的规模大约为 3000 亿美元。而仅美国的劳动力市场就高达 13 万亿美元。而这个巨大的差距。正在被 AI 技术快速的缩小。最近。我看了 a16z 合伙人亚历克斯·兰佩尔 Alex Rampell 在2025 年 LP 峰会上的演讲。他用自己独特的视角。解释了这场正在发生的变革。那就是软件不再只是记录信息的工具。而是开始真正执行劳动本身。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">今天大飞就来带大家回顾一下这期演讲。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">看看软件行业究竟在发生什么样的变革。以及未来的趋势将会怎样演化。Alex 首先从软件发展的历史说起。他认为过去 70 年里。几乎所有软件公司做的事情都是把文件柜变成数据库。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/uiZEWwN5EKU_74.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如从航空订票系统到 CRM。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">从 ERP 到电子健康记录。本质上都是在把纸质文件数字化。但是关键的问题是，虽然介质变了。但是操作流程并没有发生本质的改变。过去看纸质文件的人。现在看电脑屏幕。仅此而已。因此，这种效率提升是有限的。因为最终还是需要人来读取、理解和操作这些信息。而现在，AI 正在改变这个游戏规则。软件不再只是存储和展示信息。而是开始理解信息并采取行动。而这将是一个质的飞跃。Alex举的第一个例子是 Sabre Systems。这是美国航空和 IBM 在 1959 年共同开发的航空订票系统。在有这个系统之前。航空公司是怎么管理订票的呢？想象一下，有无数个文件柜。里面塞满了表格。假设有一位叫Betty的乘客。打电话来说她想要 4A 座位。工作人员在纸上写下来。然后她又改主意说要取消。工作人员擦掉。她再改口说要 2C 座位。工作人员又擦掉重写。这个过程效率极低。而且信息无法在不同办公室之间共享。因为所有数据都锁在一个物理的文件柜里。Sabre 把这一切搬到了 IBM 大型机上。通过分布在全球各地的终端。让旅行社可以访问同一个系统。这彻底改变了旅游行业的运作方式。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/uiZEWwN5EKU_162.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">同样的故事在各个行业重复上演。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在销售领域。Alex 提到了电影《拜金一族》里那些业务员。为了获得好的客户名单而拼命竞争。而实际上，那些名单就是一张张纸。1980 年代出现了 ACT Systems。1990 年有 GoldMine。1993 年 Tom Siebel 创立了 Siebel Systems。这些都是 CRM。也就是客户关系管理系统的先驱。然后 Salesforce 在 1999 年把这一切搬到了云端。但是本质上。在1950 年代电影里那个翻查纸质文件的销售人员。和 2010 年打开 Salesforce 记录的销售人员。做的是同样的事情。只是介质从纸变成了屏幕。制造业和库存管理也是如此。作为产品制造商。你需要知道自己有多少库存。销售情况如何。IBM 再次走在前沿。但是也有其他公司跟进了。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如 1972 年成立的 SAP、JD Edwards、Epicor、Sage 等等。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这些都是把老式纸质记录数字化的公司。法律行业也一样。Alex 说他在 1980 年代去律所的时候。发现大部分空间都被文件柜占据了。像 PC Law、LexisNexis 和 Reuters 这样的公司。很大一部分收入来自为律所提供的数字化服务。把那些本来要占据昂贵办公空间的文件数字化。会计行业也是如此。Intuit 推出了 QuickBooks。把财务报表数字化了。还有 Peachtree、MYOB 等公司。都在做同样的事情。Alex花了很多时间来讲软件的历史。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/uiZEWwN5EKU_259.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">其实主要想说明的是。尽管技术在进步。但是效率提升是很有限的。因为文件柜是人在看。数字记录也还是人在看。那个坐在电脑前帮客户解决问题的客服人员。和以前翻纸质文件的客服人员。本质上没有区别。理解了这一点。才能理解为什么整个软件行业的商业模式必须改变。Alex 用了一个非常形象的比喻来描述当前的 SaaS 定价模式。他称之为"星巴克模式"，Tall、Grande、Venti. 俗称中杯、大杯、超大杯。如果你去任何一家 SaaS 公司的网站。他们的定价页面可能都长这样。他举了 Zendesk 的例子。这是一家现在被私有化的公司。年收入 20 亿美元，销售的是座席数。他们最受欢迎的套餐是 Suite Professional。每月 115 美元。但是问题来了。如果 AI 能够非常好地回答客户的问题。那么当每个客服人员的生产力都提高了9000 倍的时候。你还需要多少座席呢？让我们来看看具体的数字。假设一家公司有 1000 个客服人员在呼叫中心工作。每人的总成本是7.5 万美元一年。那就是每年 7500 万美元的人力成本。软件成本是多少呢？1000 个座席乘以 115 美元再乘以 12 个月。大约是每年 140 万美元。显然，人力成本远远高于软件成本。如果每个人工客服一年回答 2000 个问题。那么每个问题的成本大约是 37 美元的人力成本。加上 69 美分的软件成本。总共大约 38 美元。如果现在 AI 可以回答所有问题。那么会发生什么呢？这可能会朝两个方向发展。一种可能是，如果 AI 能处理一切。那你还需要多少座席呢？答案是零，一个都不需要。如果Zendesk还按座席收费的方式。那么他们的收入就会从 140 万美元降到零。这对 Zendesk 来说是灾难性的。但是另一种可能是。也许 Zendesk 可以收费 500 万美元一年。然后对客户说，嘿。你不要再花 7500 万美元在客服上了。改成花 500 万美元。全付给我们。只需要500 万，你就省下了 7000 万。所以说。Zendesk 现在真的是处在一个十字路口。他们的收入可能归零。也可能增长三倍。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">但是他们自己也不知道答案会是什么。Alex 说他一直在和 Zendesk 的 CEO 交流。他们现在正在新西兰试点基于结果的定价模式。结果如何，我们可以拭目以待。接下来。Alex 给出了一个让人震撼的数据对比。他说仅看护士这一个职业。美国的注册护士每年的收入总计大约 是6500 亿美元。大约有 450 万注册护士。这个单一职业的市场规模。就比整个全球软件市场还大。当然。这不意味着护理软件市场会有 6500 亿美元。只是为了说明软件真正竞争的池子有多大。这里Alex其实想表达的核心观点是。软件不再只是数字化文件柜。而是开始在文件柜上执行操作。这意味着什么呢？拿旅行订票来说。如果软件拥有了旅行数据。它可以帮你重新预订航班。或者你不需要和旅行代理沟通。可以直接和美联航的 AI 对话。让它来帮你搞定一切。销售领域更明显。Salesforce 是按座席收费的。但是它真正应该做的是直接帮客户销售。客户其实并不想付那 1000 个座席的钱。而是想为获得的客户付费。制造业、法律业、医疗健康领域、甚至连HR和薪酬系统也应该是如此。这背后的意义是。以前软件公司坐拥海量的数据。结果只是把数据展示给了人类。让人类来做决策和采取行动。而现在软件可以直接理解数据并且采取行动。完成从"提供信息"到"完成工作"的根本转变。这也正是为什么软件要瞄上劳动力市场的原因。Alex还分享了一个特别有意思的案例。他说因为跟腱受伤。不能跑步和骑自行车了。所以有大把时间。于是他就开始在 Craigslist 上浏览招聘信息。当然他不是为了自己找工作。而是为了观察市场。他找到了一个真实的招聘广告。Plaza Lane Optometry 这家眼科诊所。在招聘前台接待员。这个职位已经挂了 6 个月了。年薪是 4.5 万美元。显然，这个职位的薪酬并不高。如果他们挂个年薪 10 万美元。这个职位也许早就填上了，但是显然。他们只能支付 4.5 万美元的成本。如果我们仔细去看职位的职责说明。会发现第一条是开门关门上锁。这个AI 确实做不了。但是其他的很多职责 AI 就完全可以做了。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/uiZEWwN5EKU_547.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如和保险公司争论。在预约前一天给病人打电话防止爽约等等。如果我们从软件市场的角度来看眼科诊所。会断定这不是一个好的软件市场。Plaza Lane Optometry 可能每年只会花 500 美元在软件上。大概会买一个 Microsoft Office 许可证。或者可能用 Squarespace 或者Wix 建了个网站。就没了。所以他们每年软件支出大概也就是 500 美元。但在现在。Alex 看到很多公司开始这么干了。他们浏览 Craigslist，寻找招聘信息。然后说，嘿。Plaza Lane Optometry. 我想申请这个职位。眼科医生看到就会问，好啊。说说你的资质，你以前在哪工作？AI 然后回答，其实我是一家软件公司。我不能开门关门。但是我可以做其他八件事。要不要看个演示呢？一开始眼科医生可能会说不。但是后来可能会说。好吧，试试看吧。而且软件的价格是每年 2 万美元。远远低于招不到人的 4.5 万美元。Alex想用这个例子来说明市场扩张的逻辑。对于这些小型企业来说。软件的支出很小。但是劳动力的支出很高。当软件开始承担劳动力的工作的时候。它就进入了一个大得多的市场。这些以前被认为是不够大的行业。现在突然变得很有吸引力。因为软件不再只是卖给他们 500 美元的工具。而是在卖给他们 2 万美元的劳动力替代方案。Alex 在演讲中还播放了两段真实的 AI 语音通话录音。让抽象的概念变成了具体的现实。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/uiZEWwN5EKU_644.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第一个案例来自他们投资的公司 Happy Robot。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">服务货运和卡车运输行业。录音中。AI 正在和一个潜在的客户洽谈运费。我们先来听一下。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">整个对话我听了几遍，非常自然。有讨价还价。有妥协，完全不像是在和机器说话。不知道大家能不能听出来。谁是机器人。谁是人类么？可以说，这就是新的图灵测试。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/uiZEWwN5EKU_711.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第二个案例来自一家叫 Salient 的公司。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">专门做汽车贷款的催收业务。在录音中。AI 用西班牙语和一个客户通话。您的账户目前逾期 51 天。金额 825.35 美元。您今天能还款吗？Alex 特别指出。Salient 可以说几十种语言。包括他加禄语、越南语、普通话等等。通过这个例子，Alex 想要解释。为什么说AI取代人类的工作。不仅仅是成本问题。而是因为AI具有人类无法比拟的独特优势。解决了很多人类劳动力无法解决的结构性问题。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先是间歇性需求。想象一下黑色星期五的零售商。销售额在黑色星期五期间会大幅增长。所以需要雇佣大量的收银员。或者如果是在线零售商。则需要雇佣大量的客服人员来回答问题。实际上因为还需要培训。所以9月份就应该开始招聘了。但是过了黑五怎么办呢？把他们都解雇然后 11个月以后再重新雇佣吗？所以这个问题就很棘手。同样。还有很多其他行业也有间歇性需求的问题。但是 AI 在这方面非常擅长。其次是令人沮丧的工作。什么是令人沮丧的工作？催收就是其中的一种。因为你打电话给别人说。嘿，你逾期了。大概率对方会直接爆粗口。人类会对这样的工作感到疲惫。因为这不是一份愉快的工作。但是 AI 不会被这些困扰。所以对于这种会令人沮丧的工作。AI非常非常适合。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/uiZEWwN5EKU_799.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第三呢是监管确定性。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">Alex提到。他联合创办过一家叫做Affirm的公司。他们每个季度都要接受 UDAAP。也就是不公平、欺骗性和滥用性行为的培训。有很多法律规定你可以对客户说什么、不能说什么。想象一下你打电话给客户说，嘿。你欠钱了。客户爆了粗口。然后万一你那天心情不太好。也回了一句Fuck。这可能就会让你陷入麻烦。但是如果让一个经过编程的AI机器人。从头到尾来进行整个通话。你就会有更多的确定性。远比人类更加可靠。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第四是语言能力。Alex 说他花了太多时间。来学习俄语、日语和一点西班牙语。但是如果客户只会说波斯语怎么办呢？你在斯坦福能找到会说波斯语的护士吗？那如果万一客户只会说蒙古语呢？现在，有了AI之后。无论是护士、催收员、谈判员。所有这些工作都可以立刻用几十种不同的语言完成。显然，我们不可能在爱荷华州。找到一个会说塞尔维亚语、能够按需去做令人沮丧的工作。还能够经常处理间歇性需求的人。但是AI在这些方面都很出色。在解决了人类劳动力的结构性问题之后。Alex 特别强调了AI对劳动力市场的扩大效应。这也是为什么他要从文件柜的故事开始讲起。根据美国劳工统计局的数据。美国增长最快的工作是美甲师美容师。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/uiZEWwN5EKU_887.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第二快的就是合规官Compliance Officer。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">但是合规官不需要软件。对于银行来说。需要的只是更多的人。所以，没有公司专门做合规软件。因为这个软件市场太小了。不过，现在有了AI之后。你可以对银行说。我可以提供端到端的合规解决方案。每年只需要1000 万美元。我的软件产品可以追踪一切的合规问题。所以银行就不需要那么多人了。Alex还提出了另一个有趣的观点。那就是因为 AI。很多过去不可行的业务。现在也变得可行了。Alex 因为受伤不能骑自行车。他的车库里有很多闲置的自行车。他想问。为什么没有人做自行车的 Airbnb呢？</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">其实没人做这个的原因很简单。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">因为这是个很糟糕的生意。为什么说它糟糕呢？其中一个核心原则就是。不管有没有 AI。如果你的客户获取成本（CAC）加上销售成本（COGS）。大于客户的生命周期价值（LTV）。那就不要做，因为这不是一个生意。但是现在有了 AI，情况不同了。想象一下。如果你要做一个自行车的Airbnb。该怎么找到那些车库里有闲置自行车的人呢？你难道要雇佣一群昂贵的斯坦福学生。给他们提供 20 种不同口味的椰子水。满足他们千禧一代的各种需求。然后让他们在帕洛阿尔托的销售运营团队工作吗？不，正确的做法应该是。让 AI 销售代表给每个人打电话。它每年的成本只有几百美元。既不是 10 万美元，也不需要椰子水。如果有紧急的情况怎么办呢？那就直接让AI 代表来接听客服热线就好了。当然，如何对用户进行筛选。做背景调查。甚至判断自行车是不是好的。是不是偷来的。所有这些事情AI 都可以做。所以在Alex看来。实际上有一大类的业务。本来是可行的。但就是因为客户获取成本或者销售成本太高了。而没做成。现在有了 AI。你可以用 vibe coding 的方式快速启动这些业务。这会大大扩展市场的规模。因为 CAC 下降了，COGS 下降了。当然。每个公司最终都会开始使用这些工具。所以就像美国著名棒球教练尤吉·贝拉（Yogi Berra）的那句名言一样。太拥挤了，没人去那里了。但是现在很多公司。还尝试5 年前根本行不通的那些老想法。最后，Alex 在演讲结尾指出。这是一个全球性机会。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">虽然美国劳动力市场很大。一年13 万亿美元。但是全球劳动力市场要大得多。作为风险投资人。他们的工作是找到最好的公司。让软件市场看起来很小。接下来说说大飞我的感受。整个演讲听下来。Alex 其实想要描绘一个完整的转变图景。从文件柜到数据库，花了 70 年。这 70 年软件行业创造了 2.2 万亿美元的市值。以及每年 3000 亿美元的收入。但是这只是把记录数字化了。效率的提升终究有限。而现在 AI 让软件能够真正的执行工作。这意味着软件行业可以触及的市场。从 3000 亿美元扩大到 13 万亿美元甚至更多。简单来说，就是软件的角色。从记录工具变成了劳动力本身。这个转变的深远影响。可能还没有被大多数人所充分的理解。从全球视角来看，这个机会应该更大。发展中国家的劳动力成本虽然较低。但是仍然是软件成本的很多倍。而且很多发展中国家面临着熟练劳动力短缺的问题。AI 可以提供一致的、高质量的服务。不受地理位置限制。不受语言障碍的限制。这对全球经济发展的影响可能也会是革命性的。从商业模式角度来看。这意味着几乎所有 SaaS 公司都需要重新思考他们的定价策略。按座席收费在 AI 时代估计是行不通的。因为座席数会趋近于零。但是按结果收费又会带来新的挑战。如何定义结果？如何衡量价值？如何定价呢？一个帮你催收到 100 万美元的 AI 究竟值多少钱呢？帮你获得 10 个新客户的 AI 又值多少钱？这些都是需要重新思考的问题。从劳动力市场角度看。这场变革的影响应该说是双重的。一方面，很多传统工作确实会被取代。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/uiZEWwN5EKU_1142.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如客服、催收员、初级的销售人员、前台接待等等。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这些工作中的大部分任务可以由 AI 完成。但是另一方面。这也会创造新的工作机会。需要有人训练 AI，监督 AI。处理 AI 无法处理的复杂情况。需要有人设计 AI 的对话流程。优化 AI 的表现。确保 AI 符合法规的要求。这些都是新的工作类型。回到 Alex 演讲的标题。软件正在吞噬劳动力（Software is Eating Labor）。这个标题其实呼应了Marc Andreessen 十多年前的文章。软件正在吞噬世界（Software is Eating the World）。当然。劳动力市场确实是世界的一部分。但是这次也可能有些不同。过去的软件革命。大多都是关于信息的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如我们如何更好地记录、存储、检索和展示信息。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这创造了巨大的价值。但是软件本质上还是一个工具。需要人来使用。而现在的 AI 革命是关于行动的。软件不再只是提供信息。而是直接完成工作。这彻底改变了软件的性质。让它从工具变成了劳动力。回想到在演讲的开头。Alex 展示的那张卡尔·马克思的照片。他说。如果你读过《资本论》就会知道。书中的核心观点是资本和劳动。资本在剥削劳动。但是现在发生的是。投资人给公司投资资本。公司用这些资本来购买或者租用 GPU。然后雇佣工程师，给他们用GPU。然后产出可以完成劳动工作的软件。这就像是新的质能等价公式，E=MC²。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/uiZEWwN5EKU_1228.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">好了。以上就是这场演讲的主要内容了。那大家是如何看待Alex所提出。软件将会吞噬掉劳动力市场的观点呢？这其中是否蕴藏着大量的机会呢？还是觉得这可能就是个噱头。AI根本就无法代替大多数劳动力工作呢？欢迎在评论区发表你的看法。感谢收看本期视频。我们下期再见。</p>

<hr style="clear:both" />
=============
<p><a href="https://www.youtube.com/watch?v=uiZEWwN5EKU">https://www.youtube.com/watch?v=uiZEWwN5EKU</a></p><p>最近，我看了 a16z 合伙人亚历克斯·兰佩尔 Alex Rampell 在2025 年 LP 峰会上的演讲，他用自己独特的视角，解释了软件领域正在发生的变革，那就是软件不再只是记录信息的工具，而是开始真正执行劳动本身。今天大飞就来带大家回顾一下这期演讲，看看软件行业究竟在发生什么样的变革，以及未来的趋势将会怎样演化。</p><p></p><p><a href="https://youtu.be/dhyhR4Bzc0I?si=XIHic9m-o9tg6knW">https://youtu.be/dhyhR4Bzc0I?si=XIHic9m-o9tg6knW</a></p>]]>
      </description>
      <content:encoded><![CDATA[<hr style="clear:both" />

<p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://img.youtube.com/vi/uiZEWwN5EKU/maxresdefault.jpg" /></p><p><a href="https://www.youtube.com/watch?v=uiZEWwN5EKU">https://www.youtube.com/watch?v=uiZEWwN5EKU</a></p><h1>值得閱讀的理由</h1> <ul> <li>深入理解軟體產業的未來趨勢：本摘要將揭示軟體如何從單純的資訊工具，轉變為直接執行勞動的實體，這對於所有軟體開發者、投資人及企業決策者都至關重要。</li> <li>洞悉AI對商業模式的顛覆性影響：摘要詳細闡述了SaaS產業現有的「按座席收費」模式所面臨的挑戰，並探討了基於「成果」的計費方式，幫助讀者預見新的營收增長點與策略。</li> <li>掌握AI解決勞動力結構性問題的獨特優勢：從應對間歇性需求到處理繁瑣工作，從確保合規性到克服語言障礙，AI不僅能降低成本，更能以人類無法比擬的方式解決企業面臨的實際難題。</li> </ul> <hr /> <h1>摘要</h1> <p>這段影片由「最佳拍檔」的<a href="#author-intro">大飛</a>主講，深入探討了軟體行業的未來方向。作者引述了a16z合夥人亞歷克斯·蘭佩爾（Alex Rampell）在2025年LP峰會上的演講，提出了軟體產業的最終目標並非僅止於用戶數或市場份額，而是整個龐大的勞動力市場。全球SaaS市場規模約為3000億美元，而單是美國的勞動力市場便高達13兆美元，AI技術正迅速縮小這巨大的差距。演講的核心觀點是，軟體不再只是記錄資訊的工具，而是開始真正執行勞動本身，這將徹底改變軟體行業的本質及未來趨勢。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/uiZEWwN5EKU_80.jpg" /></p> <h2>軟體發展史：從文件櫃到數據庫</h2> <p>亞歷克斯·蘭佩爾首先回顧了軟體發展的歷史，指出過去70年來，幾乎所有軟體公司都在將實體文件櫃轉換為數字<strong>數據庫</strong>。無論是航空訂票系統Sabre Systems，還是客戶關係管理系統（CRM）如Siebel Systems和Salesforce，抑或是製造業和會計領域的SAP和QuickBooks，其本質都是將紙質文件數位化。然而，儘管介質發生了變化，但操作流程並未實質改變，最終仍需<strong>人工</strong>來讀取、理解和操作這些資訊，因此效率提升是有限的。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/uiZEWwN5EKU_300.jpg" /></p> <h2>AI帶來的典範轉移：軟體執行勞動</h2> <p>作者指出，AI技術正在顛覆這一遊戲規則。軟體不再只是儲存和展示資訊，而是開始<strong>理解資訊並採取行動</strong>。這標誌著軟體功能的一個質的飛躍，從被動的記錄工具轉變為主動的勞動執行者，從根本上改變了資訊處理的方式與效率。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/uiZEWwN5EKU_370.jpg" /></p> <h2>SaaS商業模式的衝擊：「星巴克模式」的終結</h2> <p>亞歷克斯·蘭佩爾以「星巴克模式」（中杯、大杯、超大杯）來形容目前SaaS公司常見的<strong>按「座席數」</strong>定價方式，並以客服軟體Zendesk為例進行說明。他指出，假設一家公司有1000名客服人員，每年的人力成本高達7500萬美元，而軟體成本僅約140萬美元。當AI能大幅提高客服生產力，甚至能處理絕大部分客戶問題時，對「座席」的需求將趨近於零。這將對Zendesk這類公司的營收構成巨大挑戰，但同時也存在轉機：如果Zendesk能提供以<strong>成果為導向</strong>的服務（例如每年500萬美元，幫助客戶節省7000萬美元人力成本），其收入反而可能大幅增長。這凸顯了SaaS公司商業模式面臨的巨大十字路口。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/uiZEWwN5EKU_570.jpg" /></p> <h2>軟體的新戰場：勞動力市場的潛力</h2> <p>亞歷克斯·蘭佩爾提出了一個令人震驚的數據：僅美國註冊護士一個職業，每年的總收入就達到6500億美元，這個單一勞動力市場的規模甚至比整個全球軟體市場還要大。這旨在強調，軟體真正的競爭池子是整個勞動力市場。軟體不再只是將文件櫃數位化，而是開始在這些數據上執行<strong>操作</strong>，將角色從「提供資訊」轉變為「<strong>完成工作</strong>」，這也是軟體產業瞄準勞動力市場的核心原因。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/uiZEWwN5EKU_615.jpg" /></p> <h2>案例分析：從眼科診所看市場擴張邏輯</h2> <p>作者分享了一個有趣的案例：一家小型眼科診所招聘前台接待員，年薪4.5萬美元，但職位已懸空六個月。傳統上，這類小型企業每年軟體開支可能只有數百美元，被認為是「不夠大」的軟體市場。然而，當AI公司「應聘」這個職位，聲稱能承擔大部分職責（除了開關門），並以每年2萬美元的價格提供服務時，情況就完全不同了。這不僅大大低於招聘不到人的成本，也讓過去不具吸引力的小型市場，因軟體提供<strong>勞動力替代方案</strong>而變得極具潛力，從根本上擴大了軟體市場的定義與規模。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/uiZEWwN5EKU_850.jpg" /></p> <h2>AI親自上陣：語音通話實例</h2> <p>亞歷克斯·蘭佩爾在演講中播放了兩段真實的AI語音通話錄音，將抽象概念轉化為具體現實。第一個案例來自投資公司Happy Robot，AI在貨運和卡車運輸行業中與潛在客戶自然地<strong>洽談運費</strong>，甚至包含了討價還價。第二個案例來自Salient公司，其AI能以西班牙語進行汽車貸款<strong>催收</strong>，並能支援幾十種語言。這些錄音展現了AI在複雜對話和情緒敏感場景中的高度擬真能力，堪稱是新的圖靈測試。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/uiZEWwN5EKU_945.jpg" /></p> <h2>AI取代人力的獨特優勢</h2> <p>作者進一步闡釋了AI取代人類工作不僅是成本問題，更是因為AI擁有許多人類無法比擬的獨特優勢，解決了許多結構性問題：首先是<strong>間歇性需求</strong>，如黑色星期五期間零售商對客服的暴增需求，AI能隨時擴展。其次是<strong>令人沮喪的工作</strong>，如催收，AI不會感到疲憊或受負面情緒影響。第三是<strong>監管確定性</strong>，AI能嚴格遵守法規，降低人類可能因情緒失控而帶來的違規風險。第四是<strong>語言能力</strong>，AI能即時處理幾十種不同語言的溝通，這對任何個人來說都是不可能實現的。因此，AI在處理間歇性、沮喪性、高度受監管且多語言的工作方面表現卓越。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/uiZEWwN5EKU_1180.jpg" /></p> <h2>AI如何拓展新業務領域</h2> <p>因為AI的出現，許多過去被認為不可行或市場太小的業務，現在也變得可行。例如，合規官曾是美國增長最快的工作之一，但因為需要大量人力而非軟體，因此沒有公司專門開發合規軟體。但現在，AI可以提供<strong>端到端</strong>的合規解決方案。此外，許多因客戶獲取成本（CAC）或銷售成本（COGS）過高而無法啟動的業務（例如自行車版Airbnb），現在也能通過AI銷售代表和客服降低成本，從而大大<strong>擴展市場規模</strong>。AI通過「vibe coding」方式，能快速啟動這些潛在業務，使沉寂的想法重新煥發生命力。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/uiZEWwN5EKU_21.jpg" /></p> <h2>大飛的總結與反思</h2> <p>大飛在影片結尾分享了他對演講的總結與感受。他認為，軟體行業在過去70年從「文件櫃」到「數據庫」的轉變，創造了巨大的價值，但效率提升有限。而AI的革命性意義在於，它使軟體能夠「執行工作」，將軟體從<strong>記錄工具</strong>轉變為<strong>勞動力本身</strong>，市場規模從數千億美元擴大到數十兆美元，甚至更大。這意味著所有SaaS公司都需重新思考其商業模式，從「按座席」轉向更具挑戰的「<strong>按結果</strong>」收費。勞動力市場也將面臨雙重影響：一方面，客服、催收等傳統工作將被取代；另一方面，訓練、監督、設計和優化AI將創造新的工作機會。</p> <p>「軟體正在吞噬勞動力」的觀點呼應了馬克·安德森（Marc Andreessen）多年前提出的「軟體正在吞噬世界」。過去的軟體革命主要圍繞「資訊」，而現在的AI革命則聚焦於「<strong>行動</strong>」。軟體不再只是提供資訊，而是直接完成工作，這徹底改變了軟體從「工具」到「勞動力」的性質。大飛也提到了亞歷克斯·蘭佩爾用卡爾·馬克思的《資本論》來比喻，指出資本家投資資本，購買GPU，僱傭工程師利用GPU產出能執行勞動的軟體，這就像新的質能等價公式，E=MC²一般，具備深遠的影響。最後，大飛邀請觀眾分享對「軟體吞噬勞動力」這一觀點的看法。</p> <hr /><hr />================================================</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">大家好，这里是最佳拍档，我是大飞。不知道大家有没有想过。软件行业真正追逐的目标是什么呢？如果有人说，软件行业真正的目标。既不是更多的用户。也不是更高的市场份额。而是整个劳动力市场。你会不会觉得这听起来像是天方夜谭呢？但是当我们看到一组数字的时候。也许就会明白。全球 SaaS市场的规模大约为 3000 亿美元。而仅美国的劳动力市场就高达 13 万亿美元。而这个巨大的差距。正在被 AI 技术快速的缩小。最近。我看了 a16z 合伙人亚历克斯·兰佩尔 Alex Rampell 在2025 年 LP 峰会上的演讲。他用自己独特的视角。解释了这场正在发生的变革。那就是软件不再只是记录信息的工具。而是开始真正执行劳动本身。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">今天大飞就来带大家回顾一下这期演讲。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">看看软件行业究竟在发生什么样的变革。以及未来的趋势将会怎样演化。Alex 首先从软件发展的历史说起。他认为过去 70 年里。几乎所有软件公司做的事情都是把文件柜变成数据库。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/uiZEWwN5EKU_74.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如从航空订票系统到 CRM。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">从 ERP 到电子健康记录。本质上都是在把纸质文件数字化。但是关键的问题是，虽然介质变了。但是操作流程并没有发生本质的改变。过去看纸质文件的人。现在看电脑屏幕。仅此而已。因此，这种效率提升是有限的。因为最终还是需要人来读取、理解和操作这些信息。而现在，AI 正在改变这个游戏规则。软件不再只是存储和展示信息。而是开始理解信息并采取行动。而这将是一个质的飞跃。Alex举的第一个例子是 Sabre Systems。这是美国航空和 IBM 在 1959 年共同开发的航空订票系统。在有这个系统之前。航空公司是怎么管理订票的呢？想象一下，有无数个文件柜。里面塞满了表格。假设有一位叫Betty的乘客。打电话来说她想要 4A 座位。工作人员在纸上写下来。然后她又改主意说要取消。工作人员擦掉。她再改口说要 2C 座位。工作人员又擦掉重写。这个过程效率极低。而且信息无法在不同办公室之间共享。因为所有数据都锁在一个物理的文件柜里。Sabre 把这一切搬到了 IBM 大型机上。通过分布在全球各地的终端。让旅行社可以访问同一个系统。这彻底改变了旅游行业的运作方式。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/uiZEWwN5EKU_162.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">同样的故事在各个行业重复上演。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在销售领域。Alex 提到了电影《拜金一族》里那些业务员。为了获得好的客户名单而拼命竞争。而实际上，那些名单就是一张张纸。1980 年代出现了 ACT Systems。1990 年有 GoldMine。1993 年 Tom Siebel 创立了 Siebel Systems。这些都是 CRM。也就是客户关系管理系统的先驱。然后 Salesforce 在 1999 年把这一切搬到了云端。但是本质上。在1950 年代电影里那个翻查纸质文件的销售人员。和 2010 年打开 Salesforce 记录的销售人员。做的是同样的事情。只是介质从纸变成了屏幕。制造业和库存管理也是如此。作为产品制造商。你需要知道自己有多少库存。销售情况如何。IBM 再次走在前沿。但是也有其他公司跟进了。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如 1972 年成立的 SAP、JD Edwards、Epicor、Sage 等等。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这些都是把老式纸质记录数字化的公司。法律行业也一样。Alex 说他在 1980 年代去律所的时候。发现大部分空间都被文件柜占据了。像 PC Law、LexisNexis 和 Reuters 这样的公司。很大一部分收入来自为律所提供的数字化服务。把那些本来要占据昂贵办公空间的文件数字化。会计行业也是如此。Intuit 推出了 QuickBooks。把财务报表数字化了。还有 Peachtree、MYOB 等公司。都在做同样的事情。Alex花了很多时间来讲软件的历史。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/uiZEWwN5EKU_259.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">其实主要想说明的是。尽管技术在进步。但是效率提升是很有限的。因为文件柜是人在看。数字记录也还是人在看。那个坐在电脑前帮客户解决问题的客服人员。和以前翻纸质文件的客服人员。本质上没有区别。理解了这一点。才能理解为什么整个软件行业的商业模式必须改变。Alex 用了一个非常形象的比喻来描述当前的 SaaS 定价模式。他称之为"星巴克模式"，Tall、Grande、Venti. 俗称中杯、大杯、超大杯。如果你去任何一家 SaaS 公司的网站。他们的定价页面可能都长这样。他举了 Zendesk 的例子。这是一家现在被私有化的公司。年收入 20 亿美元，销售的是座席数。他们最受欢迎的套餐是 Suite Professional。每月 115 美元。但是问题来了。如果 AI 能够非常好地回答客户的问题。那么当每个客服人员的生产力都提高了9000 倍的时候。你还需要多少座席呢？让我们来看看具体的数字。假设一家公司有 1000 个客服人员在呼叫中心工作。每人的总成本是7.5 万美元一年。那就是每年 7500 万美元的人力成本。软件成本是多少呢？1000 个座席乘以 115 美元再乘以 12 个月。大约是每年 140 万美元。显然，人力成本远远高于软件成本。如果每个人工客服一年回答 2000 个问题。那么每个问题的成本大约是 37 美元的人力成本。加上 69 美分的软件成本。总共大约 38 美元。如果现在 AI 可以回答所有问题。那么会发生什么呢？这可能会朝两个方向发展。一种可能是，如果 AI 能处理一切。那你还需要多少座席呢？答案是零，一个都不需要。如果Zendesk还按座席收费的方式。那么他们的收入就会从 140 万美元降到零。这对 Zendesk 来说是灾难性的。但是另一种可能是。也许 Zendesk 可以收费 500 万美元一年。然后对客户说，嘿。你不要再花 7500 万美元在客服上了。改成花 500 万美元。全付给我们。只需要500 万，你就省下了 7000 万。所以说。Zendesk 现在真的是处在一个十字路口。他们的收入可能归零。也可能增长三倍。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">但是他们自己也不知道答案会是什么。Alex 说他一直在和 Zendesk 的 CEO 交流。他们现在正在新西兰试点基于结果的定价模式。结果如何，我们可以拭目以待。接下来。Alex 给出了一个让人震撼的数据对比。他说仅看护士这一个职业。美国的注册护士每年的收入总计大约 是6500 亿美元。大约有 450 万注册护士。这个单一职业的市场规模。就比整个全球软件市场还大。当然。这不意味着护理软件市场会有 6500 亿美元。只是为了说明软件真正竞争的池子有多大。这里Alex其实想表达的核心观点是。软件不再只是数字化文件柜。而是开始在文件柜上执行操作。这意味着什么呢？拿旅行订票来说。如果软件拥有了旅行数据。它可以帮你重新预订航班。或者你不需要和旅行代理沟通。可以直接和美联航的 AI 对话。让它来帮你搞定一切。销售领域更明显。Salesforce 是按座席收费的。但是它真正应该做的是直接帮客户销售。客户其实并不想付那 1000 个座席的钱。而是想为获得的客户付费。制造业、法律业、医疗健康领域、甚至连HR和薪酬系统也应该是如此。这背后的意义是。以前软件公司坐拥海量的数据。结果只是把数据展示给了人类。让人类来做决策和采取行动。而现在软件可以直接理解数据并且采取行动。完成从"提供信息"到"完成工作"的根本转变。这也正是为什么软件要瞄上劳动力市场的原因。Alex还分享了一个特别有意思的案例。他说因为跟腱受伤。不能跑步和骑自行车了。所以有大把时间。于是他就开始在 Craigslist 上浏览招聘信息。当然他不是为了自己找工作。而是为了观察市场。他找到了一个真实的招聘广告。Plaza Lane Optometry 这家眼科诊所。在招聘前台接待员。这个职位已经挂了 6 个月了。年薪是 4.5 万美元。显然，这个职位的薪酬并不高。如果他们挂个年薪 10 万美元。这个职位也许早就填上了，但是显然。他们只能支付 4.5 万美元的成本。如果我们仔细去看职位的职责说明。会发现第一条是开门关门上锁。这个AI 确实做不了。但是其他的很多职责 AI 就完全可以做了。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/uiZEWwN5EKU_547.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如和保险公司争论。在预约前一天给病人打电话防止爽约等等。如果我们从软件市场的角度来看眼科诊所。会断定这不是一个好的软件市场。Plaza Lane Optometry 可能每年只会花 500 美元在软件上。大概会买一个 Microsoft Office 许可证。或者可能用 Squarespace 或者Wix 建了个网站。就没了。所以他们每年软件支出大概也就是 500 美元。但在现在。Alex 看到很多公司开始这么干了。他们浏览 Craigslist，寻找招聘信息。然后说，嘿。Plaza Lane Optometry. 我想申请这个职位。眼科医生看到就会问，好啊。说说你的资质，你以前在哪工作？AI 然后回答，其实我是一家软件公司。我不能开门关门。但是我可以做其他八件事。要不要看个演示呢？一开始眼科医生可能会说不。但是后来可能会说。好吧，试试看吧。而且软件的价格是每年 2 万美元。远远低于招不到人的 4.5 万美元。Alex想用这个例子来说明市场扩张的逻辑。对于这些小型企业来说。软件的支出很小。但是劳动力的支出很高。当软件开始承担劳动力的工作的时候。它就进入了一个大得多的市场。这些以前被认为是不够大的行业。现在突然变得很有吸引力。因为软件不再只是卖给他们 500 美元的工具。而是在卖给他们 2 万美元的劳动力替代方案。Alex 在演讲中还播放了两段真实的 AI 语音通话录音。让抽象的概念变成了具体的现实。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/uiZEWwN5EKU_644.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第一个案例来自他们投资的公司 Happy Robot。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">服务货运和卡车运输行业。录音中。AI 正在和一个潜在的客户洽谈运费。我们先来听一下。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">整个对话我听了几遍，非常自然。有讨价还价。有妥协，完全不像是在和机器说话。不知道大家能不能听出来。谁是机器人。谁是人类么？可以说，这就是新的图灵测试。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/uiZEWwN5EKU_711.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第二个案例来自一家叫 Salient 的公司。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">专门做汽车贷款的催收业务。在录音中。AI 用西班牙语和一个客户通话。您的账户目前逾期 51 天。金额 825.35 美元。您今天能还款吗？Alex 特别指出。Salient 可以说几十种语言。包括他加禄语、越南语、普通话等等。通过这个例子，Alex 想要解释。为什么说AI取代人类的工作。不仅仅是成本问题。而是因为AI具有人类无法比拟的独特优势。解决了很多人类劳动力无法解决的结构性问题。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先是间歇性需求。想象一下黑色星期五的零售商。销售额在黑色星期五期间会大幅增长。所以需要雇佣大量的收银员。或者如果是在线零售商。则需要雇佣大量的客服人员来回答问题。实际上因为还需要培训。所以9月份就应该开始招聘了。但是过了黑五怎么办呢？把他们都解雇然后 11个月以后再重新雇佣吗？所以这个问题就很棘手。同样。还有很多其他行业也有间歇性需求的问题。但是 AI 在这方面非常擅长。其次是令人沮丧的工作。什么是令人沮丧的工作？催收就是其中的一种。因为你打电话给别人说。嘿，你逾期了。大概率对方会直接爆粗口。人类会对这样的工作感到疲惫。因为这不是一份愉快的工作。但是 AI 不会被这些困扰。所以对于这种会令人沮丧的工作。AI非常非常适合。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/uiZEWwN5EKU_799.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第三呢是监管确定性。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">Alex提到。他联合创办过一家叫做Affirm的公司。他们每个季度都要接受 UDAAP。也就是不公平、欺骗性和滥用性行为的培训。有很多法律规定你可以对客户说什么、不能说什么。想象一下你打电话给客户说，嘿。你欠钱了。客户爆了粗口。然后万一你那天心情不太好。也回了一句Fuck。这可能就会让你陷入麻烦。但是如果让一个经过编程的AI机器人。从头到尾来进行整个通话。你就会有更多的确定性。远比人类更加可靠。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第四是语言能力。Alex 说他花了太多时间。来学习俄语、日语和一点西班牙语。但是如果客户只会说波斯语怎么办呢？你在斯坦福能找到会说波斯语的护士吗？那如果万一客户只会说蒙古语呢？现在，有了AI之后。无论是护士、催收员、谈判员。所有这些工作都可以立刻用几十种不同的语言完成。显然，我们不可能在爱荷华州。找到一个会说塞尔维亚语、能够按需去做令人沮丧的工作。还能够经常处理间歇性需求的人。但是AI在这些方面都很出色。在解决了人类劳动力的结构性问题之后。Alex 特别强调了AI对劳动力市场的扩大效应。这也是为什么他要从文件柜的故事开始讲起。根据美国劳工统计局的数据。美国增长最快的工作是美甲师美容师。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/uiZEWwN5EKU_887.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第二快的就是合规官Compliance Officer。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">但是合规官不需要软件。对于银行来说。需要的只是更多的人。所以，没有公司专门做合规软件。因为这个软件市场太小了。不过，现在有了AI之后。你可以对银行说。我可以提供端到端的合规解决方案。每年只需要1000 万美元。我的软件产品可以追踪一切的合规问题。所以银行就不需要那么多人了。Alex还提出了另一个有趣的观点。那就是因为 AI。很多过去不可行的业务。现在也变得可行了。Alex 因为受伤不能骑自行车。他的车库里有很多闲置的自行车。他想问。为什么没有人做自行车的 Airbnb呢？</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">其实没人做这个的原因很简单。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">因为这是个很糟糕的生意。为什么说它糟糕呢？其中一个核心原则就是。不管有没有 AI。如果你的客户获取成本（CAC）加上销售成本（COGS）。大于客户的生命周期价值（LTV）。那就不要做，因为这不是一个生意。但是现在有了 AI，情况不同了。想象一下。如果你要做一个自行车的Airbnb。该怎么找到那些车库里有闲置自行车的人呢？你难道要雇佣一群昂贵的斯坦福学生。给他们提供 20 种不同口味的椰子水。满足他们千禧一代的各种需求。然后让他们在帕洛阿尔托的销售运营团队工作吗？不，正确的做法应该是。让 AI 销售代表给每个人打电话。它每年的成本只有几百美元。既不是 10 万美元，也不需要椰子水。如果有紧急的情况怎么办呢？那就直接让AI 代表来接听客服热线就好了。当然，如何对用户进行筛选。做背景调查。甚至判断自行车是不是好的。是不是偷来的。所有这些事情AI 都可以做。所以在Alex看来。实际上有一大类的业务。本来是可行的。但就是因为客户获取成本或者销售成本太高了。而没做成。现在有了 AI。你可以用 vibe coding 的方式快速启动这些业务。这会大大扩展市场的规模。因为 CAC 下降了，COGS 下降了。当然。每个公司最终都会开始使用这些工具。所以就像美国著名棒球教练尤吉·贝拉（Yogi Berra）的那句名言一样。太拥挤了，没人去那里了。但是现在很多公司。还尝试5 年前根本行不通的那些老想法。最后，Alex 在演讲结尾指出。这是一个全球性机会。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">虽然美国劳动力市场很大。一年13 万亿美元。但是全球劳动力市场要大得多。作为风险投资人。他们的工作是找到最好的公司。让软件市场看起来很小。接下来说说大飞我的感受。整个演讲听下来。Alex 其实想要描绘一个完整的转变图景。从文件柜到数据库，花了 70 年。这 70 年软件行业创造了 2.2 万亿美元的市值。以及每年 3000 亿美元的收入。但是这只是把记录数字化了。效率的提升终究有限。而现在 AI 让软件能够真正的执行工作。这意味着软件行业可以触及的市场。从 3000 亿美元扩大到 13 万亿美元甚至更多。简单来说，就是软件的角色。从记录工具变成了劳动力本身。这个转变的深远影响。可能还没有被大多数人所充分的理解。从全球视角来看，这个机会应该更大。发展中国家的劳动力成本虽然较低。但是仍然是软件成本的很多倍。而且很多发展中国家面临着熟练劳动力短缺的问题。AI 可以提供一致的、高质量的服务。不受地理位置限制。不受语言障碍的限制。这对全球经济发展的影响可能也会是革命性的。从商业模式角度来看。这意味着几乎所有 SaaS 公司都需要重新思考他们的定价策略。按座席收费在 AI 时代估计是行不通的。因为座席数会趋近于零。但是按结果收费又会带来新的挑战。如何定义结果？如何衡量价值？如何定价呢？一个帮你催收到 100 万美元的 AI 究竟值多少钱呢？帮你获得 10 个新客户的 AI 又值多少钱？这些都是需要重新思考的问题。从劳动力市场角度看。这场变革的影响应该说是双重的。一方面，很多传统工作确实会被取代。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/uiZEWwN5EKU_1142.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如客服、催收员、初级的销售人员、前台接待等等。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这些工作中的大部分任务可以由 AI 完成。但是另一方面。这也会创造新的工作机会。需要有人训练 AI，监督 AI。处理 AI 无法处理的复杂情况。需要有人设计 AI 的对话流程。优化 AI 的表现。确保 AI 符合法规的要求。这些都是新的工作类型。回到 Alex 演讲的标题。软件正在吞噬劳动力（Software is Eating Labor）。这个标题其实呼应了Marc Andreessen 十多年前的文章。软件正在吞噬世界（Software is Eating the World）。当然。劳动力市场确实是世界的一部分。但是这次也可能有些不同。过去的软件革命。大多都是关于信息的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如我们如何更好地记录、存储、检索和展示信息。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这创造了巨大的价值。但是软件本质上还是一个工具。需要人来使用。而现在的 AI 革命是关于行动的。软件不再只是提供信息。而是直接完成工作。这彻底改变了软件的性质。让它从工具变成了劳动力。回想到在演讲的开头。Alex 展示的那张卡尔·马克思的照片。他说。如果你读过《资本论》就会知道。书中的核心观点是资本和劳动。资本在剥削劳动。但是现在发生的是。投资人给公司投资资本。公司用这些资本来购买或者租用 GPU。然后雇佣工程师，给他们用GPU。然后产出可以完成劳动工作的软件。这就像是新的质能等价公式，E=MC²。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/uiZEWwN5EKU_1228.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">好了。以上就是这场演讲的主要内容了。那大家是如何看待Alex所提出。软件将会吞噬掉劳动力市场的观点呢？这其中是否蕴藏着大量的机会呢？还是觉得这可能就是个噱头。AI根本就无法代替大多数劳动力工作呢？欢迎在评论区发表你的看法。感谢收看本期视频。我们下期再见。</p>

<hr style="clear:both" />
=============
<p><a href="https://www.youtube.com/watch?v=uiZEWwN5EKU">https://www.youtube.com/watch?v=uiZEWwN5EKU</a></p><p>最近，我看了 a16z 合伙人亚历克斯·兰佩尔 Alex Rampell 在2025 年 LP 峰会上的演讲，他用自己独特的视角，解释了软件领域正在发生的变革，那就是软件不再只是记录信息的工具，而是开始真正执行劳动本身。今天大飞就来带大家回顾一下这期演讲，看看软件行业究竟在发生什么样的变革，以及未来的趋势将会怎样演化。</p><p></p><p><a href="https://youtu.be/dhyhR4Bzc0I?si=XIHic9m-o9tg6knW">https://youtu.be/dhyhR4Bzc0I?si=XIHic9m-o9tg6knW</a></p>]]></content:encoded>
      <itunes:image href="https://i.ytimg.com/vi/uiZEWwN5EKU/hqdefault.jpg"/>
      <pubDate>2025-10-08T09:01:18.000Z</pubDate>
    </item><item>
      <title><![CDATA[【人工智能】英伟达的护城河 | 黄仁勋BG2专访 | 三种“缩放定律” | 与华尔街的认知分歧 | 英伟达的增长逻辑 | 战略绑定OpenAI | 极限协同设计 | CUDA | 中美竞争 | 未来]]></title>
      <link>https://www.youtube.com/watch?v=--HbDQk2-jA</link>
      <itunes:title><![CDATA[【人工智能】英伟达的护城河 | 黄仁勋BG2专访 | 三种“缩放定律” | 与华尔街的认知分歧 | 英伟达的增长逻辑 | 战略绑定OpenAI | 极限协同设计 | CUDA | 中美竞争 | 未来]]></itunes:title>
      <itunes:author><![CDATA[最佳拍档]]></itunes:author>
      <itunes:summary>
        <![CDATA[<hr style="clear:both" />

<p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://img.youtube.com/vi/--HbDQk2-jA/maxresdefault.jpg" /></p><p><a href="https://www.youtube.com/watch?v=--HbDQk2-jA">https://www.youtube.com/watch?v=--HbDQk2-jA</a></p><h1>值得閱讀的理由</h1> <ul> <li>深入了解<strong>黃仁勳對AI算力需求的獨到見解</strong>，包括他提出的「三重縮放定律」，揭示AI未來主要算力消耗將從訓練轉向使用。</li> <li>掌握<strong>NVIDIA如何構建其堅不可摧的商業護城河</strong>，不僅透過「極限協同設計」實現系統級創新，更透過開放生態策略應對競爭。</li> <li>理解<strong>黃仁勳對全球AI競賽、美中科技關係及AI社會影響的全面思考</strong>，包括他對「主權AI」的定義、對美中政策的批判，以及對「美國夢」作為人才磁石的深刻洞察。</li> </ul> <hr /> <h1>摘要</h1> <p>這段影片由「最佳拍檔」的「大飛」主持，回顧了NVIDIA執行長<strong>黃仁勳</strong>與知名播客BG2長達100多分鐘的獨家對話。這場訪談標題為「NVIDIA：OpenAI、計算的未來與美國夢」，被譽為黃仁勳近期資訊密度最高、含金量最足的一次分享。影片作者表示，黃仁勳在訪談中系統性地闡釋了華爾街與矽谷之間存在的巨大認知分歧，詳細剖析了NVIDIA看似堅不可摧的商業護城河，並分享了他對全球人工智能競賽、大國博弈及未來社會形態的完整思考。</p> <p><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_0.jpg" /></p></p> <hr /> <h2>AI算力需求的「三重縮放定律」</h2> <p>影片作者指出，一年前市場曾擔憂AI預訓練需求放緩會導致算力中心過剩，但黃仁勳當時大膽預測推理（Inference）需求將成長「十億倍」的量級，而事實證明，連這個預測都低估了實際需求。黃仁勳在訪談中系統性地提出了AI算力需求由三種「<strong>縮放定律</strong>」共同驅動的關鍵觀點：</p> <p>第一種是「<strong>預訓練縮放定律</strong>」（Pre-training Scaling Law），即模型越大、數據越多、訓練時間越長，模型就越智能。這是過去幾年大模型算力增長的主要驅動力。</p> <p>第二種是「<strong>後訓練縮放定律</strong>」（Post-training Scaling Law），指的是模型在預訓練後，透過強化學習和大量「試錯」、「推理」來精通特定技能的過程，這個「練習」過程本身需要消耗大量的推理計算。</p> <p>第三種是「<strong>推理時思維縮放定律</strong>」（Inference-time Thinking Scaling Law），黃仁勳認為這是市場最未理解透徹的「革命性」驅動因素。它描述AI在回答問題前，會進行內部「思考」、查證、梳理邏輯，這是一個多輪的內部推理過程，而非簡單一步計算。任務越複雜，AI「思考」時間越長，答案質量越高，而每一次內部循環都是一次計算消耗。</p> <p>這三重定律徹底改變了傳統認知，未來AI系統的絕大部分算力消耗將發生在「使用」階段，尤其當AI Agent能自主完成複雜任務時，其背後「思考」的計算資源將是過去的億萬倍。</p> <p><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_90.jpg" /></p></p> <hr /> <h2>黃仁勳解析NVIDIA增長邏輯：三大宏大敘事框架</h2> <p>影片作者提到，這種指數級的增長需求與華爾街的線性預測模型之間形成了黃仁勳所說的「<strong>巨大的認知分歧</strong>」。主持人布拉德·格斯特納直接指出，一方面，Sam Altman和Sundar Pichai等行業領袖談論「萬億級」算力投資；另一方面，華爾街分析師普遍預測NVIDIA的增長率將在2027年「斷崖式下跌」到約8%。黃仁勳為此給出了一個三層宏大敘事框架，闡釋NVIDIA的增長邏輯：</p> <p>第一層是「<strong>物理定律層面的轉變</strong>」，即通用計算時代已結束，未來是<strong>加速計算和AI計算</strong>的時代。隨著摩爾定律走到盡頭，靠CPU性能提升推動計算發展的模式已不可行。全球價值數萬億美元的CPU數據中心基礎設施，在下一輪更新時必須轉向GPU、TPU等加速計算架構。這是一個龐大的「<strong>存量市場替換</strong>」，而非創造新市場。</p> <p>第二層是「<strong>現有應用的遷移</strong>」，將網際網路的核心工作負載從CPU遷移到GPU，這足以驅動數百億美元的需求。黃仁勳以Meta、Google、Amazon、字節跳動等巨頭的核心業務（如搜索、推薦引擎、電商購物）為例，說明它們正全面轉向AI和GPU，以提供更好的個性化體驗和更高效率。這是在用更先進的技術重塑一個已服務全球40億人的龐大市場。</p> <p>第三層是「<strong>未來的增量</strong>」，指的是<strong>AI作為「智能工廠」對全球GDP的賦能</strong>。這是最令人興奮的部分，也是AI創造全新價值的地方。黃仁勳將AI工廠類比為工業革命中的「馬達」，透過生成Token來增強人類的智力勞動。他估算全球50萬億美元的「智力勞動」相關GDP，若每年需要10萬億美元的「AI Token生成服務」來增強，以50%毛利率計算，將需要5萬億美元的AI基礎設施來支撐。這三層框架清晰表明NVIDIA的增長既來自舊設施替換、現有業務升級，也來自未來AI創造的新價值。</p> <p><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_230.jpg" /></p></p> <hr /> <h2>NVIDIA與OpenAI的「星際之門」戰略結盟</h2> <p>影片作者接著探討了NVIDIA與OpenAI的「<strong>星際之門</strong>」（Stargate）計畫。黃仁勳明確指出，這不是一次普通的商業合作，而是一次「<strong>戰略綁定</strong>」，背後邏輯深遠。他毫不掩飾對OpenAI未來的看好，認為OpenAI很可能成為下一個數萬億美元級別的「超大規模公司」（Hyperscale Company），滲透到個人生活、企業辦公、工業生產等各領域。因此，能在OpenAI成為巨擘前進行投資，黃仁勳認為是「能想像到的最聰明投資之一」，且這是OpenAI主動給NVIDIA的機會。</p> <p>這次合作的深度和廣度遠超外界想像，黃仁勳將其拆分為三個層次：</p> <p>首先是「<strong>現有雲合作的延續</strong>」，NVIDIA將繼續與微軟合作，為OpenAI在Azure雲平台構建價值「數千億美元」的算力集群；同時，還會與甲骨文、軟銀合作，建設數個吉瓦（Gigawatts）級數據中心。</p> <p>其次是「<strong>幫助OpenAI自建基礎設施</strong>」，這是新合作的核心。過去OpenAI主要租用雲服務商資源，但現在將自行建設AI基礎設施，NVIDIA會從最底層開始參與，包括晶片設計、軟體開發、系統集成，甚至是整個AI工廠的規劃和營運，提供「<strong>全棧式</strong>」支持。</p> <p>最後是「<strong>建立直接的戰略關係</strong>」，黃仁勳將此關係類比為NVIDIA與Meta、Google、微軟、xAI等頂級科技公司的直接合作，意味著OpenAI規模已大到可與核心技術供應商直接、平等對話，形成深度綁定的戰略夥伴關係。</p> <p>黃仁勳指出OpenAI做出此調整的核心原因，是面臨「<strong>雙重指數級增長壓力</strong>」。一方面是「用戶增長指數」，AI越好用，用戶數量和使用頻率就指數級增長；另一方面是「計算增長指數」，每個用戶每次與AI交互所需的計算量也在指數級增長。這兩者疊加，意味著OpenAI的算力需求將以「<strong>指數的指數</strong>」速度增長，單靠租用雲服務已無法滿足，必須同時推進「租雲」和「自建」兩條路。</p> <p><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_450.jpg" /></p></p> <hr /> <h2>英偉達的堅實護城河：「極限協同設計」與系統級創新</h2> <p>影片作者指出，與OpenAI的緊密合作，反映出NVIDIA堅固的護城河。面對AMD、英特爾以及ASIC晶片公司的競爭，黃仁勳表示NVIDIA真正的護城河不是單一晶片的性能優勢，而是一種「<strong>極限協同設計</strong>」（Extreme Co-Design）的系統級創新能力。</p> <p>首先，他解釋了NVIDIA為何要搞「<strong>年度發布週期</strong>」。隨著摩爾定律失效，晶體管性能不再大幅提升，若不能快速提升整體性能，AI生成Token的成本將持續上升。為持續降低Token成本，唯一的辦法就是「系統級的創新」，這正是「極限協同設計」的由來。黃仁勳說，極限協同設計要求<strong>同步優化模型、演算法、系統和晶片</strong>，讓它們像一個整體一樣工作，而非各自為戰。這體現在晶片層面（并行研發CPU、GPU、網路晶片和NVLink）、系統層面（晶片整合優化，數據傳輸效率）、以及軟體層面（提供從底層驅動到上層應用庫的完整軟體棧），才使NVIDIA在一年內實現了30倍的性能提升。</p> <p>其次，這種系統能力還體現在「<strong>規模化部署</strong>」的巨大挑戰上。黃仁勳以Elon Musk的xAI公司需部署50萬個GPU為例，這不僅是硬體堆疊，還要考慮電力供應、散熱、數據傳輸延遲、系統穩定性等一系列問題。客戶之所以敢下數百億美元訂單給NVIDIA，是因為NVIDIA的架構經過市場驗證，能確保大規模系統穩定運行。這種「經得起考驗的規模化部署能力」本身就是一道很高的信任壁壘。黃仁勳強調，現在的競爭已是「我的整個<strong>AI工廠</strong>比你的AI工廠效率更高」。</p> <p>他甚至斷言，即使競爭對手把ASIC晶片免費送給客戶，客戶仍應選擇NVIDIA的系統。黃仁勳解釋說，數據中心建設中土地、電力、建築成本高昂，當企業拿到寶貴的電力配額時，核心目標是「用這些電力創造最大的商業價值」。若NVIDIA的「<strong>每瓦性能</strong>」（即每瓦能生成的Token數量）是競爭對手的兩倍，客戶就能產生兩倍收入。如果Blackwell GPU性能是上一代Hopper的30倍，在同樣電力消耗下，客戶能獲得30倍潛在收入。為省一點晶片成本而放棄30倍收入，機會成本「高得離譜」。</p> <p>此外，黃仁勳分析了ASIC的「生態定位」。他認為ASIC適合「功能固定、市場規模有限」的領域（如視頻轉碼），但對於「工作負載多樣且快速變化」的AI領域，ASIC的「專用性」反成致命弱點。AI需要高度「<strong>可程式性</strong>」的計算平台來快速適配新任務、新演算法，這正是GPU和CUDA生態的核心優勢。NVIDIA透過開放NVLink Fusion等接口，允許英特爾等公司晶片接入自家生態，展現「平台化」與「生態化」思維，透過開放來擴大生態，共同做大AI算力市場。</p> <p><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_660.jpg" /></p></p> <hr /> <h2>全球AI競賽與美中關係：黃仁勳的坦率批判</h2> <p>影片作者指出，黃仁勳在訪談中還花了大量篇幅討論全球人工智能競賽，尤其是美國與中國的關係。黃仁勳首先提出AI基礎設施已成為與能源、通信同等重要的<strong>國家戰略資源</strong>，全球正掀起「<strong>主權AI</strong>」（Sovereign AI）建設浪潮。他認為每個國家都需要擁有自己的AI基礎設施，用自己的數據和文化訓練AI模型，確保AI能服務於本國特定需求，就像每個國家都有自己的電網、通信網路一樣，未來也會有自己的AI基礎設施網路。</p> <p>關於美國的對華技術政策，黃仁勳坦率批評美國採取「<strong>小院高牆</strong>」式的對華技術封鎖（如限制NVIDIA向中國出口高端晶片），這看起來是在遏制中國AI發展，但實際上不僅徒勞無功，反而更是一種危險的「<strong>單方面裁軍</strong>」。他指出這種政策的兩個主要後果：</p> <p>第一，會<strong>催生強大的競爭對手</strong>。將擁有95%市場份額的NVIDIA排除出中國市場，相當於將整個中國市場拱手讓給華為等本土企業。這些企業會在「沒有強競爭」的環境下，靠「壟斷利潤」加速技術研發和產能擴張，最終成長為NVIDIA在全球市場的強勁對手。</p> <p>第二，<strong>嚴重低估了中國的能力</strong>。黃仁勳曾警告，外界普遍認為中國造不出高端晶片或技術落後美國數年的想法都是「瘋狂的」。中國擁有最渴望成功、最勤奮的企業家以及充滿活力的內部競爭生態。中美在晶片和AI領域的技術差距其實是以「納秒」來計算的，而非數年。</p> <p>黃仁勳認為，正確的路徑是讓美國最優秀的企業在中國市場與本土企業直接競爭，這最符合美國的國家利益。這樣不僅能為美國企業創造經濟價值，還能讓美國透過技術影響力在全球AI格局中保持話語權，更重要的是，競爭能倒逼美國科技企業不斷創新，保持技術最前沿。他強調，一個自信、強大的國家應秉持「<strong>放馬過來</strong>」（Bring it on）的態度。</p> <p>談到美國的核心優勢，黃仁勳尖銳地指出，美國擁有世界上任何國家都沒有的獨特品牌聲譽：「<strong>來到美國，實現美國夢</strong>」。作為從中國台灣移民到美國、從餐館洗碗工成長為萬億市值公司CEO的親歷者，黃仁勳對「美國夢」理解深刻。這種「讓每個人都有機會透過努力改變命運」的信念，是美國吸引全球頂尖人才的根本原因。但他警告，近年來這個核心優勢正受到嚴重挑戰。他觀察到一個危險信號：頂尖中國AI研究者來美國的意願已從三年前的90%驟降到現在的10%-15%。他呼籲美國政策制定者必須謹慎區分「與中國競爭」和「對中國人強硬」這兩個概念，後者會摧毀美國最寶貴的資產——作為全球人才燈塔的品牌形象。</p> <p><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_1050.jpg" /></p></p> <hr /> <h2>AI的未來願景與社會變革</h2> <p>影片作者總結道，展望未來，黃仁勳堅信人工智能將從根本上改變社會，帶來巨大的<strong>生產力提升</strong>，而非大規模失業。他認為智力不是零和遊戲，周圍聰明的人和工具越多，能想到的新點子、能解決的新問題就越多，創造的崗位也會越多。每項工作都會改變，有些會消失，但經濟整體會增長。在他看來，AI本身就是最偉大的<strong>均衡器</strong>，過去一個人想利用計算機創造經濟價值至少要學習Python編程，現在只需學習人類語言，技術鴻溝正被技術本身填平。</p> <p>對於未來的具體形態，他預言在未來五年內，人工智能與機器人技術的融合將成為現實，每個人都會像電影《星球大戰》中一樣擁有自己的「<strong>R2-D2</strong>」機器人。雲端的人工智能和實體世界的機器人將無處不在，生物學的複雜性將被揭示，每個人都將擁有自己的「<strong>數字孿生</strong>」，用於預測健康狀況和疾病。面對這種指數級加速的變化，黃仁勳給出的建議很簡單：就是「<strong>登上那列火車</strong>」，不要試圖去預測火車未來會到哪個站點，因為當它呈指數級加速時，任何預測都是徒勞的。唯一的策略就是趁現在它還相對較慢時跳上去，然後隨著它一起經歷指數級的旅程。NVIDIA從晶片公司到AI基礎設施公司的進化本身，就是登上這列火車的最好證明，而對於整個世界來說，這趟旅程才剛剛開始。</p> <p><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_23.jpg" /></p></p> <hr /><hr />================================================</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">大家好，这里是最佳拍档，我是大飞。9月26日。知名播客BG2和英伟达的CEO黄仁勋进行了一场独家对话。这次访谈的标题叫做“NVIDIA：。OpenAI、计算的未来和美国梦”，时长超过100分钟。堪称黄仁勋近期信息密度最高、干货最足的一次分享。在这场对话中。黄仁勋系统性地解释了华尔街与硅谷之间存在的巨大认知分歧。并且详细拆解了英伟达看似坚不可摧的商业护城河。以及他对全球人工智能竞赛、大国博弈和未来社会形态的完整思考。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">今天我们就来回顾一下这场访谈的内容。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">可能大家还记得。一年前市场上有个挺流行的担忧。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">AI的预训练需求好像暂时放缓了。会不会导致之前建的算力中心过剩呢？当时黄仁勋给出了一个非常大胆的预测。推理（Inference）需求的增长不是100倍。也不是1000倍。而是会达到“10亿倍”的量级。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_60.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">现在一年过去了，事实证明。连黄仁勋当时的这个预测。都还是低估了实际的需求增长。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">为什么会这样呢？黄仁勋在访谈里系统性地提出了一个关键观点。那就是AI的算力需求。其实是由三种“缩放定律”（Scaling Laws）所共同驱动的。而不是大家之前以为的只有一种。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第一种是“预训练缩放定律”（Pre-training Scaling Law）。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这也是大家最熟悉的一种。简单说就是模型越大、用的数据越多、训练时间越长。模型就越智能。过去几年。不管是GPT系列还是其他大模型。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">其实都是靠这个定律驱动算力增长的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如GPT-3用了千亿参数。训练时消耗的算力达到了每天几百PFlops。这背后就是预训练缩放定律在起作用。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">但是黄仁勋强调。这只是算力需求的“第一引擎”，真正关键的是另外两种。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第二种是“后训练缩放定律”（Post-training Scaling Law）。后训练指的就是模型在预训练之后。通过类似“练习”的方式。来精通某项特定技能的过程。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_122.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这个过程会结合强化学习。让AI通过大量“试错”和“推理”来优化自己。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">举个例子，我们想让AI学会写代码。光给它看海量的代码库还不够。还得让它不断尝试编写代码、调试错误、运行测试。直到能写出符合要求的程序。这个“练习”的过程，就是后训练。而这个过程本身。需要消耗的推理计算量非常大。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第三种是“推理时思维缩放定律”（Inference-time Thinking Scaling Law）。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这也是黄仁勋认为市场最没理解透的一点。堪称“革命性”的算力驱动因素。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">他在访谈里说道。旧的推理方式是一次性的。你问AI一个问题，它会直接给你答案。但是新推理方式是‘思考’，也就是在回答之前。AI会先自己琢磨、查证、梳理逻辑。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这种“思考”不是简单的一步计算。而是一个复杂的过程。AI在生成最终答案前。会进行多轮的内部推理、研究事实。甚至调用外部工具。这个过程简单的话会形成“思维链”，复杂一点的话还会形成“推理树”，而且任务越复杂。AI“思考”的时间就越长。答案质量就越高，而每一次内部循环。都是一次计算消耗。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_195.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这三种缩放定律放在一起。就彻底改变了我们对“训练”和“推理”的传统认知。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">过去大家觉得。算力主要消耗在“训练”阶段。但是现在看来。未来AI系统的大部分算力消耗。会发生在“使用”阶段。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">想象一下。当一个AI Agent能像人类员工一样。自主完成复杂的任务。它背后“思考”所消耗的计算资源。会是过去简单任务的亿万倍。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这也能解释为什么OpenAI、谷歌这些公司。一边发布更强大的基础模型。一边把重心转向能执行复杂任务的Agent系统。因为这才是算力需求的真正未来。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">不过，这种指数级的增长需求。与华尔街的线性预测模型之间。形成了黄仁勋所说的“巨大的认知分歧”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">访谈主持人布拉德·格斯特纳直接抛出了这个问题。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">一方面。Sam Altman和Sundar Pichai这些行业领袖。都在谈论“万亿级”的算力投资；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">但是另一方面。覆盖Nvidia的25位华尔街分析师。普遍预测Nvidia的增长率会在2027年“断崖式下跌”到大约8%。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_262.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">为什么会有这么大的认知差距呢？黄仁勋给出了一个三层的宏大叙事框架。不仅阐释了Nvidia的增长逻辑。也回答了“增长从哪来”、“增长能持续多久”这两个核心问题。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第一层是“物理定律层面的转变”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">通用计算的时代已经结束。未来是加速计算和AI计算的时代。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">黄仁勋说，摩尔定律已经走到尽头。靠CPU性能提升来推动计算发展的模式。已经行不通了。这意味着。全球现有的、价值数万亿美元的、基于通用计算。也就是CPU的数据中心基础设施。在下一轮更新的时候。必须转向加速计算架构。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如用GPU、TPU这些专门为AI设计的芯片。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这不是“创造新的市场”，而是“存量市场的替换”，仅仅把旧的CPU数据中心替换成加速计算数据中心。就已经是一个庞大的市场空间了。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第二层是“现有应用的迁移”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">把互联网的核心工作负载从CPU迁移到GPU。就足以驱动数百亿美元的需求。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_326.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">黄仁勋举了个例子。像搜索、推荐引擎、电商购物这些。支撑Meta、谷歌、亚马逊、字节跳动等科技巨头核心业务的系统。过去都是跑在CPU上的。但是现在。它们正在全面转向用AI和GPU。因为AI能提供更好的个性化体验。效率也更高。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这个过程不是在“造新的东西”，而是用更先进的技术。重塑一个已经存在的、服务全球40亿人的庞大市场。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">黄仁勋说道。这就像从煤油灯转向电力。从螺旋桨飞机转向喷气式飞机一样。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第三层是“未来的增量”，指的是AI作为“智能工厂”，对全球GDP的赋能。这是最让人兴奋的部分。也是AI创造全新价值的地方。黄仁勋把AI工厂类比成工业革命中的“马达”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">马达替代了体力劳动。而AI工厂则通过生成Token来增强人类的智力劳动。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">他还做了一个经济学估算，首先。全球GDP中。大约50%-65%和“智力劳动”相关。比如设计、研发、咨询、编程这些需要动脑的工作。总价值大概是50万亿美元；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_397.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">其次。AI会对这50万亿美元的经济活动进行“增强”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">假设一家公司有一个年薪10万美元的员工。公司愿意额外花1万美元给这个员工配备AI服务。换来2-3倍的生产力提升。这笔投资是非常划算的。黄仁勋说。Nvidia内部已经给每一位芯片设计师和软件工程师。都配了AI助手。结果生产力的提升非常明显；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">最后。如果全球50万亿美元的“智能GDP”，每年需要价值10万亿美元的“AI Token生成服务”来增强。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">假设这些服务的毛利率是50%，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">那么就需要价值5万亿美元的AI基础设施。来支撑这些服务的运行。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这三层框架一出来。Nvidia的增长逻辑就从“卖芯片”的简单故事。变成了和全球经济结构变迁同频共振的宏大叙事。它清晰地表明，Nvidia的增长。既来自旧设施的替换。也来自现有业务的升级。还来自未来AI创造的新价值。这也能解释为什么Nvidia的收入和数据中心的电力消耗高度相关。因为算力越多。需要的电力就越多。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_467.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如阿里巴巴的吴泳铭就说过。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">他们的数据中心电力消耗会在本年代末增长10倍。而AI生成的Token数量每几个月就翻一番。背后都是对算力的无尽需求。以及对Nvidia AI基础设施的依赖。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">聊完英伟达的增长逻辑。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">我们再来看这次访谈里另一个重磅话题。那就是Nvidia和OpenAI的“星际之门”（Stargate）计划。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">最近大家可能都注意到一个新闻。那就是英伟达要投资OpenAI千亿美元。来搞“星际之门”项目。很多人觉得这只是一次普通的商业合作。Nvidia卖芯片。OpenAI买算力。但是黄仁勋在访谈里明确的说道。这是一次“战略绑定”，背后的逻辑比大家想的深得多。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">黄仁勋毫不掩饰对OpenAI未来的看好。他认为。OpenAI很可能将成为下一个数万亿美元级别的超大规模公司（Hyperscale Company）。什么是超大规模公司？就是像Meta、谷歌、微软这样。既能服务消费者市场。又能服务企业市场。甚至成为全球基础设施一部分的公司。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_528.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">黄仁勋认为。OpenAI未来也会达到这个级别。它的AI服务会渗透到个人生活、企业办公、工业生产等各个领域。就像现在的互联网一样普及。所以。能在OpenAI成为“巨无霸”之前进行投资。黄仁勋觉得是“他们能想象到的最聪明的投资之一”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而且特别关键的一点是。这笔投资不是Nvidia强制要求的。而是OpenAI主动给Nvidia的机会。这说明OpenAI也认可Nvidia的战略价值。想和它深度绑定。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">其次，这次合作的深度和广度。远超外界想象。黄仁勋把双方的合作拆成了三个层次。第一个层次是“现有云合作的延续”，Nvidia会继续和微软合作。为OpenAI在Azure云平台上构建价值“数千亿美元”的算力集群；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">同时，还会和甲骨文、软银合作。建设数个吉瓦（Gigawatts）级的数据中心。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第二个层次是“帮助OpenAI自建基础设施”，这是这次新合作的核心。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">过去OpenAI的算力主要靠租云服务商的资源。但是现在它要自己建AI基础设施了。而Nvidia会从最底层开始参与。包括芯片设计、软件开发、系统集成。甚至整个AI工厂的规划和运营。是“全栈式”的支持。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_607.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第三个层次是“建立直接的战略关系”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">黄仁勋把这种关系类比成Nvidia和Meta的Mark Zuckerberg、谷歌的Sundar Pichai、微软的Satya Nadella、xAI的Elon Musk之间的直接合作。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这意味着OpenAI的规模已经大到。不需要再通过云服务商做“中间人”了。而是可以和最核心的技术供应商直接、平等地对话。形成深度绑定的战略伙伴关系。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">为什么OpenAI要做这样的调整呢？黄仁勋点出了其中的核心原因。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">那就是OpenAI面临“双重指数级增长压力”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">一方面是“用户增长指数”，AI越好用。应用场景越多。用户数量和使用频率就会指数级增长；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">另一方面是“计算增长指数”，就像我们前面聊的。每个用户每次和AI交互。因为“思考”的引入。需要的计算量也在指数级增长。这两个指数叠加在一起。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">意味着OpenAI的算力需求。会以“指数的指数”的速度增长。单靠租云服务已经满足不了了。所以必须同时推进“租云”和“自建”两条路。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_670.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">才能够确保算力供给跟得上需求。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">其实呢与Openai的紧密合作。背后。反映出的正是英伟达坚固的护城河。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">现在市场上有很多竞争对手。比如说AMD英特尔。还有一些公司在做ASIC芯片。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">主持人直接问黄仁勋。Nvidia的竞争护城河是在扩大还是缩小呢？</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">黄仁勋的回答则是。Nvidia真正的护城河。不是某一款芯片的性能优势。而是一种被称为“极限协同设计”（Extreme Co-Design）的系统级创新能力。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">黄仁勋解释了为什么Nvidia要搞“年度发布周期”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">过去芯片的更新是18-24个月一次。但是现在改成了每年一次。因为摩尔定律失效以后。晶体管的性能不再大幅提升。如果不能快速提升整体性能。AI生成Token的成本就会持续上升。而要持续降低Token成本。唯一的办法就是“系统级的创新”，这就是“极限协同设计”的由来。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">黄仁勋说。极限协同设计要求同步优化模型、算法、系统和芯片。让它们像一个整体一样工作。而不是各自为战。这和传统的“盒子内的创新”完全不同。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_740.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">过去是只想着把CPU做得更快。但是现在要同步升级构成AI数据中心的所有核心组件。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">具体来说。“极限协同设计”体现在三个层面。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在芯片层面。Nvidia会并行研发革命性的CPU、GPU、网络芯片和NVLink；。系统层面。会把这些芯片以最优化的方式整合起来。确保它们之间的数据传输效率最高。不会出现“某一个组件拖后腿”的情况；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">软件层面。会提供从底层驱动到上层应用库的完整软件栈。让开发者能轻松用上整个系统的能力。不用自己去解决硬件兼容、数据传输这些复杂问题。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">正是这种跨所有层面的协同设计。才让Nvidia从Hopper架构到Blackwell架构。在一年内实现了性能提升30倍的突破。这绝对不是靠单一芯片的技术进步能做到的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">其次。这种系统能力还体现在“规模化部署”的巨大挑战上。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">黄仁勋举了个例子。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">Elon Musk的xAI公司要部署Colossus 2的集群。需要用到50万个GPU。这不是简单地把50万个GPU堆在一起就行。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_808.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">要考虑电力供应、散热、数据传输延迟、系统稳定性等一系列的问题。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">任何一个环节出问题。整个集群都没法正常工作。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而客户之所以敢下数百亿美元的订单给Nvidia。就是因为Nvidia的架构经过了市场验证。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">能确保这么大规模的系统稳定运行。这种“经得住考验的规模化部署能力”，本身就是一道很高的信任壁垒。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">所以黄仁勋说。现在的竞争已经不是“我的芯片比你的快”，而是“我的整个AI工厂。比你的AI工厂效率更高”了。这种从“组件思维”到“系统思维”的跃迁。正是Nvidia能远超竞争对手的根本原因。毕竟。竞争对手可能能做出一款性能不错的芯片。但是要想做到“芯片、系统、软件”全链条的协同创新。还要能支撑几十万GPU的规模化部署。难度要大得多。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">黄仁勋甚至说。即使竞争对手把他们的ASIC芯片免费送给客户。客户还是应该选择Nvidia的系统。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_870.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">很多人听到这话会觉得不可思议。免费的芯片都不要？但是黄仁勋的逻辑。其实紧扣了数据中心的“现实约束”，那就是电力和空间都是有限的。黄仁勋解释说，数据中心建设中。土地、电力、建筑这些投入的成本非常高。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">当一家企业拿到宝贵的2吉瓦电力配额的时候。它的核心目标不再是“节省芯片成本”，而是“用这些电力创造最大的商业价值”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">他用简单的算术算了一笔账。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">如果每瓦性能。或者说每瓦能生成的Token数量。是竞争对手的两倍。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">那么客户用同样的数据中心。就能产生两倍的收入，谁不想要呢？</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">再具体一点。假设竞争对手的ASIC芯片。性能和Nvidia上一代的Hopper GPU差不多；。而Nvidia新一代的Blackwell GPU。性能是Hopper的30倍。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这意味着，在同样的电力消耗下。用Blackwell的客户能够获得30倍的潜在收入。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在这种情况下。为了节省一点芯片成本而放弃30倍的收入。这种机会成本显然“高得离谱”。任何理性的CFO。都会选择“每瓦性能”最高的解决方案。因为这直接决定了企业收入的上限。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_940.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">除此之外。黄仁勋还分析了ASIC的“生态定位”。他认为。ASIC适合那些“功能固定、市场规模有限”的领域。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如视频转码器、智能网卡等等。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这些任务的算法很少变。用ASIC能做到很高的效率。但是对于AI这种“工作负载多样且快速变化”的领域。ASIC的“专用性”反而成了致命弱点。因为AI需要处理的任务实在是太多了。聊天、写代码、生成图片视频、做数据分析、制定商业计划。等等等等。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而且底层算法还在不断演进。这就要求计算平台必须具备高度的“可编程性”，能够快速适配新任务、新算法。而这正是GPU和CUDA生态的核心优势。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这也能解释为什么谷歌虽然有自己的TPU。但同时也是Nvidia GPU的大客户。因为在一个复杂的计算集群里。既需要TPU这样的“专用辅助”芯片。也需要GPU这样的“通用主力”芯片。通过合理组合来实现整体最优。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_1001.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而Nvidia通过开放NVLink Fusion等接口。允许英特尔等公司的芯片接入自己的生态。这正是“平台化”和“生态化”思维的体现。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">非但不靠封闭来阻挡对手。反而是靠开放来扩大生态。让更多伙伴参与进来。一起把AI算力的市场做大。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在这场对话中。黄仁勋还花了大量篇幅讨论了全球人工智能竞赛。尤其是美国与中国的关系。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">黄仁勋首先提出了一个观点。AI基础设施已经成为和能源、通信同等重要的国家战略资源。所以全球范围内正在掀起“主权AI”（Sovereign AI）的建设浪潮。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">什么是主权AI？就是每个国家都需要拥有自己的AI基础设施。用自己的数据和文化训练AI模型。确保AI能服务于本国的特定需求。不管是工业生产、制造业升级。还是国家安全。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">黄仁勋认为。虽然各国会使用GPT、Gemini这些全球领先的模型。但是同时必须建立自己的主权AI能力。因为AI不仅是技术。还承载着文化、价值观和历史。一个国家不能把核心的智能需求。完全依赖于其他国家的技术。就像每个国家都会有自己的电网、通信网络一样。未来也会有自己的AI基础设施网络。而这为Nvidia等基础设施提供商。创造了全球性的全新市场。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_1083.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">关于美国的对华技术政策。黄仁勋提出了坦率的批评。他认为。美国采取“小院高墙”式的对华技术封锁。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如限制Nvidia向中国出口高端芯片。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">看起来是在遏制中国AI发展的做法。但是实际上不仅徒劳无功。反而更是一种危险的“单方面裁军”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">他指出了这种政策的两个主要后果。第一，会催生强大的竞争对手。把拥有95%市场份额的Nvidia排除出中国市场。相当于把整个中国市场拱手让给华为等本土企业。这些企业会在“没有强竞争”的环境下。会靠着“垄断利润”加速技术研发和产能扩张。最终成长为Nvidia在全球市场的强劲对手。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第二，严重低估了中国的能力。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">黄仁勋曾经警告说。外界普遍认为中国造不出高端芯片。或者技术上落后美国数年。这些想法都是“疯狂”的。实际上。中国拥有世界上最渴望成功、最勤奋的企业家。还有充满活力的内部竞争生态。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_1144.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">中国在芯片和AI领域和美国的技术差距。其实是以“纳秒”来计算的。不是大家想的“几年”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">当然这里的纳秒是打了引号的。意思是强调中美之间的差距很小。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">那么，正确的路径应该是什么呢？黄仁勋认为。让美国最优秀的企业在中国市场和本土企业直接竞争。才最符合美国的国家利益。这样做不仅能为美国企业创造经济价值。还能让美国通过技术影响力。在全球AI格局中保持话语权；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">更重要的是。竞争能倒逼美国科技企业不断创新。保持在技术最前沿。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">黄仁勋说道，一个自信、强大的国家。应该秉持‘放马过来’（Bring it on）的态度。相信自己的体系和人民能在竞争中胜出。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在谈到美国的核心优势时。黄仁勋的回答更是尖锐。他说。美国拥有一个世界上任何国家都没有的独特品牌声誉。那就是来到美国，实现美国梦。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">作为一个从中国台湾移民到美国。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_1204.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">从餐馆洗碗工成长为万亿市值公司CEO的亲历者。黄仁勋对“美国梦”的理解非常深刻。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这种“让每个人都有机会通过努力改变命运”的信念。是美国吸引全球顶尖人才的根本原因。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如过去几十年。全球最优秀的科学家、工程师、创业者都愿意去美国。因为那里有更好的机会、更开放的环境。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">但是黄仁勋警告说，近些年来。这个核心优势正在受到严重的挑战。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">他观察到一个非常危险的信号。顶尖中国AI研究者来美国的意愿。已经从三年前的90%骤降到现在的10%-15%。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这不是一个小变化。而是关乎美国未来的“生存危机”级别的早期预警。因为AI行业的竞争。本质上是人才的竞争，没有顶尖人才。再先进的技术也难以持续领先。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">所以黄仁勋呼吁美国政策制定者。必须谨慎区分“与中国竞争”和“对中国人强硬”这两个概念。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">“与中国竞争”是在技术、市场上的良性比拼。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_1266.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而“对中国人强硬”则是把优秀的中国人才拒之门外。这会摧毁美国最宝贵的资产。也就是美国作为全球人才灯塔的品牌形象。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">展望未来。黄仁勋认为人工智能将从根本上改变社会。他坚信。人工智能会带来巨大的生产力提升。而不是大规模的失业。那种认为AI会摧毁就业的观点。前提是“我们再也没有新的想法了”，但是他认为智力不是零和游戏。周围聪明的人和工具越多。能想到的新点子、能解决的新问题就越多。创造的岗位也会越多。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">每项工作都会改变，有些会消失。但是经济整体会增长。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在他看来。AI本身就是最伟大的均衡器。过去。一个人想利用计算机创造经济价值。至少得学习Python编程。现在，他们只需要学习人类语言。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">技术鸿沟正在被技术本身填平。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">对于未来的具体形态。他预言在未来五年内。人工智能与机器人技术的融合将成为现实。每个人都会像电影星球大战中一样。有自己的“R2-D2”机器人。成为生活中的伙伴和向导。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_1334.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">云端的人工智能和实体世界的机器人将无处不在。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">生物学的复杂性将被揭示。每个人都将拥有自己的“数字孪生”，用于预测健康状况和疾病。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">面对这种指数级加速的变化。黄仁勋给出的建议很简单。那就是登上那列火车。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">不要试图去预测火车未来会到哪个站点。因为当它呈指数级加速的时候。任何预测都是徒劳的。唯一的策略就是趁现在它还相对较慢时跳上去。然后随着它一起经历指数级的旅程。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">也许，从芯片公司到AI基础设施公司。英伟达的进化本身就是登上这列火车的最好证明。而对于整个世界来说。这趟旅程才刚刚开始。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">好了，感谢观看本期视频。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">我们下期再见。</p>

<hr style="clear:both" />
=============
<p><a href="https://www.youtube.com/watch?v=--HbDQk2-jA">https://www.youtube.com/watch?v=--HbDQk2-jA</a></p><p>9月26日，知名播客BG2和英伟达的CEO黄仁勋进行了一场独家对话。这次访谈的标题叫做“NVIDIA：OpenAI、计算的未来和美国梦”，时长超过100分钟，堪称黄仁勋近期信息密度最高、干货最足的一次分享。在这场对话中，黄仁勋系统性地解释了华尔街与硅谷之间存在的巨大认知分歧，并且详细拆解了英伟达看似坚不可摧的商业护城河，以及他对全球人工智能竞赛、大国博弈和未来社会形态的完整思考，今天我们就来回顾一下这场访谈的内容。</p><p><a href="https://youtu.be/pE6sw_E9Gh0?si=L41UzPmgS_lP5i43">https://youtu.be/pE6sw_E9Gh0?si=L41UzPmgS_lP5i43</a></p>]]>
      </itunes:summary>
      <description>
        <![CDATA[<hr style="clear:both" />

<p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://img.youtube.com/vi/--HbDQk2-jA/maxresdefault.jpg" /></p><p><a href="https://www.youtube.com/watch?v=--HbDQk2-jA">https://www.youtube.com/watch?v=--HbDQk2-jA</a></p><h1>值得閱讀的理由</h1> <ul> <li>深入了解<strong>黃仁勳對AI算力需求的獨到見解</strong>，包括他提出的「三重縮放定律」，揭示AI未來主要算力消耗將從訓練轉向使用。</li> <li>掌握<strong>NVIDIA如何構建其堅不可摧的商業護城河</strong>，不僅透過「極限協同設計」實現系統級創新，更透過開放生態策略應對競爭。</li> <li>理解<strong>黃仁勳對全球AI競賽、美中科技關係及AI社會影響的全面思考</strong>，包括他對「主權AI」的定義、對美中政策的批判，以及對「美國夢」作為人才磁石的深刻洞察。</li> </ul> <hr /> <h1>摘要</h1> <p>這段影片由「最佳拍檔」的「大飛」主持，回顧了NVIDIA執行長<strong>黃仁勳</strong>與知名播客BG2長達100多分鐘的獨家對話。這場訪談標題為「NVIDIA：OpenAI、計算的未來與美國夢」，被譽為黃仁勳近期資訊密度最高、含金量最足的一次分享。影片作者表示，黃仁勳在訪談中系統性地闡釋了華爾街與矽谷之間存在的巨大認知分歧，詳細剖析了NVIDIA看似堅不可摧的商業護城河，並分享了他對全球人工智能競賽、大國博弈及未來社會形態的完整思考。</p> <p><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_0.jpg" /></p></p> <hr /> <h2>AI算力需求的「三重縮放定律」</h2> <p>影片作者指出，一年前市場曾擔憂AI預訓練需求放緩會導致算力中心過剩，但黃仁勳當時大膽預測推理（Inference）需求將成長「十億倍」的量級，而事實證明，連這個預測都低估了實際需求。黃仁勳在訪談中系統性地提出了AI算力需求由三種「<strong>縮放定律</strong>」共同驅動的關鍵觀點：</p> <p>第一種是「<strong>預訓練縮放定律</strong>」（Pre-training Scaling Law），即模型越大、數據越多、訓練時間越長，模型就越智能。這是過去幾年大模型算力增長的主要驅動力。</p> <p>第二種是「<strong>後訓練縮放定律</strong>」（Post-training Scaling Law），指的是模型在預訓練後，透過強化學習和大量「試錯」、「推理」來精通特定技能的過程，這個「練習」過程本身需要消耗大量的推理計算。</p> <p>第三種是「<strong>推理時思維縮放定律</strong>」（Inference-time Thinking Scaling Law），黃仁勳認為這是市場最未理解透徹的「革命性」驅動因素。它描述AI在回答問題前，會進行內部「思考」、查證、梳理邏輯，這是一個多輪的內部推理過程，而非簡單一步計算。任務越複雜，AI「思考」時間越長，答案質量越高，而每一次內部循環都是一次計算消耗。</p> <p>這三重定律徹底改變了傳統認知，未來AI系統的絕大部分算力消耗將發生在「使用」階段，尤其當AI Agent能自主完成複雜任務時，其背後「思考」的計算資源將是過去的億萬倍。</p> <p><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_90.jpg" /></p></p> <hr /> <h2>黃仁勳解析NVIDIA增長邏輯：三大宏大敘事框架</h2> <p>影片作者提到，這種指數級的增長需求與華爾街的線性預測模型之間形成了黃仁勳所說的「<strong>巨大的認知分歧</strong>」。主持人布拉德·格斯特納直接指出，一方面，Sam Altman和Sundar Pichai等行業領袖談論「萬億級」算力投資；另一方面，華爾街分析師普遍預測NVIDIA的增長率將在2027年「斷崖式下跌」到約8%。黃仁勳為此給出了一個三層宏大敘事框架，闡釋NVIDIA的增長邏輯：</p> <p>第一層是「<strong>物理定律層面的轉變</strong>」，即通用計算時代已結束，未來是<strong>加速計算和AI計算</strong>的時代。隨著摩爾定律走到盡頭，靠CPU性能提升推動計算發展的模式已不可行。全球價值數萬億美元的CPU數據中心基礎設施，在下一輪更新時必須轉向GPU、TPU等加速計算架構。這是一個龐大的「<strong>存量市場替換</strong>」，而非創造新市場。</p> <p>第二層是「<strong>現有應用的遷移</strong>」，將網際網路的核心工作負載從CPU遷移到GPU，這足以驅動數百億美元的需求。黃仁勳以Meta、Google、Amazon、字節跳動等巨頭的核心業務（如搜索、推薦引擎、電商購物）為例，說明它們正全面轉向AI和GPU，以提供更好的個性化體驗和更高效率。這是在用更先進的技術重塑一個已服務全球40億人的龐大市場。</p> <p>第三層是「<strong>未來的增量</strong>」，指的是<strong>AI作為「智能工廠」對全球GDP的賦能</strong>。這是最令人興奮的部分，也是AI創造全新價值的地方。黃仁勳將AI工廠類比為工業革命中的「馬達」，透過生成Token來增強人類的智力勞動。他估算全球50萬億美元的「智力勞動」相關GDP，若每年需要10萬億美元的「AI Token生成服務」來增強，以50%毛利率計算，將需要5萬億美元的AI基礎設施來支撐。這三層框架清晰表明NVIDIA的增長既來自舊設施替換、現有業務升級，也來自未來AI創造的新價值。</p> <p><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_230.jpg" /></p></p> <hr /> <h2>NVIDIA與OpenAI的「星際之門」戰略結盟</h2> <p>影片作者接著探討了NVIDIA與OpenAI的「<strong>星際之門</strong>」（Stargate）計畫。黃仁勳明確指出，這不是一次普通的商業合作，而是一次「<strong>戰略綁定</strong>」，背後邏輯深遠。他毫不掩飾對OpenAI未來的看好，認為OpenAI很可能成為下一個數萬億美元級別的「超大規模公司」（Hyperscale Company），滲透到個人生活、企業辦公、工業生產等各領域。因此，能在OpenAI成為巨擘前進行投資，黃仁勳認為是「能想像到的最聰明投資之一」，且這是OpenAI主動給NVIDIA的機會。</p> <p>這次合作的深度和廣度遠超外界想像，黃仁勳將其拆分為三個層次：</p> <p>首先是「<strong>現有雲合作的延續</strong>」，NVIDIA將繼續與微軟合作，為OpenAI在Azure雲平台構建價值「數千億美元」的算力集群；同時，還會與甲骨文、軟銀合作，建設數個吉瓦（Gigawatts）級數據中心。</p> <p>其次是「<strong>幫助OpenAI自建基礎設施</strong>」，這是新合作的核心。過去OpenAI主要租用雲服務商資源，但現在將自行建設AI基礎設施，NVIDIA會從最底層開始參與，包括晶片設計、軟體開發、系統集成，甚至是整個AI工廠的規劃和營運，提供「<strong>全棧式</strong>」支持。</p> <p>最後是「<strong>建立直接的戰略關係</strong>」，黃仁勳將此關係類比為NVIDIA與Meta、Google、微軟、xAI等頂級科技公司的直接合作，意味著OpenAI規模已大到可與核心技術供應商直接、平等對話，形成深度綁定的戰略夥伴關係。</p> <p>黃仁勳指出OpenAI做出此調整的核心原因，是面臨「<strong>雙重指數級增長壓力</strong>」。一方面是「用戶增長指數」，AI越好用，用戶數量和使用頻率就指數級增長；另一方面是「計算增長指數」，每個用戶每次與AI交互所需的計算量也在指數級增長。這兩者疊加，意味著OpenAI的算力需求將以「<strong>指數的指數</strong>」速度增長，單靠租用雲服務已無法滿足，必須同時推進「租雲」和「自建」兩條路。</p> <p><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_450.jpg" /></p></p> <hr /> <h2>英偉達的堅實護城河：「極限協同設計」與系統級創新</h2> <p>影片作者指出，與OpenAI的緊密合作，反映出NVIDIA堅固的護城河。面對AMD、英特爾以及ASIC晶片公司的競爭，黃仁勳表示NVIDIA真正的護城河不是單一晶片的性能優勢，而是一種「<strong>極限協同設計</strong>」（Extreme Co-Design）的系統級創新能力。</p> <p>首先，他解釋了NVIDIA為何要搞「<strong>年度發布週期</strong>」。隨著摩爾定律失效，晶體管性能不再大幅提升，若不能快速提升整體性能，AI生成Token的成本將持續上升。為持續降低Token成本，唯一的辦法就是「系統級的創新」，這正是「極限協同設計」的由來。黃仁勳說，極限協同設計要求<strong>同步優化模型、演算法、系統和晶片</strong>，讓它們像一個整體一樣工作，而非各自為戰。這體現在晶片層面（并行研發CPU、GPU、網路晶片和NVLink）、系統層面（晶片整合優化，數據傳輸效率）、以及軟體層面（提供從底層驅動到上層應用庫的完整軟體棧），才使NVIDIA在一年內實現了30倍的性能提升。</p> <p>其次，這種系統能力還體現在「<strong>規模化部署</strong>」的巨大挑戰上。黃仁勳以Elon Musk的xAI公司需部署50萬個GPU為例，這不僅是硬體堆疊，還要考慮電力供應、散熱、數據傳輸延遲、系統穩定性等一系列問題。客戶之所以敢下數百億美元訂單給NVIDIA，是因為NVIDIA的架構經過市場驗證，能確保大規模系統穩定運行。這種「經得起考驗的規模化部署能力」本身就是一道很高的信任壁壘。黃仁勳強調，現在的競爭已是「我的整個<strong>AI工廠</strong>比你的AI工廠效率更高」。</p> <p>他甚至斷言，即使競爭對手把ASIC晶片免費送給客戶，客戶仍應選擇NVIDIA的系統。黃仁勳解釋說，數據中心建設中土地、電力、建築成本高昂，當企業拿到寶貴的電力配額時，核心目標是「用這些電力創造最大的商業價值」。若NVIDIA的「<strong>每瓦性能</strong>」（即每瓦能生成的Token數量）是競爭對手的兩倍，客戶就能產生兩倍收入。如果Blackwell GPU性能是上一代Hopper的30倍，在同樣電力消耗下，客戶能獲得30倍潛在收入。為省一點晶片成本而放棄30倍收入，機會成本「高得離譜」。</p> <p>此外，黃仁勳分析了ASIC的「生態定位」。他認為ASIC適合「功能固定、市場規模有限」的領域（如視頻轉碼），但對於「工作負載多樣且快速變化」的AI領域，ASIC的「專用性」反成致命弱點。AI需要高度「<strong>可程式性</strong>」的計算平台來快速適配新任務、新演算法，這正是GPU和CUDA生態的核心優勢。NVIDIA透過開放NVLink Fusion等接口，允許英特爾等公司晶片接入自家生態，展現「平台化」與「生態化」思維，透過開放來擴大生態，共同做大AI算力市場。</p> <p><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_660.jpg" /></p></p> <hr /> <h2>全球AI競賽與美中關係：黃仁勳的坦率批判</h2> <p>影片作者指出，黃仁勳在訪談中還花了大量篇幅討論全球人工智能競賽，尤其是美國與中國的關係。黃仁勳首先提出AI基礎設施已成為與能源、通信同等重要的<strong>國家戰略資源</strong>，全球正掀起「<strong>主權AI</strong>」（Sovereign AI）建設浪潮。他認為每個國家都需要擁有自己的AI基礎設施，用自己的數據和文化訓練AI模型，確保AI能服務於本國特定需求，就像每個國家都有自己的電網、通信網路一樣，未來也會有自己的AI基礎設施網路。</p> <p>關於美國的對華技術政策，黃仁勳坦率批評美國採取「<strong>小院高牆</strong>」式的對華技術封鎖（如限制NVIDIA向中國出口高端晶片），這看起來是在遏制中國AI發展，但實際上不僅徒勞無功，反而更是一種危險的「<strong>單方面裁軍</strong>」。他指出這種政策的兩個主要後果：</p> <p>第一，會<strong>催生強大的競爭對手</strong>。將擁有95%市場份額的NVIDIA排除出中國市場，相當於將整個中國市場拱手讓給華為等本土企業。這些企業會在「沒有強競爭」的環境下，靠「壟斷利潤」加速技術研發和產能擴張，最終成長為NVIDIA在全球市場的強勁對手。</p> <p>第二，<strong>嚴重低估了中國的能力</strong>。黃仁勳曾警告，外界普遍認為中國造不出高端晶片或技術落後美國數年的想法都是「瘋狂的」。中國擁有最渴望成功、最勤奮的企業家以及充滿活力的內部競爭生態。中美在晶片和AI領域的技術差距其實是以「納秒」來計算的，而非數年。</p> <p>黃仁勳認為，正確的路徑是讓美國最優秀的企業在中國市場與本土企業直接競爭，這最符合美國的國家利益。這樣不僅能為美國企業創造經濟價值，還能讓美國透過技術影響力在全球AI格局中保持話語權，更重要的是，競爭能倒逼美國科技企業不斷創新，保持技術最前沿。他強調，一個自信、強大的國家應秉持「<strong>放馬過來</strong>」（Bring it on）的態度。</p> <p>談到美國的核心優勢，黃仁勳尖銳地指出，美國擁有世界上任何國家都沒有的獨特品牌聲譽：「<strong>來到美國，實現美國夢</strong>」。作為從中國台灣移民到美國、從餐館洗碗工成長為萬億市值公司CEO的親歷者，黃仁勳對「美國夢」理解深刻。這種「讓每個人都有機會透過努力改變命運」的信念，是美國吸引全球頂尖人才的根本原因。但他警告，近年來這個核心優勢正受到嚴重挑戰。他觀察到一個危險信號：頂尖中國AI研究者來美國的意願已從三年前的90%驟降到現在的10%-15%。他呼籲美國政策制定者必須謹慎區分「與中國競爭」和「對中國人強硬」這兩個概念，後者會摧毀美國最寶貴的資產——作為全球人才燈塔的品牌形象。</p> <p><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_1050.jpg" /></p></p> <hr /> <h2>AI的未來願景與社會變革</h2> <p>影片作者總結道，展望未來，黃仁勳堅信人工智能將從根本上改變社會，帶來巨大的<strong>生產力提升</strong>，而非大規模失業。他認為智力不是零和遊戲，周圍聰明的人和工具越多，能想到的新點子、能解決的新問題就越多，創造的崗位也會越多。每項工作都會改變，有些會消失，但經濟整體會增長。在他看來，AI本身就是最偉大的<strong>均衡器</strong>，過去一個人想利用計算機創造經濟價值至少要學習Python編程，現在只需學習人類語言，技術鴻溝正被技術本身填平。</p> <p>對於未來的具體形態，他預言在未來五年內，人工智能與機器人技術的融合將成為現實，每個人都會像電影《星球大戰》中一樣擁有自己的「<strong>R2-D2</strong>」機器人。雲端的人工智能和實體世界的機器人將無處不在，生物學的複雜性將被揭示，每個人都將擁有自己的「<strong>數字孿生</strong>」，用於預測健康狀況和疾病。面對這種指數級加速的變化，黃仁勳給出的建議很簡單：就是「<strong>登上那列火車</strong>」，不要試圖去預測火車未來會到哪個站點，因為當它呈指數級加速時，任何預測都是徒勞的。唯一的策略就是趁現在它還相對較慢時跳上去，然後隨著它一起經歷指數級的旅程。NVIDIA從晶片公司到AI基礎設施公司的進化本身，就是登上這列火車的最好證明，而對於整個世界來說，這趟旅程才剛剛開始。</p> <p><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_23.jpg" /></p></p> <hr /><hr />================================================</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">大家好，这里是最佳拍档，我是大飞。9月26日。知名播客BG2和英伟达的CEO黄仁勋进行了一场独家对话。这次访谈的标题叫做“NVIDIA：。OpenAI、计算的未来和美国梦”，时长超过100分钟。堪称黄仁勋近期信息密度最高、干货最足的一次分享。在这场对话中。黄仁勋系统性地解释了华尔街与硅谷之间存在的巨大认知分歧。并且详细拆解了英伟达看似坚不可摧的商业护城河。以及他对全球人工智能竞赛、大国博弈和未来社会形态的完整思考。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">今天我们就来回顾一下这场访谈的内容。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">可能大家还记得。一年前市场上有个挺流行的担忧。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">AI的预训练需求好像暂时放缓了。会不会导致之前建的算力中心过剩呢？当时黄仁勋给出了一个非常大胆的预测。推理（Inference）需求的增长不是100倍。也不是1000倍。而是会达到“10亿倍”的量级。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_60.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">现在一年过去了，事实证明。连黄仁勋当时的这个预测。都还是低估了实际的需求增长。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">为什么会这样呢？黄仁勋在访谈里系统性地提出了一个关键观点。那就是AI的算力需求。其实是由三种“缩放定律”（Scaling Laws）所共同驱动的。而不是大家之前以为的只有一种。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第一种是“预训练缩放定律”（Pre-training Scaling Law）。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这也是大家最熟悉的一种。简单说就是模型越大、用的数据越多、训练时间越长。模型就越智能。过去几年。不管是GPT系列还是其他大模型。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">其实都是靠这个定律驱动算力增长的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如GPT-3用了千亿参数。训练时消耗的算力达到了每天几百PFlops。这背后就是预训练缩放定律在起作用。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">但是黄仁勋强调。这只是算力需求的“第一引擎”，真正关键的是另外两种。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第二种是“后训练缩放定律”（Post-training Scaling Law）。后训练指的就是模型在预训练之后。通过类似“练习”的方式。来精通某项特定技能的过程。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_122.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这个过程会结合强化学习。让AI通过大量“试错”和“推理”来优化自己。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">举个例子，我们想让AI学会写代码。光给它看海量的代码库还不够。还得让它不断尝试编写代码、调试错误、运行测试。直到能写出符合要求的程序。这个“练习”的过程，就是后训练。而这个过程本身。需要消耗的推理计算量非常大。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第三种是“推理时思维缩放定律”（Inference-time Thinking Scaling Law）。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这也是黄仁勋认为市场最没理解透的一点。堪称“革命性”的算力驱动因素。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">他在访谈里说道。旧的推理方式是一次性的。你问AI一个问题，它会直接给你答案。但是新推理方式是‘思考’，也就是在回答之前。AI会先自己琢磨、查证、梳理逻辑。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这种“思考”不是简单的一步计算。而是一个复杂的过程。AI在生成最终答案前。会进行多轮的内部推理、研究事实。甚至调用外部工具。这个过程简单的话会形成“思维链”，复杂一点的话还会形成“推理树”，而且任务越复杂。AI“思考”的时间就越长。答案质量就越高，而每一次内部循环。都是一次计算消耗。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_195.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这三种缩放定律放在一起。就彻底改变了我们对“训练”和“推理”的传统认知。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">过去大家觉得。算力主要消耗在“训练”阶段。但是现在看来。未来AI系统的大部分算力消耗。会发生在“使用”阶段。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">想象一下。当一个AI Agent能像人类员工一样。自主完成复杂的任务。它背后“思考”所消耗的计算资源。会是过去简单任务的亿万倍。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这也能解释为什么OpenAI、谷歌这些公司。一边发布更强大的基础模型。一边把重心转向能执行复杂任务的Agent系统。因为这才是算力需求的真正未来。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">不过，这种指数级的增长需求。与华尔街的线性预测模型之间。形成了黄仁勋所说的“巨大的认知分歧”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">访谈主持人布拉德·格斯特纳直接抛出了这个问题。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">一方面。Sam Altman和Sundar Pichai这些行业领袖。都在谈论“万亿级”的算力投资；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">但是另一方面。覆盖Nvidia的25位华尔街分析师。普遍预测Nvidia的增长率会在2027年“断崖式下跌”到大约8%。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_262.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">为什么会有这么大的认知差距呢？黄仁勋给出了一个三层的宏大叙事框架。不仅阐释了Nvidia的增长逻辑。也回答了“增长从哪来”、“增长能持续多久”这两个核心问题。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第一层是“物理定律层面的转变”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">通用计算的时代已经结束。未来是加速计算和AI计算的时代。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">黄仁勋说，摩尔定律已经走到尽头。靠CPU性能提升来推动计算发展的模式。已经行不通了。这意味着。全球现有的、价值数万亿美元的、基于通用计算。也就是CPU的数据中心基础设施。在下一轮更新的时候。必须转向加速计算架构。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如用GPU、TPU这些专门为AI设计的芯片。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这不是“创造新的市场”，而是“存量市场的替换”，仅仅把旧的CPU数据中心替换成加速计算数据中心。就已经是一个庞大的市场空间了。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第二层是“现有应用的迁移”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">把互联网的核心工作负载从CPU迁移到GPU。就足以驱动数百亿美元的需求。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_326.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">黄仁勋举了个例子。像搜索、推荐引擎、电商购物这些。支撑Meta、谷歌、亚马逊、字节跳动等科技巨头核心业务的系统。过去都是跑在CPU上的。但是现在。它们正在全面转向用AI和GPU。因为AI能提供更好的个性化体验。效率也更高。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这个过程不是在“造新的东西”，而是用更先进的技术。重塑一个已经存在的、服务全球40亿人的庞大市场。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">黄仁勋说道。这就像从煤油灯转向电力。从螺旋桨飞机转向喷气式飞机一样。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第三层是“未来的增量”，指的是AI作为“智能工厂”，对全球GDP的赋能。这是最让人兴奋的部分。也是AI创造全新价值的地方。黄仁勋把AI工厂类比成工业革命中的“马达”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">马达替代了体力劳动。而AI工厂则通过生成Token来增强人类的智力劳动。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">他还做了一个经济学估算，首先。全球GDP中。大约50%-65%和“智力劳动”相关。比如设计、研发、咨询、编程这些需要动脑的工作。总价值大概是50万亿美元；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_397.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">其次。AI会对这50万亿美元的经济活动进行“增强”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">假设一家公司有一个年薪10万美元的员工。公司愿意额外花1万美元给这个员工配备AI服务。换来2-3倍的生产力提升。这笔投资是非常划算的。黄仁勋说。Nvidia内部已经给每一位芯片设计师和软件工程师。都配了AI助手。结果生产力的提升非常明显；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">最后。如果全球50万亿美元的“智能GDP”，每年需要价值10万亿美元的“AI Token生成服务”来增强。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">假设这些服务的毛利率是50%，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">那么就需要价值5万亿美元的AI基础设施。来支撑这些服务的运行。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这三层框架一出来。Nvidia的增长逻辑就从“卖芯片”的简单故事。变成了和全球经济结构变迁同频共振的宏大叙事。它清晰地表明，Nvidia的增长。既来自旧设施的替换。也来自现有业务的升级。还来自未来AI创造的新价值。这也能解释为什么Nvidia的收入和数据中心的电力消耗高度相关。因为算力越多。需要的电力就越多。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_467.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如阿里巴巴的吴泳铭就说过。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">他们的数据中心电力消耗会在本年代末增长10倍。而AI生成的Token数量每几个月就翻一番。背后都是对算力的无尽需求。以及对Nvidia AI基础设施的依赖。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">聊完英伟达的增长逻辑。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">我们再来看这次访谈里另一个重磅话题。那就是Nvidia和OpenAI的“星际之门”（Stargate）计划。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">最近大家可能都注意到一个新闻。那就是英伟达要投资OpenAI千亿美元。来搞“星际之门”项目。很多人觉得这只是一次普通的商业合作。Nvidia卖芯片。OpenAI买算力。但是黄仁勋在访谈里明确的说道。这是一次“战略绑定”，背后的逻辑比大家想的深得多。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">黄仁勋毫不掩饰对OpenAI未来的看好。他认为。OpenAI很可能将成为下一个数万亿美元级别的超大规模公司（Hyperscale Company）。什么是超大规模公司？就是像Meta、谷歌、微软这样。既能服务消费者市场。又能服务企业市场。甚至成为全球基础设施一部分的公司。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_528.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">黄仁勋认为。OpenAI未来也会达到这个级别。它的AI服务会渗透到个人生活、企业办公、工业生产等各个领域。就像现在的互联网一样普及。所以。能在OpenAI成为“巨无霸”之前进行投资。黄仁勋觉得是“他们能想象到的最聪明的投资之一”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而且特别关键的一点是。这笔投资不是Nvidia强制要求的。而是OpenAI主动给Nvidia的机会。这说明OpenAI也认可Nvidia的战略价值。想和它深度绑定。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">其次，这次合作的深度和广度。远超外界想象。黄仁勋把双方的合作拆成了三个层次。第一个层次是“现有云合作的延续”，Nvidia会继续和微软合作。为OpenAI在Azure云平台上构建价值“数千亿美元”的算力集群；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">同时，还会和甲骨文、软银合作。建设数个吉瓦（Gigawatts）级的数据中心。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第二个层次是“帮助OpenAI自建基础设施”，这是这次新合作的核心。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">过去OpenAI的算力主要靠租云服务商的资源。但是现在它要自己建AI基础设施了。而Nvidia会从最底层开始参与。包括芯片设计、软件开发、系统集成。甚至整个AI工厂的规划和运营。是“全栈式”的支持。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_607.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第三个层次是“建立直接的战略关系”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">黄仁勋把这种关系类比成Nvidia和Meta的Mark Zuckerberg、谷歌的Sundar Pichai、微软的Satya Nadella、xAI的Elon Musk之间的直接合作。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这意味着OpenAI的规模已经大到。不需要再通过云服务商做“中间人”了。而是可以和最核心的技术供应商直接、平等地对话。形成深度绑定的战略伙伴关系。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">为什么OpenAI要做这样的调整呢？黄仁勋点出了其中的核心原因。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">那就是OpenAI面临“双重指数级增长压力”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">一方面是“用户增长指数”，AI越好用。应用场景越多。用户数量和使用频率就会指数级增长；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">另一方面是“计算增长指数”，就像我们前面聊的。每个用户每次和AI交互。因为“思考”的引入。需要的计算量也在指数级增长。这两个指数叠加在一起。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">意味着OpenAI的算力需求。会以“指数的指数”的速度增长。单靠租云服务已经满足不了了。所以必须同时推进“租云”和“自建”两条路。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_670.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">才能够确保算力供给跟得上需求。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">其实呢与Openai的紧密合作。背后。反映出的正是英伟达坚固的护城河。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">现在市场上有很多竞争对手。比如说AMD英特尔。还有一些公司在做ASIC芯片。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">主持人直接问黄仁勋。Nvidia的竞争护城河是在扩大还是缩小呢？</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">黄仁勋的回答则是。Nvidia真正的护城河。不是某一款芯片的性能优势。而是一种被称为“极限协同设计”（Extreme Co-Design）的系统级创新能力。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">黄仁勋解释了为什么Nvidia要搞“年度发布周期”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">过去芯片的更新是18-24个月一次。但是现在改成了每年一次。因为摩尔定律失效以后。晶体管的性能不再大幅提升。如果不能快速提升整体性能。AI生成Token的成本就会持续上升。而要持续降低Token成本。唯一的办法就是“系统级的创新”，这就是“极限协同设计”的由来。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">黄仁勋说。极限协同设计要求同步优化模型、算法、系统和芯片。让它们像一个整体一样工作。而不是各自为战。这和传统的“盒子内的创新”完全不同。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_740.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">过去是只想着把CPU做得更快。但是现在要同步升级构成AI数据中心的所有核心组件。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">具体来说。“极限协同设计”体现在三个层面。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在芯片层面。Nvidia会并行研发革命性的CPU、GPU、网络芯片和NVLink；。系统层面。会把这些芯片以最优化的方式整合起来。确保它们之间的数据传输效率最高。不会出现“某一个组件拖后腿”的情况；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">软件层面。会提供从底层驱动到上层应用库的完整软件栈。让开发者能轻松用上整个系统的能力。不用自己去解决硬件兼容、数据传输这些复杂问题。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">正是这种跨所有层面的协同设计。才让Nvidia从Hopper架构到Blackwell架构。在一年内实现了性能提升30倍的突破。这绝对不是靠单一芯片的技术进步能做到的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">其次。这种系统能力还体现在“规模化部署”的巨大挑战上。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">黄仁勋举了个例子。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">Elon Musk的xAI公司要部署Colossus 2的集群。需要用到50万个GPU。这不是简单地把50万个GPU堆在一起就行。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_808.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">要考虑电力供应、散热、数据传输延迟、系统稳定性等一系列的问题。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">任何一个环节出问题。整个集群都没法正常工作。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而客户之所以敢下数百亿美元的订单给Nvidia。就是因为Nvidia的架构经过了市场验证。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">能确保这么大规模的系统稳定运行。这种“经得住考验的规模化部署能力”，本身就是一道很高的信任壁垒。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">所以黄仁勋说。现在的竞争已经不是“我的芯片比你的快”，而是“我的整个AI工厂。比你的AI工厂效率更高”了。这种从“组件思维”到“系统思维”的跃迁。正是Nvidia能远超竞争对手的根本原因。毕竟。竞争对手可能能做出一款性能不错的芯片。但是要想做到“芯片、系统、软件”全链条的协同创新。还要能支撑几十万GPU的规模化部署。难度要大得多。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">黄仁勋甚至说。即使竞争对手把他们的ASIC芯片免费送给客户。客户还是应该选择Nvidia的系统。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_870.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">很多人听到这话会觉得不可思议。免费的芯片都不要？但是黄仁勋的逻辑。其实紧扣了数据中心的“现实约束”，那就是电力和空间都是有限的。黄仁勋解释说，数据中心建设中。土地、电力、建筑这些投入的成本非常高。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">当一家企业拿到宝贵的2吉瓦电力配额的时候。它的核心目标不再是“节省芯片成本”，而是“用这些电力创造最大的商业价值”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">他用简单的算术算了一笔账。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">如果每瓦性能。或者说每瓦能生成的Token数量。是竞争对手的两倍。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">那么客户用同样的数据中心。就能产生两倍的收入，谁不想要呢？</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">再具体一点。假设竞争对手的ASIC芯片。性能和Nvidia上一代的Hopper GPU差不多；。而Nvidia新一代的Blackwell GPU。性能是Hopper的30倍。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这意味着，在同样的电力消耗下。用Blackwell的客户能够获得30倍的潜在收入。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在这种情况下。为了节省一点芯片成本而放弃30倍的收入。这种机会成本显然“高得离谱”。任何理性的CFO。都会选择“每瓦性能”最高的解决方案。因为这直接决定了企业收入的上限。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_940.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">除此之外。黄仁勋还分析了ASIC的“生态定位”。他认为。ASIC适合那些“功能固定、市场规模有限”的领域。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如视频转码器、智能网卡等等。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这些任务的算法很少变。用ASIC能做到很高的效率。但是对于AI这种“工作负载多样且快速变化”的领域。ASIC的“专用性”反而成了致命弱点。因为AI需要处理的任务实在是太多了。聊天、写代码、生成图片视频、做数据分析、制定商业计划。等等等等。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而且底层算法还在不断演进。这就要求计算平台必须具备高度的“可编程性”，能够快速适配新任务、新算法。而这正是GPU和CUDA生态的核心优势。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这也能解释为什么谷歌虽然有自己的TPU。但同时也是Nvidia GPU的大客户。因为在一个复杂的计算集群里。既需要TPU这样的“专用辅助”芯片。也需要GPU这样的“通用主力”芯片。通过合理组合来实现整体最优。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_1001.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而Nvidia通过开放NVLink Fusion等接口。允许英特尔等公司的芯片接入自己的生态。这正是“平台化”和“生态化”思维的体现。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">非但不靠封闭来阻挡对手。反而是靠开放来扩大生态。让更多伙伴参与进来。一起把AI算力的市场做大。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在这场对话中。黄仁勋还花了大量篇幅讨论了全球人工智能竞赛。尤其是美国与中国的关系。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">黄仁勋首先提出了一个观点。AI基础设施已经成为和能源、通信同等重要的国家战略资源。所以全球范围内正在掀起“主权AI”（Sovereign AI）的建设浪潮。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">什么是主权AI？就是每个国家都需要拥有自己的AI基础设施。用自己的数据和文化训练AI模型。确保AI能服务于本国的特定需求。不管是工业生产、制造业升级。还是国家安全。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">黄仁勋认为。虽然各国会使用GPT、Gemini这些全球领先的模型。但是同时必须建立自己的主权AI能力。因为AI不仅是技术。还承载着文化、价值观和历史。一个国家不能把核心的智能需求。完全依赖于其他国家的技术。就像每个国家都会有自己的电网、通信网络一样。未来也会有自己的AI基础设施网络。而这为Nvidia等基础设施提供商。创造了全球性的全新市场。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_1083.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">关于美国的对华技术政策。黄仁勋提出了坦率的批评。他认为。美国采取“小院高墙”式的对华技术封锁。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如限制Nvidia向中国出口高端芯片。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">看起来是在遏制中国AI发展的做法。但是实际上不仅徒劳无功。反而更是一种危险的“单方面裁军”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">他指出了这种政策的两个主要后果。第一，会催生强大的竞争对手。把拥有95%市场份额的Nvidia排除出中国市场。相当于把整个中国市场拱手让给华为等本土企业。这些企业会在“没有强竞争”的环境下。会靠着“垄断利润”加速技术研发和产能扩张。最终成长为Nvidia在全球市场的强劲对手。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第二，严重低估了中国的能力。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">黄仁勋曾经警告说。外界普遍认为中国造不出高端芯片。或者技术上落后美国数年。这些想法都是“疯狂”的。实际上。中国拥有世界上最渴望成功、最勤奋的企业家。还有充满活力的内部竞争生态。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_1144.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">中国在芯片和AI领域和美国的技术差距。其实是以“纳秒”来计算的。不是大家想的“几年”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">当然这里的纳秒是打了引号的。意思是强调中美之间的差距很小。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">那么，正确的路径应该是什么呢？黄仁勋认为。让美国最优秀的企业在中国市场和本土企业直接竞争。才最符合美国的国家利益。这样做不仅能为美国企业创造经济价值。还能让美国通过技术影响力。在全球AI格局中保持话语权；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">更重要的是。竞争能倒逼美国科技企业不断创新。保持在技术最前沿。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">黄仁勋说道，一个自信、强大的国家。应该秉持‘放马过来’（Bring it on）的态度。相信自己的体系和人民能在竞争中胜出。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在谈到美国的核心优势时。黄仁勋的回答更是尖锐。他说。美国拥有一个世界上任何国家都没有的独特品牌声誉。那就是来到美国，实现美国梦。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">作为一个从中国台湾移民到美国。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_1204.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">从餐馆洗碗工成长为万亿市值公司CEO的亲历者。黄仁勋对“美国梦”的理解非常深刻。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这种“让每个人都有机会通过努力改变命运”的信念。是美国吸引全球顶尖人才的根本原因。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如过去几十年。全球最优秀的科学家、工程师、创业者都愿意去美国。因为那里有更好的机会、更开放的环境。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">但是黄仁勋警告说，近些年来。这个核心优势正在受到严重的挑战。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">他观察到一个非常危险的信号。顶尖中国AI研究者来美国的意愿。已经从三年前的90%骤降到现在的10%-15%。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这不是一个小变化。而是关乎美国未来的“生存危机”级别的早期预警。因为AI行业的竞争。本质上是人才的竞争，没有顶尖人才。再先进的技术也难以持续领先。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">所以黄仁勋呼吁美国政策制定者。必须谨慎区分“与中国竞争”和“对中国人强硬”这两个概念。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">“与中国竞争”是在技术、市场上的良性比拼。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_1266.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而“对中国人强硬”则是把优秀的中国人才拒之门外。这会摧毁美国最宝贵的资产。也就是美国作为全球人才灯塔的品牌形象。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">展望未来。黄仁勋认为人工智能将从根本上改变社会。他坚信。人工智能会带来巨大的生产力提升。而不是大规模的失业。那种认为AI会摧毁就业的观点。前提是“我们再也没有新的想法了”，但是他认为智力不是零和游戏。周围聪明的人和工具越多。能想到的新点子、能解决的新问题就越多。创造的岗位也会越多。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">每项工作都会改变，有些会消失。但是经济整体会增长。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在他看来。AI本身就是最伟大的均衡器。过去。一个人想利用计算机创造经济价值。至少得学习Python编程。现在，他们只需要学习人类语言。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">技术鸿沟正在被技术本身填平。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">对于未来的具体形态。他预言在未来五年内。人工智能与机器人技术的融合将成为现实。每个人都会像电影星球大战中一样。有自己的“R2-D2”机器人。成为生活中的伙伴和向导。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_1334.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">云端的人工智能和实体世界的机器人将无处不在。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">生物学的复杂性将被揭示。每个人都将拥有自己的“数字孪生”，用于预测健康状况和疾病。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">面对这种指数级加速的变化。黄仁勋给出的建议很简单。那就是登上那列火车。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">不要试图去预测火车未来会到哪个站点。因为当它呈指数级加速的时候。任何预测都是徒劳的。唯一的策略就是趁现在它还相对较慢时跳上去。然后随着它一起经历指数级的旅程。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">也许，从芯片公司到AI基础设施公司。英伟达的进化本身就是登上这列火车的最好证明。而对于整个世界来说。这趟旅程才刚刚开始。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">好了，感谢观看本期视频。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">我们下期再见。</p>

<hr style="clear:both" />
=============
<p><a href="https://www.youtube.com/watch?v=--HbDQk2-jA">https://www.youtube.com/watch?v=--HbDQk2-jA</a></p><p>9月26日，知名播客BG2和英伟达的CEO黄仁勋进行了一场独家对话。这次访谈的标题叫做“NVIDIA：OpenAI、计算的未来和美国梦”，时长超过100分钟，堪称黄仁勋近期信息密度最高、干货最足的一次分享。在这场对话中，黄仁勋系统性地解释了华尔街与硅谷之间存在的巨大认知分歧，并且详细拆解了英伟达看似坚不可摧的商业护城河，以及他对全球人工智能竞赛、大国博弈和未来社会形态的完整思考，今天我们就来回顾一下这场访谈的内容。</p><p><a href="https://youtu.be/pE6sw_E9Gh0?si=L41UzPmgS_lP5i43">https://youtu.be/pE6sw_E9Gh0?si=L41UzPmgS_lP5i43</a></p>]]>
      </description>
      <content:encoded><![CDATA[<hr style="clear:both" />

<p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://img.youtube.com/vi/--HbDQk2-jA/maxresdefault.jpg" /></p><p><a href="https://www.youtube.com/watch?v=--HbDQk2-jA">https://www.youtube.com/watch?v=--HbDQk2-jA</a></p><h1>值得閱讀的理由</h1> <ul> <li>深入了解<strong>黃仁勳對AI算力需求的獨到見解</strong>，包括他提出的「三重縮放定律」，揭示AI未來主要算力消耗將從訓練轉向使用。</li> <li>掌握<strong>NVIDIA如何構建其堅不可摧的商業護城河</strong>，不僅透過「極限協同設計」實現系統級創新，更透過開放生態策略應對競爭。</li> <li>理解<strong>黃仁勳對全球AI競賽、美中科技關係及AI社會影響的全面思考</strong>，包括他對「主權AI」的定義、對美中政策的批判，以及對「美國夢」作為人才磁石的深刻洞察。</li> </ul> <hr /> <h1>摘要</h1> <p>這段影片由「最佳拍檔」的「大飛」主持，回顧了NVIDIA執行長<strong>黃仁勳</strong>與知名播客BG2長達100多分鐘的獨家對話。這場訪談標題為「NVIDIA：OpenAI、計算的未來與美國夢」，被譽為黃仁勳近期資訊密度最高、含金量最足的一次分享。影片作者表示，黃仁勳在訪談中系統性地闡釋了華爾街與矽谷之間存在的巨大認知分歧，詳細剖析了NVIDIA看似堅不可摧的商業護城河，並分享了他對全球人工智能競賽、大國博弈及未來社會形態的完整思考。</p> <p><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_0.jpg" /></p></p> <hr /> <h2>AI算力需求的「三重縮放定律」</h2> <p>影片作者指出，一年前市場曾擔憂AI預訓練需求放緩會導致算力中心過剩，但黃仁勳當時大膽預測推理（Inference）需求將成長「十億倍」的量級，而事實證明，連這個預測都低估了實際需求。黃仁勳在訪談中系統性地提出了AI算力需求由三種「<strong>縮放定律</strong>」共同驅動的關鍵觀點：</p> <p>第一種是「<strong>預訓練縮放定律</strong>」（Pre-training Scaling Law），即模型越大、數據越多、訓練時間越長，模型就越智能。這是過去幾年大模型算力增長的主要驅動力。</p> <p>第二種是「<strong>後訓練縮放定律</strong>」（Post-training Scaling Law），指的是模型在預訓練後，透過強化學習和大量「試錯」、「推理」來精通特定技能的過程，這個「練習」過程本身需要消耗大量的推理計算。</p> <p>第三種是「<strong>推理時思維縮放定律</strong>」（Inference-time Thinking Scaling Law），黃仁勳認為這是市場最未理解透徹的「革命性」驅動因素。它描述AI在回答問題前，會進行內部「思考」、查證、梳理邏輯，這是一個多輪的內部推理過程，而非簡單一步計算。任務越複雜，AI「思考」時間越長，答案質量越高，而每一次內部循環都是一次計算消耗。</p> <p>這三重定律徹底改變了傳統認知，未來AI系統的絕大部分算力消耗將發生在「使用」階段，尤其當AI Agent能自主完成複雜任務時，其背後「思考」的計算資源將是過去的億萬倍。</p> <p><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_90.jpg" /></p></p> <hr /> <h2>黃仁勳解析NVIDIA增長邏輯：三大宏大敘事框架</h2> <p>影片作者提到，這種指數級的增長需求與華爾街的線性預測模型之間形成了黃仁勳所說的「<strong>巨大的認知分歧</strong>」。主持人布拉德·格斯特納直接指出，一方面，Sam Altman和Sundar Pichai等行業領袖談論「萬億級」算力投資；另一方面，華爾街分析師普遍預測NVIDIA的增長率將在2027年「斷崖式下跌」到約8%。黃仁勳為此給出了一個三層宏大敘事框架，闡釋NVIDIA的增長邏輯：</p> <p>第一層是「<strong>物理定律層面的轉變</strong>」，即通用計算時代已結束，未來是<strong>加速計算和AI計算</strong>的時代。隨著摩爾定律走到盡頭，靠CPU性能提升推動計算發展的模式已不可行。全球價值數萬億美元的CPU數據中心基礎設施，在下一輪更新時必須轉向GPU、TPU等加速計算架構。這是一個龐大的「<strong>存量市場替換</strong>」，而非創造新市場。</p> <p>第二層是「<strong>現有應用的遷移</strong>」，將網際網路的核心工作負載從CPU遷移到GPU，這足以驅動數百億美元的需求。黃仁勳以Meta、Google、Amazon、字節跳動等巨頭的核心業務（如搜索、推薦引擎、電商購物）為例，說明它們正全面轉向AI和GPU，以提供更好的個性化體驗和更高效率。這是在用更先進的技術重塑一個已服務全球40億人的龐大市場。</p> <p>第三層是「<strong>未來的增量</strong>」，指的是<strong>AI作為「智能工廠」對全球GDP的賦能</strong>。這是最令人興奮的部分，也是AI創造全新價值的地方。黃仁勳將AI工廠類比為工業革命中的「馬達」，透過生成Token來增強人類的智力勞動。他估算全球50萬億美元的「智力勞動」相關GDP，若每年需要10萬億美元的「AI Token生成服務」來增強，以50%毛利率計算，將需要5萬億美元的AI基礎設施來支撐。這三層框架清晰表明NVIDIA的增長既來自舊設施替換、現有業務升級，也來自未來AI創造的新價值。</p> <p><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_230.jpg" /></p></p> <hr /> <h2>NVIDIA與OpenAI的「星際之門」戰略結盟</h2> <p>影片作者接著探討了NVIDIA與OpenAI的「<strong>星際之門</strong>」（Stargate）計畫。黃仁勳明確指出，這不是一次普通的商業合作，而是一次「<strong>戰略綁定</strong>」，背後邏輯深遠。他毫不掩飾對OpenAI未來的看好，認為OpenAI很可能成為下一個數萬億美元級別的「超大規模公司」（Hyperscale Company），滲透到個人生活、企業辦公、工業生產等各領域。因此，能在OpenAI成為巨擘前進行投資，黃仁勳認為是「能想像到的最聰明投資之一」，且這是OpenAI主動給NVIDIA的機會。</p> <p>這次合作的深度和廣度遠超外界想像，黃仁勳將其拆分為三個層次：</p> <p>首先是「<strong>現有雲合作的延續</strong>」，NVIDIA將繼續與微軟合作，為OpenAI在Azure雲平台構建價值「數千億美元」的算力集群；同時，還會與甲骨文、軟銀合作，建設數個吉瓦（Gigawatts）級數據中心。</p> <p>其次是「<strong>幫助OpenAI自建基礎設施</strong>」，這是新合作的核心。過去OpenAI主要租用雲服務商資源，但現在將自行建設AI基礎設施，NVIDIA會從最底層開始參與，包括晶片設計、軟體開發、系統集成，甚至是整個AI工廠的規劃和營運，提供「<strong>全棧式</strong>」支持。</p> <p>最後是「<strong>建立直接的戰略關係</strong>」，黃仁勳將此關係類比為NVIDIA與Meta、Google、微軟、xAI等頂級科技公司的直接合作，意味著OpenAI規模已大到可與核心技術供應商直接、平等對話，形成深度綁定的戰略夥伴關係。</p> <p>黃仁勳指出OpenAI做出此調整的核心原因，是面臨「<strong>雙重指數級增長壓力</strong>」。一方面是「用戶增長指數」，AI越好用，用戶數量和使用頻率就指數級增長；另一方面是「計算增長指數」，每個用戶每次與AI交互所需的計算量也在指數級增長。這兩者疊加，意味著OpenAI的算力需求將以「<strong>指數的指數</strong>」速度增長，單靠租用雲服務已無法滿足，必須同時推進「租雲」和「自建」兩條路。</p> <p><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_450.jpg" /></p></p> <hr /> <h2>英偉達的堅實護城河：「極限協同設計」與系統級創新</h2> <p>影片作者指出，與OpenAI的緊密合作，反映出NVIDIA堅固的護城河。面對AMD、英特爾以及ASIC晶片公司的競爭，黃仁勳表示NVIDIA真正的護城河不是單一晶片的性能優勢，而是一種「<strong>極限協同設計</strong>」（Extreme Co-Design）的系統級創新能力。</p> <p>首先，他解釋了NVIDIA為何要搞「<strong>年度發布週期</strong>」。隨著摩爾定律失效，晶體管性能不再大幅提升，若不能快速提升整體性能，AI生成Token的成本將持續上升。為持續降低Token成本，唯一的辦法就是「系統級的創新」，這正是「極限協同設計」的由來。黃仁勳說，極限協同設計要求<strong>同步優化模型、演算法、系統和晶片</strong>，讓它們像一個整體一樣工作，而非各自為戰。這體現在晶片層面（并行研發CPU、GPU、網路晶片和NVLink）、系統層面（晶片整合優化，數據傳輸效率）、以及軟體層面（提供從底層驅動到上層應用庫的完整軟體棧），才使NVIDIA在一年內實現了30倍的性能提升。</p> <p>其次，這種系統能力還體現在「<strong>規模化部署</strong>」的巨大挑戰上。黃仁勳以Elon Musk的xAI公司需部署50萬個GPU為例，這不僅是硬體堆疊，還要考慮電力供應、散熱、數據傳輸延遲、系統穩定性等一系列問題。客戶之所以敢下數百億美元訂單給NVIDIA，是因為NVIDIA的架構經過市場驗證，能確保大規模系統穩定運行。這種「經得起考驗的規模化部署能力」本身就是一道很高的信任壁壘。黃仁勳強調，現在的競爭已是「我的整個<strong>AI工廠</strong>比你的AI工廠效率更高」。</p> <p>他甚至斷言，即使競爭對手把ASIC晶片免費送給客戶，客戶仍應選擇NVIDIA的系統。黃仁勳解釋說，數據中心建設中土地、電力、建築成本高昂，當企業拿到寶貴的電力配額時，核心目標是「用這些電力創造最大的商業價值」。若NVIDIA的「<strong>每瓦性能</strong>」（即每瓦能生成的Token數量）是競爭對手的兩倍，客戶就能產生兩倍收入。如果Blackwell GPU性能是上一代Hopper的30倍，在同樣電力消耗下，客戶能獲得30倍潛在收入。為省一點晶片成本而放棄30倍收入，機會成本「高得離譜」。</p> <p>此外，黃仁勳分析了ASIC的「生態定位」。他認為ASIC適合「功能固定、市場規模有限」的領域（如視頻轉碼），但對於「工作負載多樣且快速變化」的AI領域，ASIC的「專用性」反成致命弱點。AI需要高度「<strong>可程式性</strong>」的計算平台來快速適配新任務、新演算法，這正是GPU和CUDA生態的核心優勢。NVIDIA透過開放NVLink Fusion等接口，允許英特爾等公司晶片接入自家生態，展現「平台化」與「生態化」思維，透過開放來擴大生態，共同做大AI算力市場。</p> <p><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_660.jpg" /></p></p> <hr /> <h2>全球AI競賽與美中關係：黃仁勳的坦率批判</h2> <p>影片作者指出，黃仁勳在訪談中還花了大量篇幅討論全球人工智能競賽，尤其是美國與中國的關係。黃仁勳首先提出AI基礎設施已成為與能源、通信同等重要的<strong>國家戰略資源</strong>，全球正掀起「<strong>主權AI</strong>」（Sovereign AI）建設浪潮。他認為每個國家都需要擁有自己的AI基礎設施，用自己的數據和文化訓練AI模型，確保AI能服務於本國特定需求，就像每個國家都有自己的電網、通信網路一樣，未來也會有自己的AI基礎設施網路。</p> <p>關於美國的對華技術政策，黃仁勳坦率批評美國採取「<strong>小院高牆</strong>」式的對華技術封鎖（如限制NVIDIA向中國出口高端晶片），這看起來是在遏制中國AI發展，但實際上不僅徒勞無功，反而更是一種危險的「<strong>單方面裁軍</strong>」。他指出這種政策的兩個主要後果：</p> <p>第一，會<strong>催生強大的競爭對手</strong>。將擁有95%市場份額的NVIDIA排除出中國市場，相當於將整個中國市場拱手讓給華為等本土企業。這些企業會在「沒有強競爭」的環境下，靠「壟斷利潤」加速技術研發和產能擴張，最終成長為NVIDIA在全球市場的強勁對手。</p> <p>第二，<strong>嚴重低估了中國的能力</strong>。黃仁勳曾警告，外界普遍認為中國造不出高端晶片或技術落後美國數年的想法都是「瘋狂的」。中國擁有最渴望成功、最勤奮的企業家以及充滿活力的內部競爭生態。中美在晶片和AI領域的技術差距其實是以「納秒」來計算的，而非數年。</p> <p>黃仁勳認為，正確的路徑是讓美國最優秀的企業在中國市場與本土企業直接競爭，這最符合美國的國家利益。這樣不僅能為美國企業創造經濟價值，還能讓美國透過技術影響力在全球AI格局中保持話語權，更重要的是，競爭能倒逼美國科技企業不斷創新，保持技術最前沿。他強調，一個自信、強大的國家應秉持「<strong>放馬過來</strong>」（Bring it on）的態度。</p> <p>談到美國的核心優勢，黃仁勳尖銳地指出，美國擁有世界上任何國家都沒有的獨特品牌聲譽：「<strong>來到美國，實現美國夢</strong>」。作為從中國台灣移民到美國、從餐館洗碗工成長為萬億市值公司CEO的親歷者，黃仁勳對「美國夢」理解深刻。這種「讓每個人都有機會透過努力改變命運」的信念，是美國吸引全球頂尖人才的根本原因。但他警告，近年來這個核心優勢正受到嚴重挑戰。他觀察到一個危險信號：頂尖中國AI研究者來美國的意願已從三年前的90%驟降到現在的10%-15%。他呼籲美國政策制定者必須謹慎區分「與中國競爭」和「對中國人強硬」這兩個概念，後者會摧毀美國最寶貴的資產——作為全球人才燈塔的品牌形象。</p> <p><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_1050.jpg" /></p></p> <hr /> <h2>AI的未來願景與社會變革</h2> <p>影片作者總結道，展望未來，黃仁勳堅信人工智能將從根本上改變社會，帶來巨大的<strong>生產力提升</strong>，而非大規模失業。他認為智力不是零和遊戲，周圍聰明的人和工具越多，能想到的新點子、能解決的新問題就越多，創造的崗位也會越多。每項工作都會改變，有些會消失，但經濟整體會增長。在他看來，AI本身就是最偉大的<strong>均衡器</strong>，過去一個人想利用計算機創造經濟價值至少要學習Python編程，現在只需學習人類語言，技術鴻溝正被技術本身填平。</p> <p>對於未來的具體形態，他預言在未來五年內，人工智能與機器人技術的融合將成為現實，每個人都會像電影《星球大戰》中一樣擁有自己的「<strong>R2-D2</strong>」機器人。雲端的人工智能和實體世界的機器人將無處不在，生物學的複雜性將被揭示，每個人都將擁有自己的「<strong>數字孿生</strong>」，用於預測健康狀況和疾病。面對這種指數級加速的變化，黃仁勳給出的建議很簡單：就是「<strong>登上那列火車</strong>」，不要試圖去預測火車未來會到哪個站點，因為當它呈指數級加速時，任何預測都是徒勞的。唯一的策略就是趁現在它還相對較慢時跳上去，然後隨著它一起經歷指數級的旅程。NVIDIA從晶片公司到AI基礎設施公司的進化本身，就是登上這列火車的最好證明，而對於整個世界來說，這趟旅程才剛剛開始。</p> <p><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_23.jpg" /></p></p> <hr /><hr />================================================</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">大家好，这里是最佳拍档，我是大飞。9月26日。知名播客BG2和英伟达的CEO黄仁勋进行了一场独家对话。这次访谈的标题叫做“NVIDIA：。OpenAI、计算的未来和美国梦”，时长超过100分钟。堪称黄仁勋近期信息密度最高、干货最足的一次分享。在这场对话中。黄仁勋系统性地解释了华尔街与硅谷之间存在的巨大认知分歧。并且详细拆解了英伟达看似坚不可摧的商业护城河。以及他对全球人工智能竞赛、大国博弈和未来社会形态的完整思考。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">今天我们就来回顾一下这场访谈的内容。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">可能大家还记得。一年前市场上有个挺流行的担忧。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">AI的预训练需求好像暂时放缓了。会不会导致之前建的算力中心过剩呢？当时黄仁勋给出了一个非常大胆的预测。推理（Inference）需求的增长不是100倍。也不是1000倍。而是会达到“10亿倍”的量级。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_60.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">现在一年过去了，事实证明。连黄仁勋当时的这个预测。都还是低估了实际的需求增长。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">为什么会这样呢？黄仁勋在访谈里系统性地提出了一个关键观点。那就是AI的算力需求。其实是由三种“缩放定律”（Scaling Laws）所共同驱动的。而不是大家之前以为的只有一种。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第一种是“预训练缩放定律”（Pre-training Scaling Law）。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这也是大家最熟悉的一种。简单说就是模型越大、用的数据越多、训练时间越长。模型就越智能。过去几年。不管是GPT系列还是其他大模型。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">其实都是靠这个定律驱动算力增长的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如GPT-3用了千亿参数。训练时消耗的算力达到了每天几百PFlops。这背后就是预训练缩放定律在起作用。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">但是黄仁勋强调。这只是算力需求的“第一引擎”，真正关键的是另外两种。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第二种是“后训练缩放定律”（Post-training Scaling Law）。后训练指的就是模型在预训练之后。通过类似“练习”的方式。来精通某项特定技能的过程。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_122.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这个过程会结合强化学习。让AI通过大量“试错”和“推理”来优化自己。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">举个例子，我们想让AI学会写代码。光给它看海量的代码库还不够。还得让它不断尝试编写代码、调试错误、运行测试。直到能写出符合要求的程序。这个“练习”的过程，就是后训练。而这个过程本身。需要消耗的推理计算量非常大。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第三种是“推理时思维缩放定律”（Inference-time Thinking Scaling Law）。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这也是黄仁勋认为市场最没理解透的一点。堪称“革命性”的算力驱动因素。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">他在访谈里说道。旧的推理方式是一次性的。你问AI一个问题，它会直接给你答案。但是新推理方式是‘思考’，也就是在回答之前。AI会先自己琢磨、查证、梳理逻辑。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这种“思考”不是简单的一步计算。而是一个复杂的过程。AI在生成最终答案前。会进行多轮的内部推理、研究事实。甚至调用外部工具。这个过程简单的话会形成“思维链”，复杂一点的话还会形成“推理树”，而且任务越复杂。AI“思考”的时间就越长。答案质量就越高，而每一次内部循环。都是一次计算消耗。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_195.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这三种缩放定律放在一起。就彻底改变了我们对“训练”和“推理”的传统认知。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">过去大家觉得。算力主要消耗在“训练”阶段。但是现在看来。未来AI系统的大部分算力消耗。会发生在“使用”阶段。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">想象一下。当一个AI Agent能像人类员工一样。自主完成复杂的任务。它背后“思考”所消耗的计算资源。会是过去简单任务的亿万倍。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这也能解释为什么OpenAI、谷歌这些公司。一边发布更强大的基础模型。一边把重心转向能执行复杂任务的Agent系统。因为这才是算力需求的真正未来。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">不过，这种指数级的增长需求。与华尔街的线性预测模型之间。形成了黄仁勋所说的“巨大的认知分歧”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">访谈主持人布拉德·格斯特纳直接抛出了这个问题。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">一方面。Sam Altman和Sundar Pichai这些行业领袖。都在谈论“万亿级”的算力投资；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">但是另一方面。覆盖Nvidia的25位华尔街分析师。普遍预测Nvidia的增长率会在2027年“断崖式下跌”到大约8%。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_262.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">为什么会有这么大的认知差距呢？黄仁勋给出了一个三层的宏大叙事框架。不仅阐释了Nvidia的增长逻辑。也回答了“增长从哪来”、“增长能持续多久”这两个核心问题。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第一层是“物理定律层面的转变”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">通用计算的时代已经结束。未来是加速计算和AI计算的时代。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">黄仁勋说，摩尔定律已经走到尽头。靠CPU性能提升来推动计算发展的模式。已经行不通了。这意味着。全球现有的、价值数万亿美元的、基于通用计算。也就是CPU的数据中心基础设施。在下一轮更新的时候。必须转向加速计算架构。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如用GPU、TPU这些专门为AI设计的芯片。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这不是“创造新的市场”，而是“存量市场的替换”，仅仅把旧的CPU数据中心替换成加速计算数据中心。就已经是一个庞大的市场空间了。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第二层是“现有应用的迁移”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">把互联网的核心工作负载从CPU迁移到GPU。就足以驱动数百亿美元的需求。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_326.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">黄仁勋举了个例子。像搜索、推荐引擎、电商购物这些。支撑Meta、谷歌、亚马逊、字节跳动等科技巨头核心业务的系统。过去都是跑在CPU上的。但是现在。它们正在全面转向用AI和GPU。因为AI能提供更好的个性化体验。效率也更高。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这个过程不是在“造新的东西”，而是用更先进的技术。重塑一个已经存在的、服务全球40亿人的庞大市场。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">黄仁勋说道。这就像从煤油灯转向电力。从螺旋桨飞机转向喷气式飞机一样。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第三层是“未来的增量”，指的是AI作为“智能工厂”，对全球GDP的赋能。这是最让人兴奋的部分。也是AI创造全新价值的地方。黄仁勋把AI工厂类比成工业革命中的“马达”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">马达替代了体力劳动。而AI工厂则通过生成Token来增强人类的智力劳动。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">他还做了一个经济学估算，首先。全球GDP中。大约50%-65%和“智力劳动”相关。比如设计、研发、咨询、编程这些需要动脑的工作。总价值大概是50万亿美元；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_397.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">其次。AI会对这50万亿美元的经济活动进行“增强”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">假设一家公司有一个年薪10万美元的员工。公司愿意额外花1万美元给这个员工配备AI服务。换来2-3倍的生产力提升。这笔投资是非常划算的。黄仁勋说。Nvidia内部已经给每一位芯片设计师和软件工程师。都配了AI助手。结果生产力的提升非常明显；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">最后。如果全球50万亿美元的“智能GDP”，每年需要价值10万亿美元的“AI Token生成服务”来增强。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">假设这些服务的毛利率是50%，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">那么就需要价值5万亿美元的AI基础设施。来支撑这些服务的运行。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这三层框架一出来。Nvidia的增长逻辑就从“卖芯片”的简单故事。变成了和全球经济结构变迁同频共振的宏大叙事。它清晰地表明，Nvidia的增长。既来自旧设施的替换。也来自现有业务的升级。还来自未来AI创造的新价值。这也能解释为什么Nvidia的收入和数据中心的电力消耗高度相关。因为算力越多。需要的电力就越多。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_467.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如阿里巴巴的吴泳铭就说过。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">他们的数据中心电力消耗会在本年代末增长10倍。而AI生成的Token数量每几个月就翻一番。背后都是对算力的无尽需求。以及对Nvidia AI基础设施的依赖。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">聊完英伟达的增长逻辑。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">我们再来看这次访谈里另一个重磅话题。那就是Nvidia和OpenAI的“星际之门”（Stargate）计划。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">最近大家可能都注意到一个新闻。那就是英伟达要投资OpenAI千亿美元。来搞“星际之门”项目。很多人觉得这只是一次普通的商业合作。Nvidia卖芯片。OpenAI买算力。但是黄仁勋在访谈里明确的说道。这是一次“战略绑定”，背后的逻辑比大家想的深得多。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">黄仁勋毫不掩饰对OpenAI未来的看好。他认为。OpenAI很可能将成为下一个数万亿美元级别的超大规模公司（Hyperscale Company）。什么是超大规模公司？就是像Meta、谷歌、微软这样。既能服务消费者市场。又能服务企业市场。甚至成为全球基础设施一部分的公司。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_528.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">黄仁勋认为。OpenAI未来也会达到这个级别。它的AI服务会渗透到个人生活、企业办公、工业生产等各个领域。就像现在的互联网一样普及。所以。能在OpenAI成为“巨无霸”之前进行投资。黄仁勋觉得是“他们能想象到的最聪明的投资之一”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而且特别关键的一点是。这笔投资不是Nvidia强制要求的。而是OpenAI主动给Nvidia的机会。这说明OpenAI也认可Nvidia的战略价值。想和它深度绑定。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">其次，这次合作的深度和广度。远超外界想象。黄仁勋把双方的合作拆成了三个层次。第一个层次是“现有云合作的延续”，Nvidia会继续和微软合作。为OpenAI在Azure云平台上构建价值“数千亿美元”的算力集群；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">同时，还会和甲骨文、软银合作。建设数个吉瓦（Gigawatts）级的数据中心。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第二个层次是“帮助OpenAI自建基础设施”，这是这次新合作的核心。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">过去OpenAI的算力主要靠租云服务商的资源。但是现在它要自己建AI基础设施了。而Nvidia会从最底层开始参与。包括芯片设计、软件开发、系统集成。甚至整个AI工厂的规划和运营。是“全栈式”的支持。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_607.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第三个层次是“建立直接的战略关系”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">黄仁勋把这种关系类比成Nvidia和Meta的Mark Zuckerberg、谷歌的Sundar Pichai、微软的Satya Nadella、xAI的Elon Musk之间的直接合作。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这意味着OpenAI的规模已经大到。不需要再通过云服务商做“中间人”了。而是可以和最核心的技术供应商直接、平等地对话。形成深度绑定的战略伙伴关系。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">为什么OpenAI要做这样的调整呢？黄仁勋点出了其中的核心原因。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">那就是OpenAI面临“双重指数级增长压力”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">一方面是“用户增长指数”，AI越好用。应用场景越多。用户数量和使用频率就会指数级增长；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">另一方面是“计算增长指数”，就像我们前面聊的。每个用户每次和AI交互。因为“思考”的引入。需要的计算量也在指数级增长。这两个指数叠加在一起。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">意味着OpenAI的算力需求。会以“指数的指数”的速度增长。单靠租云服务已经满足不了了。所以必须同时推进“租云”和“自建”两条路。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_670.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">才能够确保算力供给跟得上需求。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">其实呢与Openai的紧密合作。背后。反映出的正是英伟达坚固的护城河。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">现在市场上有很多竞争对手。比如说AMD英特尔。还有一些公司在做ASIC芯片。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">主持人直接问黄仁勋。Nvidia的竞争护城河是在扩大还是缩小呢？</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">黄仁勋的回答则是。Nvidia真正的护城河。不是某一款芯片的性能优势。而是一种被称为“极限协同设计”（Extreme Co-Design）的系统级创新能力。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">黄仁勋解释了为什么Nvidia要搞“年度发布周期”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">过去芯片的更新是18-24个月一次。但是现在改成了每年一次。因为摩尔定律失效以后。晶体管的性能不再大幅提升。如果不能快速提升整体性能。AI生成Token的成本就会持续上升。而要持续降低Token成本。唯一的办法就是“系统级的创新”，这就是“极限协同设计”的由来。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">黄仁勋说。极限协同设计要求同步优化模型、算法、系统和芯片。让它们像一个整体一样工作。而不是各自为战。这和传统的“盒子内的创新”完全不同。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_740.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">过去是只想着把CPU做得更快。但是现在要同步升级构成AI数据中心的所有核心组件。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">具体来说。“极限协同设计”体现在三个层面。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在芯片层面。Nvidia会并行研发革命性的CPU、GPU、网络芯片和NVLink；。系统层面。会把这些芯片以最优化的方式整合起来。确保它们之间的数据传输效率最高。不会出现“某一个组件拖后腿”的情况；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">软件层面。会提供从底层驱动到上层应用库的完整软件栈。让开发者能轻松用上整个系统的能力。不用自己去解决硬件兼容、数据传输这些复杂问题。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">正是这种跨所有层面的协同设计。才让Nvidia从Hopper架构到Blackwell架构。在一年内实现了性能提升30倍的突破。这绝对不是靠单一芯片的技术进步能做到的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">其次。这种系统能力还体现在“规模化部署”的巨大挑战上。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">黄仁勋举了个例子。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">Elon Musk的xAI公司要部署Colossus 2的集群。需要用到50万个GPU。这不是简单地把50万个GPU堆在一起就行。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_808.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">要考虑电力供应、散热、数据传输延迟、系统稳定性等一系列的问题。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">任何一个环节出问题。整个集群都没法正常工作。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而客户之所以敢下数百亿美元的订单给Nvidia。就是因为Nvidia的架构经过了市场验证。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">能确保这么大规模的系统稳定运行。这种“经得住考验的规模化部署能力”，本身就是一道很高的信任壁垒。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">所以黄仁勋说。现在的竞争已经不是“我的芯片比你的快”，而是“我的整个AI工厂。比你的AI工厂效率更高”了。这种从“组件思维”到“系统思维”的跃迁。正是Nvidia能远超竞争对手的根本原因。毕竟。竞争对手可能能做出一款性能不错的芯片。但是要想做到“芯片、系统、软件”全链条的协同创新。还要能支撑几十万GPU的规模化部署。难度要大得多。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">黄仁勋甚至说。即使竞争对手把他们的ASIC芯片免费送给客户。客户还是应该选择Nvidia的系统。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_870.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">很多人听到这话会觉得不可思议。免费的芯片都不要？但是黄仁勋的逻辑。其实紧扣了数据中心的“现实约束”，那就是电力和空间都是有限的。黄仁勋解释说，数据中心建设中。土地、电力、建筑这些投入的成本非常高。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">当一家企业拿到宝贵的2吉瓦电力配额的时候。它的核心目标不再是“节省芯片成本”，而是“用这些电力创造最大的商业价值”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">他用简单的算术算了一笔账。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">如果每瓦性能。或者说每瓦能生成的Token数量。是竞争对手的两倍。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">那么客户用同样的数据中心。就能产生两倍的收入，谁不想要呢？</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">再具体一点。假设竞争对手的ASIC芯片。性能和Nvidia上一代的Hopper GPU差不多；。而Nvidia新一代的Blackwell GPU。性能是Hopper的30倍。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这意味着，在同样的电力消耗下。用Blackwell的客户能够获得30倍的潜在收入。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在这种情况下。为了节省一点芯片成本而放弃30倍的收入。这种机会成本显然“高得离谱”。任何理性的CFO。都会选择“每瓦性能”最高的解决方案。因为这直接决定了企业收入的上限。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_940.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">除此之外。黄仁勋还分析了ASIC的“生态定位”。他认为。ASIC适合那些“功能固定、市场规模有限”的领域。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如视频转码器、智能网卡等等。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这些任务的算法很少变。用ASIC能做到很高的效率。但是对于AI这种“工作负载多样且快速变化”的领域。ASIC的“专用性”反而成了致命弱点。因为AI需要处理的任务实在是太多了。聊天、写代码、生成图片视频、做数据分析、制定商业计划。等等等等。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而且底层算法还在不断演进。这就要求计算平台必须具备高度的“可编程性”，能够快速适配新任务、新算法。而这正是GPU和CUDA生态的核心优势。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这也能解释为什么谷歌虽然有自己的TPU。但同时也是Nvidia GPU的大客户。因为在一个复杂的计算集群里。既需要TPU这样的“专用辅助”芯片。也需要GPU这样的“通用主力”芯片。通过合理组合来实现整体最优。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_1001.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而Nvidia通过开放NVLink Fusion等接口。允许英特尔等公司的芯片接入自己的生态。这正是“平台化”和“生态化”思维的体现。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">非但不靠封闭来阻挡对手。反而是靠开放来扩大生态。让更多伙伴参与进来。一起把AI算力的市场做大。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在这场对话中。黄仁勋还花了大量篇幅讨论了全球人工智能竞赛。尤其是美国与中国的关系。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">黄仁勋首先提出了一个观点。AI基础设施已经成为和能源、通信同等重要的国家战略资源。所以全球范围内正在掀起“主权AI”（Sovereign AI）的建设浪潮。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">什么是主权AI？就是每个国家都需要拥有自己的AI基础设施。用自己的数据和文化训练AI模型。确保AI能服务于本国的特定需求。不管是工业生产、制造业升级。还是国家安全。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">黄仁勋认为。虽然各国会使用GPT、Gemini这些全球领先的模型。但是同时必须建立自己的主权AI能力。因为AI不仅是技术。还承载着文化、价值观和历史。一个国家不能把核心的智能需求。完全依赖于其他国家的技术。就像每个国家都会有自己的电网、通信网络一样。未来也会有自己的AI基础设施网络。而这为Nvidia等基础设施提供商。创造了全球性的全新市场。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_1083.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">关于美国的对华技术政策。黄仁勋提出了坦率的批评。他认为。美国采取“小院高墙”式的对华技术封锁。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如限制Nvidia向中国出口高端芯片。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">看起来是在遏制中国AI发展的做法。但是实际上不仅徒劳无功。反而更是一种危险的“单方面裁军”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">他指出了这种政策的两个主要后果。第一，会催生强大的竞争对手。把拥有95%市场份额的Nvidia排除出中国市场。相当于把整个中国市场拱手让给华为等本土企业。这些企业会在“没有强竞争”的环境下。会靠着“垄断利润”加速技术研发和产能扩张。最终成长为Nvidia在全球市场的强劲对手。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第二，严重低估了中国的能力。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">黄仁勋曾经警告说。外界普遍认为中国造不出高端芯片。或者技术上落后美国数年。这些想法都是“疯狂”的。实际上。中国拥有世界上最渴望成功、最勤奋的企业家。还有充满活力的内部竞争生态。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_1144.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">中国在芯片和AI领域和美国的技术差距。其实是以“纳秒”来计算的。不是大家想的“几年”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">当然这里的纳秒是打了引号的。意思是强调中美之间的差距很小。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">那么，正确的路径应该是什么呢？黄仁勋认为。让美国最优秀的企业在中国市场和本土企业直接竞争。才最符合美国的国家利益。这样做不仅能为美国企业创造经济价值。还能让美国通过技术影响力。在全球AI格局中保持话语权；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">更重要的是。竞争能倒逼美国科技企业不断创新。保持在技术最前沿。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">黄仁勋说道，一个自信、强大的国家。应该秉持‘放马过来’（Bring it on）的态度。相信自己的体系和人民能在竞争中胜出。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在谈到美国的核心优势时。黄仁勋的回答更是尖锐。他说。美国拥有一个世界上任何国家都没有的独特品牌声誉。那就是来到美国，实现美国梦。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">作为一个从中国台湾移民到美国。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_1204.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">从餐馆洗碗工成长为万亿市值公司CEO的亲历者。黄仁勋对“美国梦”的理解非常深刻。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这种“让每个人都有机会通过努力改变命运”的信念。是美国吸引全球顶尖人才的根本原因。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如过去几十年。全球最优秀的科学家、工程师、创业者都愿意去美国。因为那里有更好的机会、更开放的环境。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">但是黄仁勋警告说，近些年来。这个核心优势正在受到严重的挑战。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">他观察到一个非常危险的信号。顶尖中国AI研究者来美国的意愿。已经从三年前的90%骤降到现在的10%-15%。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这不是一个小变化。而是关乎美国未来的“生存危机”级别的早期预警。因为AI行业的竞争。本质上是人才的竞争，没有顶尖人才。再先进的技术也难以持续领先。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">所以黄仁勋呼吁美国政策制定者。必须谨慎区分“与中国竞争”和“对中国人强硬”这两个概念。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">“与中国竞争”是在技术、市场上的良性比拼。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_1266.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而“对中国人强硬”则是把优秀的中国人才拒之门外。这会摧毁美国最宝贵的资产。也就是美国作为全球人才灯塔的品牌形象。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">展望未来。黄仁勋认为人工智能将从根本上改变社会。他坚信。人工智能会带来巨大的生产力提升。而不是大规模的失业。那种认为AI会摧毁就业的观点。前提是“我们再也没有新的想法了”，但是他认为智力不是零和游戏。周围聪明的人和工具越多。能想到的新点子、能解决的新问题就越多。创造的岗位也会越多。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">每项工作都会改变，有些会消失。但是经济整体会增长。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在他看来。AI本身就是最伟大的均衡器。过去。一个人想利用计算机创造经济价值。至少得学习Python编程。现在，他们只需要学习人类语言。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">技术鸿沟正在被技术本身填平。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">对于未来的具体形态。他预言在未来五年内。人工智能与机器人技术的融合将成为现实。每个人都会像电影星球大战中一样。有自己的“R2-D2”机器人。成为生活中的伙伴和向导。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/--HbDQk2-jA_1334.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">云端的人工智能和实体世界的机器人将无处不在。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">生物学的复杂性将被揭示。每个人都将拥有自己的“数字孪生”，用于预测健康状况和疾病。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">面对这种指数级加速的变化。黄仁勋给出的建议很简单。那就是登上那列火车。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">不要试图去预测火车未来会到哪个站点。因为当它呈指数级加速的时候。任何预测都是徒劳的。唯一的策略就是趁现在它还相对较慢时跳上去。然后随着它一起经历指数级的旅程。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">也许，从芯片公司到AI基础设施公司。英伟达的进化本身就是登上这列火车的最好证明。而对于整个世界来说。这趟旅程才刚刚开始。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">好了，感谢观看本期视频。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">我们下期再见。</p>

<hr style="clear:both" />
=============
<p><a href="https://www.youtube.com/watch?v=--HbDQk2-jA">https://www.youtube.com/watch?v=--HbDQk2-jA</a></p><p>9月26日，知名播客BG2和英伟达的CEO黄仁勋进行了一场独家对话。这次访谈的标题叫做“NVIDIA：OpenAI、计算的未来和美国梦”，时长超过100分钟，堪称黄仁勋近期信息密度最高、干货最足的一次分享。在这场对话中，黄仁勋系统性地解释了华尔街与硅谷之间存在的巨大认知分歧，并且详细拆解了英伟达看似坚不可摧的商业护城河，以及他对全球人工智能竞赛、大国博弈和未来社会形态的完整思考，今天我们就来回顾一下这场访谈的内容。</p><p><a href="https://youtu.be/pE6sw_E9Gh0?si=L41UzPmgS_lP5i43">https://youtu.be/pE6sw_E9Gh0?si=L41UzPmgS_lP5i43</a></p>]]></content:encoded>
      <itunes:image href="https://i.ytimg.com/vi/--HbDQk2-jA/hqdefault.jpg"/>
      <pubDate>2025-10-05T09:22:21.000Z</pubDate>
    </item><item>
      <title><![CDATA[【人工智能】Lora无悔 | Thinking Machines最新研究 | John Schulman | 全量微调 | 什么时候可以放心用Lora | MLP层 | 学习率LR | eNTK理论]]></title>
      <link>https://www.youtube.com/watch?v=MlqXL6o--2M</link>
      <itunes:title><![CDATA[【人工智能】Lora无悔 | Thinking Machines最新研究 | John Schulman | 全量微调 | 什么时候可以放心用Lora | MLP层 | 学习率LR | eNTK理论]]></itunes:title>
      <itunes:author><![CDATA[最佳拍档]]></itunes:author>
      <itunes:summary>
        <![CDATA[<hr style="clear:both" />

<p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://img.youtube.com/vi/MlqXL6o--2M/maxresdefault.jpg" /></p><p><a href="https://www.youtube.com/watch?v=MlqXL6o--2M">https://www.youtube.com/watch?v=MlqXL6o--2M</a></p><h1>值得閱讀的理由</h1> <ul> <li>深入理解<strong>LoRA</strong>與<strong>全量微調（FullFT）</strong>在資源消耗與性能表現之間的平衡，為大模型微調提供決策依據。</li> <li>掌握<strong>約翰·舒爾曼（John Schulman）</strong>團隊提出的關鍵條件和最佳實踐，有效提升LoRA的微調效果。</li> <li>探索LoRA在監督學習與強化學習場景下的應用差異，以及未來大模型微調研究的開放性問題。</li> </ul> <hr /> <h1>摘要</h1> <p>本影片由影片的講者深入探討了當今大模型微調領域中一個核心問題：究竟該選擇<strong>低秩適應（LoRA）</strong>還是<strong>全量微調（FullFT）</strong>？這個問題的本質在於<strong>「資源」</strong>與<strong>「效果」</strong>的權衡。影片指出，儘管全量微調能最大化模型性能，但其對硬體資源的龐大需求（如儲存原始權重、梯度、優化器動量，且需使用FP32精度）使得一般團隊難以承受。相較之下，LoRA作為一種參數高效微調的主流方法，以其節省記憶體和算力的優勢吸引了廣泛關注，但其性能能否媲美全量微調一直是業界普遍的疑慮。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_90.jpg" /></p> <p>影片接著介紹了由<strong>約翰·舒爾曼（John Schulman）及其團隊</strong>發表的深度分析文章《LoRA無悔（LoRA Without Regret）》，該文章不僅回應了LoRA能否匹敵全量微調的疑問，更提出了具體的操作條件和超參數設定建議。影片強調，理解LoRA的重要性源於當前主流大型語言模型（如Llama 3、Qwen3）龐大的參數規模和預訓練資料量。在模型的「訓練後處理」階段，由於資料集規模變小且關注領域變窄，全量微調顯得過於浪費，因此催生了<strong>參數高效微調（PEFT）</strong>的發展，而LoRA正是其中最成熟的方法。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_180.jpg" /></p> <h2>LoRA的核心機制與優勢</h2> <p>影片詳細解釋了LoRA的核心思路：它不直接修改原模型的權重矩陣W，而是添加一個「低秩增量項」<strong>γBA</strong>。這裡的B和A是兩個低秩矩陣，它們的參數數量遠少於原始權重矩陣W，卻能「模擬」出原模型權重的更新效果。這種機制不僅實現了<strong>參數高效性</strong>，還帶來了多項實際優勢。首先是<strong>「多租戶服務」</strong>，由於LoRA只訓練輕量級的「適配器」，一個推理伺服器能夠同時載入多個適配器以處理不同任務，顯著節省硬體成本。其次是<strong>「訓練佈局優化」</strong>，LoRA更新的參數少，記憶體佔用小，使得訓練所需的硬體資源大幅降低，普通團隊也能用少量GPU進行訓練。最後是<strong>「載入傳輸方便」</strong>，LoRA適配器檔案體積小（僅幾MB到幾十MB），便於機器間傳輸和模型載入，提高了營運彈性。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_300.jpg" /></p> <h2>關鍵實驗發現與「低遺憾區間」</h2> <p>影片闡述了約翰·舒爾曼團隊實驗的核心目標：回答LoRA能否匹配全量微調的性能，以及需要滿足哪些條件。他們採取的實驗設計具有兩個顯著特點：一是<strong>「不聚焦特定任務，而是研究通用關係」</strong>，探討「訓練集大小」與「LoRA參數數量」之間的通用規律；二是採用<strong>「對數損失（log loss）評估」</strong>，能客觀反映模型的預測誤差和訓練表現。透過嚴謹的實驗設計，團隊得出了五個關鍵發現：</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_450.jpg" /></p> <p>1.  在中小規模的指令微調或推理資料集上，只要不超過LoRA的容量，<strong>LoRA與全量微調的性能幾乎完全一致</strong>。 2.  若資料集規模超出LoRA容量，LoRA會表現較差，但並非無法下降，而是<strong>「訓練效率變慢」</strong>。 3.  LoRA對<strong>「批量大小（batch size）」</strong>的容忍度低於全量微調，批次過大會導致LoRA性能明顯下降，且增大LoRA的秩也無法解決此問題。 4.  LoRA必須應用於模型<strong>「所有層」</strong>，尤其是<strong>MLP層或MoE層</strong>，效果才最佳；僅在注意力層使用LoRA的性能表現不佳。 5.  在<strong>強化學習（RL）</strong>場景中，即使使用<strong>極低秩的LoRA</strong>也能與全量微調性能一致，這歸因於強化學習每輪所需的資訊量較少（O(1)比特）。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_705.jpg" /></p> <p>基於這些發現，研究團隊提出了<strong>「低遺憾區間（low-regret regime）」</strong>的概念：只要LoRA的參數容量能覆蓋資料集所需的資訊，並且應用到所有層，就能與全量微調性能相當。這意味著在大多數企業的訓練後處理場景中，LoRA完全可以替代全量微調。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_750.jpg" /></p> <h2>實驗細節剖析</h2> <p>影片進一步深入探討了約翰·舒爾曼團隊的實驗細節，為實踐者提供了寶貴的參考。他們測試了從<strong>秩1到512</strong>的三個數量級LoRA秩，並對每個實驗條件進行了<strong>「學習率掃描」</strong>以找到最佳值，同時採用<strong>「恆定學習率」</strong>排除調度干擾。實驗模型涵蓋了<strong>Llama 3系列</strong>和<strong>Qwen3系列</strong>，以及<strong>混合專家（MoE）模型</strong>。資料集則選用了監督學習的<strong>Tulu3</strong>和<strong>OpenThoughts3</strong>，以及強化學習的<strong>MATH</strong>和<strong>GSM8K</strong>，確保了實驗的廣泛性和代表性。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_880.jpg" /></p> <p>關於<strong>LoRA秩的影響</strong>，高秩LoRA（如256、512）與全量微調的學習曲線幾乎重合，損失呈現「隨步驟的對數線性下降」；低秩LoRA（如1、4）在訓練初期尚可，但達到閾值後便會因「容量耗盡」而偏離最佳損失曲線。在<strong>學習率</strong>方面，研究發現LoRA的最佳學習率約為全量微調的10倍，且不同秩的LoRA之間最佳學習率差異不大，這為設定LoRA學習率提供了便利。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_990.jpg" /></p> <p><strong>批量大小效應</strong>實驗揭示，當批量大小較小（如32）時，LoRA與全量微調性能差距微小；但當批量大小增大（如256、512）時，LoRA的損失會顯著高於全量微調，且這種差距與LoRA的秩無關。這歸因於LoRA的「矩陣乘積參數化（BA）」與原權重矩陣（W）的優化動態不同，大批量會放大這種差異。幸運的是，在實際應用中，只要避免使用過大的批量，此問題便可緩解。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_1070.jpg" /></p> <p>影片特別強調了<strong>LoRA應該應用到哪些層</strong>的實驗結果。研究團隊通過三組對比實驗（僅注意力層、僅MLP層、所有層）發現，僅在注意力層使用LoRA的性能最差，而<strong>MLP層才是LoRA發揮作用的核心</strong>；將LoRA應用於所有層與僅MLP層的性能幾乎一致。這個發現與<strong>經驗神經正切核（eNTK）理論</strong>相符：參數越多的層對核函數影響越大，應用於所有層能使LoRA的eNTK與全量微調的eNTK保持一致，從而實現相似的學習動態。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_20.jpg" /></p> <h2>結論與深層意義</h2> <p>影片總結了該研究的深層意義。首先，LoRA匹配全量微調的核心條件有二：一是<strong>必須應用到所有層（尤其MLP/MoE層）</strong>，確保eNTK的一致性；二是<strong>LoRA的容量需能覆蓋資料集所需的資訊</strong>。其次，<strong>監督學習與強化學習對LoRA容量的需求存在顯著差異</strong>：監督學習需要更多資訊，適合高秩LoRA；強化學習每輪所需資訊少，低秩LoRA便足夠。這能幫助開發者在強化學習任務中節省資源，無需追求高秩。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_22.jpg" /></p> <p>研究還指出LoRA的<strong>計算效率優勢</strong>，每輪訓練的FLOPs約為全量微調的2/3，意味著在相同硬體下LoRA能多訓練50%的樣本。儘管如此，研究也提出了一些<strong>「開放問題」</strong>，例如如何更精準地預測LoRA性能、LoRA最佳學習率是全量微調10倍的理論解釋、LoRA變體的表現，以及LoE層LoRA如何與張量並行等技術結合以適配更大模型等，這些都是未來值得關注的研究方向。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_24.jpg" /></p> <p>最後，約翰·舒爾曼團隊的研究目標不僅是為了節省資源，更是為了<strong>「讓大模型的微調能力更易獲取」</strong>，使普通團隊也能利用大模型解決具體問題。透過對LoRA的深入研究，他們也深化了對「模型容量」、「資料集複雜度」、「樣本效率」等基礎問題的理解，展現了AI研究中實用技術背後蘊藏的核心原理新認知。</p> <hr /><hr />================================================</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">大家好，这里是最佳拍档，我是大飞。现在微调大模型。到底应该用LoRA还是全量微调呢？其实这个问题的核心。本质是“资源”和“效果”的平衡。全量微调（FullFT）虽然能把模型性能拉满。但是太“吃资源”了。一个万亿参数的模型。全量微调时不仅要存原权重。还要存梯度、优化器动量。而且这些数据得用FP32精度。比推理时的BF16多占一倍空间。普通团队根本扛不住硬件成本。而LoRA作为参数高效微调的主流方法。确实省内存、省算力。但是大家总是感觉没底。它的效果到底能不能跟全量微调比？会不会调了半天，性能差一截呢？而Thinking Machines Lab最近发布的一篇博客。恰恰把这个问题给讲透了。文章的标题是《LoRA无悔（LoRA Without Regret）》，作者是约翰·舒尔曼（John Schulman）和他的团队。文章不仅回答了“LoRA能不能媲美全量微调”的问题。还给出了具体的操作条件。甚至连超参数怎么设都讲清楚了。不管你是想入门LoRA。还是已经在落地中遇到问题。这篇内容都值得你仔细阅读。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_75.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先，我们得先搞清楚。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">为什么现在大家这么需要LoRA？这还得从大模型的现状说起。现在主流的语言模型。比如Llama 3、Qwen3。参数都在上万亿。预训练时用了数万亿个Token。这么大的规模。就是为了让模型学到人类书面知识里的所有模式。基础性能才能上去。但是到了“训练后处理”阶段。情况就发生了变化。不仅数据集变小了。关注的领域也变窄了。这时候如果还用全量微调。这就像用一辆卡车去拉一颗鸡蛋。太浪费了。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">就是因为这个“浪费”的问题。参数高效微调（PEFT）才发展起来。而低秩适应LoRA就是其中最成熟的方法。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">LoRA的核心思路很巧妙。它不直接改原模型的权重矩阵W。而是给W加了一个“低秩增量项”，公式是W' = W + γBA。这里的W'是微调后的权重。B和A是两个低秩矩阵。它们加起来的参数数量。比原矩阵W少得多。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_141.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">γ是个常数缩放因子。用来调整增量项的大小。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">简单说，LoRA就是用两个小矩阵。“模拟”出原模型权重的更新效果。这样既不用动原模型的海量参数。又能让模型适配新任务。这就是它“参数高效”的关键。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而且LoRA不止省参数。还有三个很实际的优势。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第一个是“多租户服务（Multi-tenant serving）”，因为LoRA只训练A和B这两个“适配器”，原模型权重不动。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">所以一个推理服务器可以在内存里存多个适配器。对应不同的任务。比如一个适配器处理客服对话。另一个处理产品摘要生成。还能批量处理这些请求。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">像陈（Chen）等人2023年提出的Punica框架。还有现在常用的vLLM、SGLang这些推理引擎。都已经支持这个功能。对企业来说能省不少硬件钱。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第二个优势是“训练布局优化”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">全量微调的时候。硬件布局得专门设计。你想，除了原权重。还要存梯度、优化器动量，精度还高。需要的加速器数量往往是推理时的10倍以上。但LoRA要更新的参数少。占用内存也少。训练时的硬件布局只比推理时稍微大一点。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_220.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">普通团队用几台GPU就能跑。门槛低多了。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第三个优势是“加载传输方便”，适配器矩阵的参数少。文件体积小。比如一个秩为128的LoRA适配器。可能就几MB或几十MB。不管是在机器间传。还是加载到模型里。都比传整个模型快得多，运维也灵活。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">正是这些优势。LoRA从2021年胡（Hu）等人发表的第一篇论文（《LoRA:。Low-Rank Adaptation of Large Language Models》）后。就越来越火。但是问题也来了，现有的研究里。LoRA和全量微调的性能对比一直不明确。大家公认的是。在“类似预训练”的场景里。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如数据集特别大。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">超过LoRA的参数存储极限的时候。LoRA性能会差一些；。但是在训练后处理的常见数据集规模下。LoRA虽然容量足够存下关键信息。可是没人能保证它的“样本效率”和“计算效率”，能跟全量微调相比。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">所以约翰·舒尔曼团队的实验。核心就是回答了。LoRA到底能不能匹配全量微调的性能？如果能，需要满足哪些条件呢？</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_289.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">他们的实验结论很明确。只要把几个关键细节做对。LoRA的样本效率和最终性能。就能和全量微调完全一致。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">那这些“关键细节”到底是什么？我们得先从他们的实验设计说起。因为好的实验设计。是结论可信的基础。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">和之前的LoRA研究相比。他们的实验有两个很重要的特点。第一个是“不盯着具体任务。而是研究通用关系”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">很多研究只会选一两个数据集。比如只测情感分析。或者只测文本摘要，但是他们不一样。专门研究“训练集大小”和“LoRA参数数量”之间的通用规律。这样得出的结论。不管你是做医疗还是教育任务。都能参考。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第二个是采用了“对数损失（log loss）评估”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">之前很多研究采用的是“基于采样的评估”，比如让模型生成文本，再人工打分。但是这种方法主观性强。不同任务里标准还不一样。而对数损失能直接反映模型的预测误差。而且能够清晰地看到不同训练步骤、不同参数设置下的模型表现规律。通用性强得多。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_357.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">就是靠着这样的实验设计。他们得出了五个关键发现。这些发现也是LoRA能够媲美全量微调的核心条件。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第一个发现。在中小规模的指令微调或者推理数据集上。LoRA和全量微调的性能完全一样。比如他们用的Tulu3和OpenThoughts3数据集。只要规模没超过LoRA的容量。两者的损失曲线、收敛速度都几乎重合。这说明在大多数企业的落地场景里。LoRA完全够用。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第二个发现。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">如果数据集规模超过了LoRA的容量。LoRA就会比全量微调差。但不是“损失降到一定程度就降不下去”了。而是“训练效率变慢”了。具体会慢多少。要取决于LoRA的参数容量和数据集大小的比例。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如一个秩为32的LoRA。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">处理100万Token的数据集没问题。但是如果数据集涨到1亿个Token。它的训练效率就会明显下降。损失下降速度变慢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_419.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第三个发现。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">LoRA对“批量大小（batch size）”的容忍度比全量微调低。如果批量 size 太大。超过某个阈值后。LoRA的性能损失就会比全量微调大得多。而且这个问题。就算增大LoRA的秩也解决不了。因为这是LoRA“矩阵乘积参数化（BA）”的固有属性。它的优化动态和原权重矩阵（W）不一样。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">批量一大，这种动态差异就会被放大。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第四个发现。LoRA一定要应用到模型的所有层。尤其是MLP层或MoE层，效果才好。之前很多研究受到第一篇LoRA论文的影响。只把LoRA用到注意力层。但是约翰·舒尔曼团队发现。仅在注意力层使用LoRA的性能很差。而MLP层才是LoRA发挥作用的关键。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比德曼（Biderman）等人2024年的研究（《LoRA Learns Less and Forgets Less》）也得出了类似结论。进一步验证了这个发现。第五个发现，在强化学习的场景里。就算用很低秩的LoRA。也能和全量微调性能一致。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_482.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">他们用数学推理任务做实验。比如让模型去解初中数学题。以答案正确性为奖励，结果发现。哪怕LoRA的秩只有1。最终的准确率也和全量微调一样。这背后有个信息论的解释。那就是监督学习里。每一轮训练能给模型提供“O(Token数量)”个比特的信息。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如一句话有100个Token。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">就能提供100比特左右的信息；。但是强化学习不一样。政策梯度算法的学习。靠的是“优势函数（Advantage Function）”，每一轮只能提供“O(1)”比特的信息。简单来说。就是强化学习需要学的信息少。就算是低秩LoRA，容量也完全够。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">基于这些发现。他们提出了一个“低遗憾区间（low-regret regime）”的概念。只要LoRA的参数容量能覆盖数据集的信息需求。并且应用到所有层。就能和全量微调性能相当。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而这个区间。刚好覆盖了大多数企业的训练后处理场景。这也意味着，LoRA在很多实际应用里。完全可以替代全量微调。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_547.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">当然，光有结论不够。我们还得知道这些结论是怎么来的。也就是他们的实验细节。这能帮我们在自己做实验的时候少走弯路。我们挑几个部分来重点讲一下。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先，他们的实验设置非常严谨。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先是LoRA的秩。覆盖了1到512三个数量级。从最低的秩1。到能覆盖大部分场景的秩128、256。再到接近全量微调的秩512。这样能全面看到秩对性能的影响。然后是学习率（LR）。为了避免“因为学习率选得不好。导致LoRA性能差”的情况出现。他们对每个实验条件都做了“学习率扫描”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如对秩32的LoRA。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">测试0.0001到0.01的不同学习率。找到最佳值；。而且采用了“恒定学习率”，没有热身或冷却阶段。这样能够排除学习率调度的干扰。更准确对比LoRA和全量微调的差异。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_608.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">模型方面，他们用了两类主流模型。一类是Llama 3系列。另一类是Qwen3系列。还包括了混合专家（MoE）模型。因为MoE模型的激活效率高。现在很多大模型都用这种架构。所以加入MoE的实验，能让结论更全面。数据集方面。监督学习用了Tulu3和OpenThoughts3。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">Tulu3是艾维森（Ivison）等人在2024年提出的。侧重“指令遵循”，比如让模型根据用户指令生成邮件、写代码；。OpenThoughts3是古哈（Guha）等人在2025年提出的。侧重“推理”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如数学题、逻辑推理题。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这两个数据集在任务类型、数据结构上差异很大。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">强化学习实验则用了MATH数据集和GSM8K数据集。都是数学推理任务。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">以答案正确与否作为奖励信号。以便专注测试强化学习场景下的LoRA性能。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">先看“LoRA秩”的影响实验。他们在Tulu3和OpenThoughts3上训练了一个epoch。对每个数据集、每个模型大小。都扫描了不同的LoRA秩和学习率。结果发现，高秩LoRA，比如256、512。和全量微调的学习曲线几乎重合。损失都是“随步骤的对数线性下降”，也就是说。每训练10倍的步骤。损失就会按固定比例下降。这和全量微调的规律完全一样。而低秩LoRA，比如1、4。在训练初期还能跟上。但是到了某个步骤阈值后。就会“偏离最小损失曲线”。这就是之前说的“容量耗尽”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_707.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">低秩矩阵存不下更多的任务信息。所以学习速度变慢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">更重要的是“学习率”的发现。他们绘制了“学习率和最终损失”的曲线。发现高秩LoRA的最小损失和全量微调几乎一样。但是最佳学习率是全量微调的10倍。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如全量微调的最佳学习率是0.001。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">LoRA就需要0.01。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比德曼（Biderman）等人2024年的研究也发现了类似的10倍比例。这说明这个规律不是偶然的。而是LoRA的固有属性。而且不同秩的LoRA。最佳学习率很接近。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如秩4和秩512的最佳学习率差异不到2倍。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这为我们设置LoRA学习率提供了很大便利。不用频繁调整。按全量微调的10倍开始试就行。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">再看“批量大小效应”的实验。他们用了OpenThoughts3里一个1万样本的子集。测试不同批量大小的影响。结果很明显，当批量大小比较小。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_768.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如32的时候。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">LoRA和全量微调的性能差距很小。而且随着训练推进，差距还会缩小；。但当批量大小变大。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如256、512的时候。LoRA的损失会明显比全量微调高。而且这个差距会一直存在。不会随训练步骤消失。更关键的是。这个差距和LoRA的秩无关。就算用秩512的LoRA。大批次下还是比全量微调差。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">他们分析。这是因为LoRA的“矩阵乘积（BA）”参数化。和原权重矩阵（W）的优化动态不同。批量一大。原矩阵能更好地利用批次信息更新。而BA矩阵的更新效率会下降。导致性能差距。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">不过好在，不管是LoRA还是全量微调。都是“小批量下性能更好”，所以实际应用里。只要别用太大的批量。这个问题就能够缓解。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">接下来。我们来重点说一说跟前面第4点发现。“LoRA该应用到哪些层”有关的实验过程。这是很多人调参时容易踩坑的地方。之前很多人习惯只把LoRA用到注意力层。觉得注意力层负责“理解上下文”，所以是关键，但是实验结果刚好相反。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_838.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">约翰·舒尔曼团队做了三组对比实验。第一组只在注意力层用LoRA。第二组只在MLP层用LoRA。第三组在所有层。也就是注意力+MLP+MoE都用了LoRA。结果发现。仅注意力层的LoRA性能最差。就算用更高的秩。参数数量和仅MLP层的秩128差不多。性能还是差一截。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如在Llama-3.1-8B模型上，仅MLP层。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">秩128，0.24B参数的损失。比仅注意力层。秩256，0.25B参数低了0.15左右。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">要知道，在语言模型里。损失降低0.1已经是很明显的提升了。而“所有层用LoRA”和“仅MLP层用LoRA”的性能几乎一样。这说明MLP层才是LoRA发挥作用的核心。注意力层加不加LoRA。对最终性能影响不大。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">他们还在MoE模型上做了实验。对Qwen3-30B-A3B-Base的每个专家都单独训练LoRA。秩设为“总秩除以活跃专家数量”，这样能保证MoE层的LoRA参数比例和其他层一致。结果和稠密模型一样。仅MLP层的LoRA优于仅注意力层。所有层和仅MLP层相当。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_918.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">为什么会这样？这就要用到马拉迪（Malladi）等人在2022年提出的“经验神经正切核（eNTK）”理论了。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">eNTK是用来描述模型微调初期学习动态的工具。它的核心是“梯度的点积”。比如对每个Token的预测。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">梯度g_i是损失对参数的偏导。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">eNTK的核函数K(i。j)就是g_i和g_j的点积。这个核函数决定了模型怎么“学习”数据里的模式。而参数越多的层。对核函数的影响越大。马拉迪等人的研究指出。当LoRA应用到所有层的时候。它的eNTK和全量微调的eNTK几乎一样。所以学习动态也相似；。但是如果只应用到注意力层。而忽略了参数更多的MLP层。eNTK就会和全量微调有明显差异。学习速度自然慢下来。这也解释了为什么仅注意力层的LoRA效果差。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">聊完实验。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">我们最后再来总结一下这篇研究的主要结论和背后的深层意义。它不止告诉了我们LoRA怎么用。还帮我们深化了对大模型微调的理解。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_986.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先是“LoRA匹配全量微调的两个核心条件”，一是要应用到所有层。尤其是MLP/MoE层。这样才能保证eNTK和全量微调的一致；。二是LoRA的容量要能覆盖数据集的信息需求。也就是“非容量约束”。只要满足这两个条件。LoRA就能和全量微调性能相当。这为我们选择微调方法提供了明确的判断标准。如果你的数据集是中小规模。模型有MLP层。就用LoRA；。如果数据集特别大。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如接近预训练规模。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">再考虑全量微调。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">其次是“监督学习和RL的容量需求的差异”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">监督学习需要的信息多。每个token 1比特。所以需要更高秩的LoRA；。强化学习需要的信息少，每轮次1比特。低秩LoRA就够。这个差异能帮我们节省资源。也就是做强化学习任务时。不用追求高秩。秩1、8就够，参数更少，训练更快。然后是“LoRA的计算效率优势”，他们算了一笔账。LoRA每轮训练的FLOPs。是全量微调的2/3左右。具体怎么算的大家可以去看一下这一段。这里我们就不一一推导了。简单来说，对于一个N×N的权重矩阵W。LoRA的总FLOPs是2N²。全量微调是3N²，所以比例就是2/3。这意味着，同样的硬件。LoRA能比全量微调多训练50%的样本。计算效率更高。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_1079.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">当然，研究也提出了一些“开放问题”，这些也是未来值得关注的方向。比如怎么更精准地预测LoRA的性能。不用每次都做实验；。为什么LoRA的最佳学习率是全量微调的10倍。目前还没有完整的理论解释；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">LoRA的变体在这种评估方法下表现如何；。还有MoE层的LoRA。怎么和张量并行、专家并行这些技术结合。适配更大的模型，等等等等。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">最后，约翰·舒尔曼团队在结语里提到。他们研究LoRA的目标。不只是为了省资源。更是为了“让大模型的微调能力更易获取”，毕竟不是每个团队都有能力做全量微调。但是LoRA能让普通团队也能用大模型来解决具体的问题。而且通过研究LoRA。他们也深化了对“模型容量”、“数据集复杂度”、“样本效率”这些基础问题的理解。这其实也是AI研究的魅力。一个实用的技术。背后往往藏着对核心原理的新认知。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_1140.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">好了，今天的内容就到这里。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">如果你在做LoRA相关的工作。希望今天的拆解能帮你少走弯路；。如果你只是感兴趣。也希望能够帮助你增加对LoRA的了解。感谢大家观看本期视频。我们下期再见。</p>

<hr style="clear:both" />
=============
<p><a href="https://www.youtube.com/watch?v=MlqXL6o--2M">https://www.youtube.com/watch?v=MlqXL6o--2M</a></p><p>Thinking Machines Lab最近发布的一篇博客，文章的标题是《LoRA无悔（LoRA Without Regret）》，作者是约翰·舒尔曼（John Schulman）和他的团队。文章不仅回答了“LoRA能不能媲美全量微调”的问题，还给出了具体的操作条件，甚至连超参数怎么设都讲清楚了。不管你是想入门LoRA，还是已经在落地中遇到问题，这篇内容都值得你仔细阅读。</p><p><a href="https://thinkingmachines.ai/blog/lora/">https://thinkingmachines.ai/blog/lora/</a></p>]]>
      </itunes:summary>
      <description>
        <![CDATA[<hr style="clear:both" />

<p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://img.youtube.com/vi/MlqXL6o--2M/maxresdefault.jpg" /></p><p><a href="https://www.youtube.com/watch?v=MlqXL6o--2M">https://www.youtube.com/watch?v=MlqXL6o--2M</a></p><h1>值得閱讀的理由</h1> <ul> <li>深入理解<strong>LoRA</strong>與<strong>全量微調（FullFT）</strong>在資源消耗與性能表現之間的平衡，為大模型微調提供決策依據。</li> <li>掌握<strong>約翰·舒爾曼（John Schulman）</strong>團隊提出的關鍵條件和最佳實踐，有效提升LoRA的微調效果。</li> <li>探索LoRA在監督學習與強化學習場景下的應用差異，以及未來大模型微調研究的開放性問題。</li> </ul> <hr /> <h1>摘要</h1> <p>本影片由影片的講者深入探討了當今大模型微調領域中一個核心問題：究竟該選擇<strong>低秩適應（LoRA）</strong>還是<strong>全量微調（FullFT）</strong>？這個問題的本質在於<strong>「資源」</strong>與<strong>「效果」</strong>的權衡。影片指出，儘管全量微調能最大化模型性能，但其對硬體資源的龐大需求（如儲存原始權重、梯度、優化器動量，且需使用FP32精度）使得一般團隊難以承受。相較之下，LoRA作為一種參數高效微調的主流方法，以其節省記憶體和算力的優勢吸引了廣泛關注，但其性能能否媲美全量微調一直是業界普遍的疑慮。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_90.jpg" /></p> <p>影片接著介紹了由<strong>約翰·舒爾曼（John Schulman）及其團隊</strong>發表的深度分析文章《LoRA無悔（LoRA Without Regret）》，該文章不僅回應了LoRA能否匹敵全量微調的疑問，更提出了具體的操作條件和超參數設定建議。影片強調，理解LoRA的重要性源於當前主流大型語言模型（如Llama 3、Qwen3）龐大的參數規模和預訓練資料量。在模型的「訓練後處理」階段，由於資料集規模變小且關注領域變窄，全量微調顯得過於浪費，因此催生了<strong>參數高效微調（PEFT）</strong>的發展，而LoRA正是其中最成熟的方法。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_180.jpg" /></p> <h2>LoRA的核心機制與優勢</h2> <p>影片詳細解釋了LoRA的核心思路：它不直接修改原模型的權重矩陣W，而是添加一個「低秩增量項」<strong>γBA</strong>。這裡的B和A是兩個低秩矩陣，它們的參數數量遠少於原始權重矩陣W，卻能「模擬」出原模型權重的更新效果。這種機制不僅實現了<strong>參數高效性</strong>，還帶來了多項實際優勢。首先是<strong>「多租戶服務」</strong>，由於LoRA只訓練輕量級的「適配器」，一個推理伺服器能夠同時載入多個適配器以處理不同任務，顯著節省硬體成本。其次是<strong>「訓練佈局優化」</strong>，LoRA更新的參數少，記憶體佔用小，使得訓練所需的硬體資源大幅降低，普通團隊也能用少量GPU進行訓練。最後是<strong>「載入傳輸方便」</strong>，LoRA適配器檔案體積小（僅幾MB到幾十MB），便於機器間傳輸和模型載入，提高了營運彈性。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_300.jpg" /></p> <h2>關鍵實驗發現與「低遺憾區間」</h2> <p>影片闡述了約翰·舒爾曼團隊實驗的核心目標：回答LoRA能否匹配全量微調的性能，以及需要滿足哪些條件。他們採取的實驗設計具有兩個顯著特點：一是<strong>「不聚焦特定任務，而是研究通用關係」</strong>，探討「訓練集大小」與「LoRA參數數量」之間的通用規律；二是採用<strong>「對數損失（log loss）評估」</strong>，能客觀反映模型的預測誤差和訓練表現。透過嚴謹的實驗設計，團隊得出了五個關鍵發現：</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_450.jpg" /></p> <p>1.  在中小規模的指令微調或推理資料集上，只要不超過LoRA的容量，<strong>LoRA與全量微調的性能幾乎完全一致</strong>。 2.  若資料集規模超出LoRA容量，LoRA會表現較差，但並非無法下降，而是<strong>「訓練效率變慢」</strong>。 3.  LoRA對<strong>「批量大小（batch size）」</strong>的容忍度低於全量微調，批次過大會導致LoRA性能明顯下降，且增大LoRA的秩也無法解決此問題。 4.  LoRA必須應用於模型<strong>「所有層」</strong>，尤其是<strong>MLP層或MoE層</strong>，效果才最佳；僅在注意力層使用LoRA的性能表現不佳。 5.  在<strong>強化學習（RL）</strong>場景中，即使使用<strong>極低秩的LoRA</strong>也能與全量微調性能一致，這歸因於強化學習每輪所需的資訊量較少（O(1)比特）。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_705.jpg" /></p> <p>基於這些發現，研究團隊提出了<strong>「低遺憾區間（low-regret regime）」</strong>的概念：只要LoRA的參數容量能覆蓋資料集所需的資訊，並且應用到所有層，就能與全量微調性能相當。這意味著在大多數企業的訓練後處理場景中，LoRA完全可以替代全量微調。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_750.jpg" /></p> <h2>實驗細節剖析</h2> <p>影片進一步深入探討了約翰·舒爾曼團隊的實驗細節，為實踐者提供了寶貴的參考。他們測試了從<strong>秩1到512</strong>的三個數量級LoRA秩，並對每個實驗條件進行了<strong>「學習率掃描」</strong>以找到最佳值，同時採用<strong>「恆定學習率」</strong>排除調度干擾。實驗模型涵蓋了<strong>Llama 3系列</strong>和<strong>Qwen3系列</strong>，以及<strong>混合專家（MoE）模型</strong>。資料集則選用了監督學習的<strong>Tulu3</strong>和<strong>OpenThoughts3</strong>，以及強化學習的<strong>MATH</strong>和<strong>GSM8K</strong>，確保了實驗的廣泛性和代表性。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_880.jpg" /></p> <p>關於<strong>LoRA秩的影響</strong>，高秩LoRA（如256、512）與全量微調的學習曲線幾乎重合，損失呈現「隨步驟的對數線性下降」；低秩LoRA（如1、4）在訓練初期尚可，但達到閾值後便會因「容量耗盡」而偏離最佳損失曲線。在<strong>學習率</strong>方面，研究發現LoRA的最佳學習率約為全量微調的10倍，且不同秩的LoRA之間最佳學習率差異不大，這為設定LoRA學習率提供了便利。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_990.jpg" /></p> <p><strong>批量大小效應</strong>實驗揭示，當批量大小較小（如32）時，LoRA與全量微調性能差距微小；但當批量大小增大（如256、512）時，LoRA的損失會顯著高於全量微調，且這種差距與LoRA的秩無關。這歸因於LoRA的「矩陣乘積參數化（BA）」與原權重矩陣（W）的優化動態不同，大批量會放大這種差異。幸運的是，在實際應用中，只要避免使用過大的批量，此問題便可緩解。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_1070.jpg" /></p> <p>影片特別強調了<strong>LoRA應該應用到哪些層</strong>的實驗結果。研究團隊通過三組對比實驗（僅注意力層、僅MLP層、所有層）發現，僅在注意力層使用LoRA的性能最差，而<strong>MLP層才是LoRA發揮作用的核心</strong>；將LoRA應用於所有層與僅MLP層的性能幾乎一致。這個發現與<strong>經驗神經正切核（eNTK）理論</strong>相符：參數越多的層對核函數影響越大，應用於所有層能使LoRA的eNTK與全量微調的eNTK保持一致，從而實現相似的學習動態。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_20.jpg" /></p> <h2>結論與深層意義</h2> <p>影片總結了該研究的深層意義。首先，LoRA匹配全量微調的核心條件有二：一是<strong>必須應用到所有層（尤其MLP/MoE層）</strong>，確保eNTK的一致性；二是<strong>LoRA的容量需能覆蓋資料集所需的資訊</strong>。其次，<strong>監督學習與強化學習對LoRA容量的需求存在顯著差異</strong>：監督學習需要更多資訊，適合高秩LoRA；強化學習每輪所需資訊少，低秩LoRA便足夠。這能幫助開發者在強化學習任務中節省資源，無需追求高秩。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_22.jpg" /></p> <p>研究還指出LoRA的<strong>計算效率優勢</strong>，每輪訓練的FLOPs約為全量微調的2/3，意味著在相同硬體下LoRA能多訓練50%的樣本。儘管如此，研究也提出了一些<strong>「開放問題」</strong>，例如如何更精準地預測LoRA性能、LoRA最佳學習率是全量微調10倍的理論解釋、LoRA變體的表現，以及LoE層LoRA如何與張量並行等技術結合以適配更大模型等，這些都是未來值得關注的研究方向。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_24.jpg" /></p> <p>最後，約翰·舒爾曼團隊的研究目標不僅是為了節省資源，更是為了<strong>「讓大模型的微調能力更易獲取」</strong>，使普通團隊也能利用大模型解決具體問題。透過對LoRA的深入研究，他們也深化了對「模型容量」、「資料集複雜度」、「樣本效率」等基礎問題的理解，展現了AI研究中實用技術背後蘊藏的核心原理新認知。</p> <hr /><hr />================================================</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">大家好，这里是最佳拍档，我是大飞。现在微调大模型。到底应该用LoRA还是全量微调呢？其实这个问题的核心。本质是“资源”和“效果”的平衡。全量微调（FullFT）虽然能把模型性能拉满。但是太“吃资源”了。一个万亿参数的模型。全量微调时不仅要存原权重。还要存梯度、优化器动量。而且这些数据得用FP32精度。比推理时的BF16多占一倍空间。普通团队根本扛不住硬件成本。而LoRA作为参数高效微调的主流方法。确实省内存、省算力。但是大家总是感觉没底。它的效果到底能不能跟全量微调比？会不会调了半天，性能差一截呢？而Thinking Machines Lab最近发布的一篇博客。恰恰把这个问题给讲透了。文章的标题是《LoRA无悔（LoRA Without Regret）》，作者是约翰·舒尔曼（John Schulman）和他的团队。文章不仅回答了“LoRA能不能媲美全量微调”的问题。还给出了具体的操作条件。甚至连超参数怎么设都讲清楚了。不管你是想入门LoRA。还是已经在落地中遇到问题。这篇内容都值得你仔细阅读。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_75.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先，我们得先搞清楚。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">为什么现在大家这么需要LoRA？这还得从大模型的现状说起。现在主流的语言模型。比如Llama 3、Qwen3。参数都在上万亿。预训练时用了数万亿个Token。这么大的规模。就是为了让模型学到人类书面知识里的所有模式。基础性能才能上去。但是到了“训练后处理”阶段。情况就发生了变化。不仅数据集变小了。关注的领域也变窄了。这时候如果还用全量微调。这就像用一辆卡车去拉一颗鸡蛋。太浪费了。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">就是因为这个“浪费”的问题。参数高效微调（PEFT）才发展起来。而低秩适应LoRA就是其中最成熟的方法。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">LoRA的核心思路很巧妙。它不直接改原模型的权重矩阵W。而是给W加了一个“低秩增量项”，公式是W' = W + γBA。这里的W'是微调后的权重。B和A是两个低秩矩阵。它们加起来的参数数量。比原矩阵W少得多。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_141.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">γ是个常数缩放因子。用来调整增量项的大小。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">简单说，LoRA就是用两个小矩阵。“模拟”出原模型权重的更新效果。这样既不用动原模型的海量参数。又能让模型适配新任务。这就是它“参数高效”的关键。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而且LoRA不止省参数。还有三个很实际的优势。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第一个是“多租户服务（Multi-tenant serving）”，因为LoRA只训练A和B这两个“适配器”，原模型权重不动。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">所以一个推理服务器可以在内存里存多个适配器。对应不同的任务。比如一个适配器处理客服对话。另一个处理产品摘要生成。还能批量处理这些请求。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">像陈（Chen）等人2023年提出的Punica框架。还有现在常用的vLLM、SGLang这些推理引擎。都已经支持这个功能。对企业来说能省不少硬件钱。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第二个优势是“训练布局优化”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">全量微调的时候。硬件布局得专门设计。你想，除了原权重。还要存梯度、优化器动量，精度还高。需要的加速器数量往往是推理时的10倍以上。但LoRA要更新的参数少。占用内存也少。训练时的硬件布局只比推理时稍微大一点。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_220.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">普通团队用几台GPU就能跑。门槛低多了。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第三个优势是“加载传输方便”，适配器矩阵的参数少。文件体积小。比如一个秩为128的LoRA适配器。可能就几MB或几十MB。不管是在机器间传。还是加载到模型里。都比传整个模型快得多，运维也灵活。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">正是这些优势。LoRA从2021年胡（Hu）等人发表的第一篇论文（《LoRA:。Low-Rank Adaptation of Large Language Models》）后。就越来越火。但是问题也来了，现有的研究里。LoRA和全量微调的性能对比一直不明确。大家公认的是。在“类似预训练”的场景里。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如数据集特别大。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">超过LoRA的参数存储极限的时候。LoRA性能会差一些；。但是在训练后处理的常见数据集规模下。LoRA虽然容量足够存下关键信息。可是没人能保证它的“样本效率”和“计算效率”，能跟全量微调相比。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">所以约翰·舒尔曼团队的实验。核心就是回答了。LoRA到底能不能匹配全量微调的性能？如果能，需要满足哪些条件呢？</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_289.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">他们的实验结论很明确。只要把几个关键细节做对。LoRA的样本效率和最终性能。就能和全量微调完全一致。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">那这些“关键细节”到底是什么？我们得先从他们的实验设计说起。因为好的实验设计。是结论可信的基础。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">和之前的LoRA研究相比。他们的实验有两个很重要的特点。第一个是“不盯着具体任务。而是研究通用关系”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">很多研究只会选一两个数据集。比如只测情感分析。或者只测文本摘要，但是他们不一样。专门研究“训练集大小”和“LoRA参数数量”之间的通用规律。这样得出的结论。不管你是做医疗还是教育任务。都能参考。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第二个是采用了“对数损失（log loss）评估”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">之前很多研究采用的是“基于采样的评估”，比如让模型生成文本，再人工打分。但是这种方法主观性强。不同任务里标准还不一样。而对数损失能直接反映模型的预测误差。而且能够清晰地看到不同训练步骤、不同参数设置下的模型表现规律。通用性强得多。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_357.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">就是靠着这样的实验设计。他们得出了五个关键发现。这些发现也是LoRA能够媲美全量微调的核心条件。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第一个发现。在中小规模的指令微调或者推理数据集上。LoRA和全量微调的性能完全一样。比如他们用的Tulu3和OpenThoughts3数据集。只要规模没超过LoRA的容量。两者的损失曲线、收敛速度都几乎重合。这说明在大多数企业的落地场景里。LoRA完全够用。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第二个发现。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">如果数据集规模超过了LoRA的容量。LoRA就会比全量微调差。但不是“损失降到一定程度就降不下去”了。而是“训练效率变慢”了。具体会慢多少。要取决于LoRA的参数容量和数据集大小的比例。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如一个秩为32的LoRA。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">处理100万Token的数据集没问题。但是如果数据集涨到1亿个Token。它的训练效率就会明显下降。损失下降速度变慢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_419.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第三个发现。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">LoRA对“批量大小（batch size）”的容忍度比全量微调低。如果批量 size 太大。超过某个阈值后。LoRA的性能损失就会比全量微调大得多。而且这个问题。就算增大LoRA的秩也解决不了。因为这是LoRA“矩阵乘积参数化（BA）”的固有属性。它的优化动态和原权重矩阵（W）不一样。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">批量一大，这种动态差异就会被放大。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第四个发现。LoRA一定要应用到模型的所有层。尤其是MLP层或MoE层，效果才好。之前很多研究受到第一篇LoRA论文的影响。只把LoRA用到注意力层。但是约翰·舒尔曼团队发现。仅在注意力层使用LoRA的性能很差。而MLP层才是LoRA发挥作用的关键。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比德曼（Biderman）等人2024年的研究（《LoRA Learns Less and Forgets Less》）也得出了类似结论。进一步验证了这个发现。第五个发现，在强化学习的场景里。就算用很低秩的LoRA。也能和全量微调性能一致。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_482.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">他们用数学推理任务做实验。比如让模型去解初中数学题。以答案正确性为奖励，结果发现。哪怕LoRA的秩只有1。最终的准确率也和全量微调一样。这背后有个信息论的解释。那就是监督学习里。每一轮训练能给模型提供“O(Token数量)”个比特的信息。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如一句话有100个Token。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">就能提供100比特左右的信息；。但是强化学习不一样。政策梯度算法的学习。靠的是“优势函数（Advantage Function）”，每一轮只能提供“O(1)”比特的信息。简单来说。就是强化学习需要学的信息少。就算是低秩LoRA，容量也完全够。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">基于这些发现。他们提出了一个“低遗憾区间（low-regret regime）”的概念。只要LoRA的参数容量能覆盖数据集的信息需求。并且应用到所有层。就能和全量微调性能相当。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而这个区间。刚好覆盖了大多数企业的训练后处理场景。这也意味着，LoRA在很多实际应用里。完全可以替代全量微调。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_547.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">当然，光有结论不够。我们还得知道这些结论是怎么来的。也就是他们的实验细节。这能帮我们在自己做实验的时候少走弯路。我们挑几个部分来重点讲一下。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先，他们的实验设置非常严谨。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先是LoRA的秩。覆盖了1到512三个数量级。从最低的秩1。到能覆盖大部分场景的秩128、256。再到接近全量微调的秩512。这样能全面看到秩对性能的影响。然后是学习率（LR）。为了避免“因为学习率选得不好。导致LoRA性能差”的情况出现。他们对每个实验条件都做了“学习率扫描”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如对秩32的LoRA。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">测试0.0001到0.01的不同学习率。找到最佳值；。而且采用了“恒定学习率”，没有热身或冷却阶段。这样能够排除学习率调度的干扰。更准确对比LoRA和全量微调的差异。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_608.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">模型方面，他们用了两类主流模型。一类是Llama 3系列。另一类是Qwen3系列。还包括了混合专家（MoE）模型。因为MoE模型的激活效率高。现在很多大模型都用这种架构。所以加入MoE的实验，能让结论更全面。数据集方面。监督学习用了Tulu3和OpenThoughts3。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">Tulu3是艾维森（Ivison）等人在2024年提出的。侧重“指令遵循”，比如让模型根据用户指令生成邮件、写代码；。OpenThoughts3是古哈（Guha）等人在2025年提出的。侧重“推理”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如数学题、逻辑推理题。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这两个数据集在任务类型、数据结构上差异很大。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">强化学习实验则用了MATH数据集和GSM8K数据集。都是数学推理任务。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">以答案正确与否作为奖励信号。以便专注测试强化学习场景下的LoRA性能。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">先看“LoRA秩”的影响实验。他们在Tulu3和OpenThoughts3上训练了一个epoch。对每个数据集、每个模型大小。都扫描了不同的LoRA秩和学习率。结果发现，高秩LoRA，比如256、512。和全量微调的学习曲线几乎重合。损失都是“随步骤的对数线性下降”，也就是说。每训练10倍的步骤。损失就会按固定比例下降。这和全量微调的规律完全一样。而低秩LoRA，比如1、4。在训练初期还能跟上。但是到了某个步骤阈值后。就会“偏离最小损失曲线”。这就是之前说的“容量耗尽”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_707.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">低秩矩阵存不下更多的任务信息。所以学习速度变慢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">更重要的是“学习率”的发现。他们绘制了“学习率和最终损失”的曲线。发现高秩LoRA的最小损失和全量微调几乎一样。但是最佳学习率是全量微调的10倍。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如全量微调的最佳学习率是0.001。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">LoRA就需要0.01。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比德曼（Biderman）等人2024年的研究也发现了类似的10倍比例。这说明这个规律不是偶然的。而是LoRA的固有属性。而且不同秩的LoRA。最佳学习率很接近。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如秩4和秩512的最佳学习率差异不到2倍。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这为我们设置LoRA学习率提供了很大便利。不用频繁调整。按全量微调的10倍开始试就行。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">再看“批量大小效应”的实验。他们用了OpenThoughts3里一个1万样本的子集。测试不同批量大小的影响。结果很明显，当批量大小比较小。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_768.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如32的时候。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">LoRA和全量微调的性能差距很小。而且随着训练推进，差距还会缩小；。但当批量大小变大。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如256、512的时候。LoRA的损失会明显比全量微调高。而且这个差距会一直存在。不会随训练步骤消失。更关键的是。这个差距和LoRA的秩无关。就算用秩512的LoRA。大批次下还是比全量微调差。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">他们分析。这是因为LoRA的“矩阵乘积（BA）”参数化。和原权重矩阵（W）的优化动态不同。批量一大。原矩阵能更好地利用批次信息更新。而BA矩阵的更新效率会下降。导致性能差距。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">不过好在，不管是LoRA还是全量微调。都是“小批量下性能更好”，所以实际应用里。只要别用太大的批量。这个问题就能够缓解。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">接下来。我们来重点说一说跟前面第4点发现。“LoRA该应用到哪些层”有关的实验过程。这是很多人调参时容易踩坑的地方。之前很多人习惯只把LoRA用到注意力层。觉得注意力层负责“理解上下文”，所以是关键，但是实验结果刚好相反。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_838.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">约翰·舒尔曼团队做了三组对比实验。第一组只在注意力层用LoRA。第二组只在MLP层用LoRA。第三组在所有层。也就是注意力+MLP+MoE都用了LoRA。结果发现。仅注意力层的LoRA性能最差。就算用更高的秩。参数数量和仅MLP层的秩128差不多。性能还是差一截。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如在Llama-3.1-8B模型上，仅MLP层。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">秩128，0.24B参数的损失。比仅注意力层。秩256，0.25B参数低了0.15左右。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">要知道，在语言模型里。损失降低0.1已经是很明显的提升了。而“所有层用LoRA”和“仅MLP层用LoRA”的性能几乎一样。这说明MLP层才是LoRA发挥作用的核心。注意力层加不加LoRA。对最终性能影响不大。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">他们还在MoE模型上做了实验。对Qwen3-30B-A3B-Base的每个专家都单独训练LoRA。秩设为“总秩除以活跃专家数量”，这样能保证MoE层的LoRA参数比例和其他层一致。结果和稠密模型一样。仅MLP层的LoRA优于仅注意力层。所有层和仅MLP层相当。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_918.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">为什么会这样？这就要用到马拉迪（Malladi）等人在2022年提出的“经验神经正切核（eNTK）”理论了。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">eNTK是用来描述模型微调初期学习动态的工具。它的核心是“梯度的点积”。比如对每个Token的预测。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">梯度g_i是损失对参数的偏导。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">eNTK的核函数K(i。j)就是g_i和g_j的点积。这个核函数决定了模型怎么“学习”数据里的模式。而参数越多的层。对核函数的影响越大。马拉迪等人的研究指出。当LoRA应用到所有层的时候。它的eNTK和全量微调的eNTK几乎一样。所以学习动态也相似；。但是如果只应用到注意力层。而忽略了参数更多的MLP层。eNTK就会和全量微调有明显差异。学习速度自然慢下来。这也解释了为什么仅注意力层的LoRA效果差。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">聊完实验。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">我们最后再来总结一下这篇研究的主要结论和背后的深层意义。它不止告诉了我们LoRA怎么用。还帮我们深化了对大模型微调的理解。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_986.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先是“LoRA匹配全量微调的两个核心条件”，一是要应用到所有层。尤其是MLP/MoE层。这样才能保证eNTK和全量微调的一致；。二是LoRA的容量要能覆盖数据集的信息需求。也就是“非容量约束”。只要满足这两个条件。LoRA就能和全量微调性能相当。这为我们选择微调方法提供了明确的判断标准。如果你的数据集是中小规模。模型有MLP层。就用LoRA；。如果数据集特别大。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如接近预训练规模。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">再考虑全量微调。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">其次是“监督学习和RL的容量需求的差异”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">监督学习需要的信息多。每个token 1比特。所以需要更高秩的LoRA；。强化学习需要的信息少，每轮次1比特。低秩LoRA就够。这个差异能帮我们节省资源。也就是做强化学习任务时。不用追求高秩。秩1、8就够，参数更少，训练更快。然后是“LoRA的计算效率优势”，他们算了一笔账。LoRA每轮训练的FLOPs。是全量微调的2/3左右。具体怎么算的大家可以去看一下这一段。这里我们就不一一推导了。简单来说，对于一个N×N的权重矩阵W。LoRA的总FLOPs是2N²。全量微调是3N²，所以比例就是2/3。这意味着，同样的硬件。LoRA能比全量微调多训练50%的样本。计算效率更高。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_1079.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">当然，研究也提出了一些“开放问题”，这些也是未来值得关注的方向。比如怎么更精准地预测LoRA的性能。不用每次都做实验；。为什么LoRA的最佳学习率是全量微调的10倍。目前还没有完整的理论解释；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">LoRA的变体在这种评估方法下表现如何；。还有MoE层的LoRA。怎么和张量并行、专家并行这些技术结合。适配更大的模型，等等等等。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">最后，约翰·舒尔曼团队在结语里提到。他们研究LoRA的目标。不只是为了省资源。更是为了“让大模型的微调能力更易获取”，毕竟不是每个团队都有能力做全量微调。但是LoRA能让普通团队也能用大模型来解决具体的问题。而且通过研究LoRA。他们也深化了对“模型容量”、“数据集复杂度”、“样本效率”这些基础问题的理解。这其实也是AI研究的魅力。一个实用的技术。背后往往藏着对核心原理的新认知。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_1140.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">好了，今天的内容就到这里。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">如果你在做LoRA相关的工作。希望今天的拆解能帮你少走弯路；。如果你只是感兴趣。也希望能够帮助你增加对LoRA的了解。感谢大家观看本期视频。我们下期再见。</p>

<hr style="clear:both" />
=============
<p><a href="https://www.youtube.com/watch?v=MlqXL6o--2M">https://www.youtube.com/watch?v=MlqXL6o--2M</a></p><p>Thinking Machines Lab最近发布的一篇博客，文章的标题是《LoRA无悔（LoRA Without Regret）》，作者是约翰·舒尔曼（John Schulman）和他的团队。文章不仅回答了“LoRA能不能媲美全量微调”的问题，还给出了具体的操作条件，甚至连超参数怎么设都讲清楚了。不管你是想入门LoRA，还是已经在落地中遇到问题，这篇内容都值得你仔细阅读。</p><p><a href="https://thinkingmachines.ai/blog/lora/">https://thinkingmachines.ai/blog/lora/</a></p>]]>
      </description>
      <content:encoded><![CDATA[<hr style="clear:both" />

<p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://img.youtube.com/vi/MlqXL6o--2M/maxresdefault.jpg" /></p><p><a href="https://www.youtube.com/watch?v=MlqXL6o--2M">https://www.youtube.com/watch?v=MlqXL6o--2M</a></p><h1>值得閱讀的理由</h1> <ul> <li>深入理解<strong>LoRA</strong>與<strong>全量微調（FullFT）</strong>在資源消耗與性能表現之間的平衡，為大模型微調提供決策依據。</li> <li>掌握<strong>約翰·舒爾曼（John Schulman）</strong>團隊提出的關鍵條件和最佳實踐，有效提升LoRA的微調效果。</li> <li>探索LoRA在監督學習與強化學習場景下的應用差異，以及未來大模型微調研究的開放性問題。</li> </ul> <hr /> <h1>摘要</h1> <p>本影片由影片的講者深入探討了當今大模型微調領域中一個核心問題：究竟該選擇<strong>低秩適應（LoRA）</strong>還是<strong>全量微調（FullFT）</strong>？這個問題的本質在於<strong>「資源」</strong>與<strong>「效果」</strong>的權衡。影片指出，儘管全量微調能最大化模型性能，但其對硬體資源的龐大需求（如儲存原始權重、梯度、優化器動量，且需使用FP32精度）使得一般團隊難以承受。相較之下，LoRA作為一種參數高效微調的主流方法，以其節省記憶體和算力的優勢吸引了廣泛關注，但其性能能否媲美全量微調一直是業界普遍的疑慮。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_90.jpg" /></p> <p>影片接著介紹了由<strong>約翰·舒爾曼（John Schulman）及其團隊</strong>發表的深度分析文章《LoRA無悔（LoRA Without Regret）》，該文章不僅回應了LoRA能否匹敵全量微調的疑問，更提出了具體的操作條件和超參數設定建議。影片強調，理解LoRA的重要性源於當前主流大型語言模型（如Llama 3、Qwen3）龐大的參數規模和預訓練資料量。在模型的「訓練後處理」階段，由於資料集規模變小且關注領域變窄，全量微調顯得過於浪費，因此催生了<strong>參數高效微調（PEFT）</strong>的發展，而LoRA正是其中最成熟的方法。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_180.jpg" /></p> <h2>LoRA的核心機制與優勢</h2> <p>影片詳細解釋了LoRA的核心思路：它不直接修改原模型的權重矩陣W，而是添加一個「低秩增量項」<strong>γBA</strong>。這裡的B和A是兩個低秩矩陣，它們的參數數量遠少於原始權重矩陣W，卻能「模擬」出原模型權重的更新效果。這種機制不僅實現了<strong>參數高效性</strong>，還帶來了多項實際優勢。首先是<strong>「多租戶服務」</strong>，由於LoRA只訓練輕量級的「適配器」，一個推理伺服器能夠同時載入多個適配器以處理不同任務，顯著節省硬體成本。其次是<strong>「訓練佈局優化」</strong>，LoRA更新的參數少，記憶體佔用小，使得訓練所需的硬體資源大幅降低，普通團隊也能用少量GPU進行訓練。最後是<strong>「載入傳輸方便」</strong>，LoRA適配器檔案體積小（僅幾MB到幾十MB），便於機器間傳輸和模型載入，提高了營運彈性。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_300.jpg" /></p> <h2>關鍵實驗發現與「低遺憾區間」</h2> <p>影片闡述了約翰·舒爾曼團隊實驗的核心目標：回答LoRA能否匹配全量微調的性能，以及需要滿足哪些條件。他們採取的實驗設計具有兩個顯著特點：一是<strong>「不聚焦特定任務，而是研究通用關係」</strong>，探討「訓練集大小」與「LoRA參數數量」之間的通用規律；二是採用<strong>「對數損失（log loss）評估」</strong>，能客觀反映模型的預測誤差和訓練表現。透過嚴謹的實驗設計，團隊得出了五個關鍵發現：</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_450.jpg" /></p> <p>1.  在中小規模的指令微調或推理資料集上，只要不超過LoRA的容量，<strong>LoRA與全量微調的性能幾乎完全一致</strong>。 2.  若資料集規模超出LoRA容量，LoRA會表現較差，但並非無法下降，而是<strong>「訓練效率變慢」</strong>。 3.  LoRA對<strong>「批量大小（batch size）」</strong>的容忍度低於全量微調，批次過大會導致LoRA性能明顯下降，且增大LoRA的秩也無法解決此問題。 4.  LoRA必須應用於模型<strong>「所有層」</strong>，尤其是<strong>MLP層或MoE層</strong>，效果才最佳；僅在注意力層使用LoRA的性能表現不佳。 5.  在<strong>強化學習（RL）</strong>場景中，即使使用<strong>極低秩的LoRA</strong>也能與全量微調性能一致，這歸因於強化學習每輪所需的資訊量較少（O(1)比特）。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_705.jpg" /></p> <p>基於這些發現，研究團隊提出了<strong>「低遺憾區間（low-regret regime）」</strong>的概念：只要LoRA的參數容量能覆蓋資料集所需的資訊，並且應用到所有層，就能與全量微調性能相當。這意味著在大多數企業的訓練後處理場景中，LoRA完全可以替代全量微調。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_750.jpg" /></p> <h2>實驗細節剖析</h2> <p>影片進一步深入探討了約翰·舒爾曼團隊的實驗細節，為實踐者提供了寶貴的參考。他們測試了從<strong>秩1到512</strong>的三個數量級LoRA秩，並對每個實驗條件進行了<strong>「學習率掃描」</strong>以找到最佳值，同時採用<strong>「恆定學習率」</strong>排除調度干擾。實驗模型涵蓋了<strong>Llama 3系列</strong>和<strong>Qwen3系列</strong>，以及<strong>混合專家（MoE）模型</strong>。資料集則選用了監督學習的<strong>Tulu3</strong>和<strong>OpenThoughts3</strong>，以及強化學習的<strong>MATH</strong>和<strong>GSM8K</strong>，確保了實驗的廣泛性和代表性。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_880.jpg" /></p> <p>關於<strong>LoRA秩的影響</strong>，高秩LoRA（如256、512）與全量微調的學習曲線幾乎重合，損失呈現「隨步驟的對數線性下降」；低秩LoRA（如1、4）在訓練初期尚可，但達到閾值後便會因「容量耗盡」而偏離最佳損失曲線。在<strong>學習率</strong>方面，研究發現LoRA的最佳學習率約為全量微調的10倍，且不同秩的LoRA之間最佳學習率差異不大，這為設定LoRA學習率提供了便利。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_990.jpg" /></p> <p><strong>批量大小效應</strong>實驗揭示，當批量大小較小（如32）時，LoRA與全量微調性能差距微小；但當批量大小增大（如256、512）時，LoRA的損失會顯著高於全量微調，且這種差距與LoRA的秩無關。這歸因於LoRA的「矩陣乘積參數化（BA）」與原權重矩陣（W）的優化動態不同，大批量會放大這種差異。幸運的是，在實際應用中，只要避免使用過大的批量，此問題便可緩解。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_1070.jpg" /></p> <p>影片特別強調了<strong>LoRA應該應用到哪些層</strong>的實驗結果。研究團隊通過三組對比實驗（僅注意力層、僅MLP層、所有層）發現，僅在注意力層使用LoRA的性能最差，而<strong>MLP層才是LoRA發揮作用的核心</strong>；將LoRA應用於所有層與僅MLP層的性能幾乎一致。這個發現與<strong>經驗神經正切核（eNTK）理論</strong>相符：參數越多的層對核函數影響越大，應用於所有層能使LoRA的eNTK與全量微調的eNTK保持一致，從而實現相似的學習動態。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_20.jpg" /></p> <h2>結論與深層意義</h2> <p>影片總結了該研究的深層意義。首先，LoRA匹配全量微調的核心條件有二：一是<strong>必須應用到所有層（尤其MLP/MoE層）</strong>，確保eNTK的一致性；二是<strong>LoRA的容量需能覆蓋資料集所需的資訊</strong>。其次，<strong>監督學習與強化學習對LoRA容量的需求存在顯著差異</strong>：監督學習需要更多資訊，適合高秩LoRA；強化學習每輪所需資訊少，低秩LoRA便足夠。這能幫助開發者在強化學習任務中節省資源，無需追求高秩。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_22.jpg" /></p> <p>研究還指出LoRA的<strong>計算效率優勢</strong>，每輪訓練的FLOPs約為全量微調的2/3，意味著在相同硬體下LoRA能多訓練50%的樣本。儘管如此，研究也提出了一些<strong>「開放問題」</strong>，例如如何更精準地預測LoRA性能、LoRA最佳學習率是全量微調10倍的理論解釋、LoRA變體的表現，以及LoE層LoRA如何與張量並行等技術結合以適配更大模型等，這些都是未來值得關注的研究方向。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_24.jpg" /></p> <p>最後，約翰·舒爾曼團隊的研究目標不僅是為了節省資源，更是為了<strong>「讓大模型的微調能力更易獲取」</strong>，使普通團隊也能利用大模型解決具體問題。透過對LoRA的深入研究，他們也深化了對「模型容量」、「資料集複雜度」、「樣本效率」等基礎問題的理解，展現了AI研究中實用技術背後蘊藏的核心原理新認知。</p> <hr /><hr />================================================</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">大家好，这里是最佳拍档，我是大飞。现在微调大模型。到底应该用LoRA还是全量微调呢？其实这个问题的核心。本质是“资源”和“效果”的平衡。全量微调（FullFT）虽然能把模型性能拉满。但是太“吃资源”了。一个万亿参数的模型。全量微调时不仅要存原权重。还要存梯度、优化器动量。而且这些数据得用FP32精度。比推理时的BF16多占一倍空间。普通团队根本扛不住硬件成本。而LoRA作为参数高效微调的主流方法。确实省内存、省算力。但是大家总是感觉没底。它的效果到底能不能跟全量微调比？会不会调了半天，性能差一截呢？而Thinking Machines Lab最近发布的一篇博客。恰恰把这个问题给讲透了。文章的标题是《LoRA无悔（LoRA Without Regret）》，作者是约翰·舒尔曼（John Schulman）和他的团队。文章不仅回答了“LoRA能不能媲美全量微调”的问题。还给出了具体的操作条件。甚至连超参数怎么设都讲清楚了。不管你是想入门LoRA。还是已经在落地中遇到问题。这篇内容都值得你仔细阅读。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_75.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先，我们得先搞清楚。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">为什么现在大家这么需要LoRA？这还得从大模型的现状说起。现在主流的语言模型。比如Llama 3、Qwen3。参数都在上万亿。预训练时用了数万亿个Token。这么大的规模。就是为了让模型学到人类书面知识里的所有模式。基础性能才能上去。但是到了“训练后处理”阶段。情况就发生了变化。不仅数据集变小了。关注的领域也变窄了。这时候如果还用全量微调。这就像用一辆卡车去拉一颗鸡蛋。太浪费了。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">就是因为这个“浪费”的问题。参数高效微调（PEFT）才发展起来。而低秩适应LoRA就是其中最成熟的方法。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">LoRA的核心思路很巧妙。它不直接改原模型的权重矩阵W。而是给W加了一个“低秩增量项”，公式是W' = W + γBA。这里的W'是微调后的权重。B和A是两个低秩矩阵。它们加起来的参数数量。比原矩阵W少得多。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_141.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">γ是个常数缩放因子。用来调整增量项的大小。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">简单说，LoRA就是用两个小矩阵。“模拟”出原模型权重的更新效果。这样既不用动原模型的海量参数。又能让模型适配新任务。这就是它“参数高效”的关键。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而且LoRA不止省参数。还有三个很实际的优势。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第一个是“多租户服务（Multi-tenant serving）”，因为LoRA只训练A和B这两个“适配器”，原模型权重不动。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">所以一个推理服务器可以在内存里存多个适配器。对应不同的任务。比如一个适配器处理客服对话。另一个处理产品摘要生成。还能批量处理这些请求。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">像陈（Chen）等人2023年提出的Punica框架。还有现在常用的vLLM、SGLang这些推理引擎。都已经支持这个功能。对企业来说能省不少硬件钱。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第二个优势是“训练布局优化”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">全量微调的时候。硬件布局得专门设计。你想，除了原权重。还要存梯度、优化器动量，精度还高。需要的加速器数量往往是推理时的10倍以上。但LoRA要更新的参数少。占用内存也少。训练时的硬件布局只比推理时稍微大一点。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_220.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">普通团队用几台GPU就能跑。门槛低多了。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第三个优势是“加载传输方便”，适配器矩阵的参数少。文件体积小。比如一个秩为128的LoRA适配器。可能就几MB或几十MB。不管是在机器间传。还是加载到模型里。都比传整个模型快得多，运维也灵活。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">正是这些优势。LoRA从2021年胡（Hu）等人发表的第一篇论文（《LoRA:。Low-Rank Adaptation of Large Language Models》）后。就越来越火。但是问题也来了，现有的研究里。LoRA和全量微调的性能对比一直不明确。大家公认的是。在“类似预训练”的场景里。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如数据集特别大。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">超过LoRA的参数存储极限的时候。LoRA性能会差一些；。但是在训练后处理的常见数据集规模下。LoRA虽然容量足够存下关键信息。可是没人能保证它的“样本效率”和“计算效率”，能跟全量微调相比。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">所以约翰·舒尔曼团队的实验。核心就是回答了。LoRA到底能不能匹配全量微调的性能？如果能，需要满足哪些条件呢？</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_289.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">他们的实验结论很明确。只要把几个关键细节做对。LoRA的样本效率和最终性能。就能和全量微调完全一致。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">那这些“关键细节”到底是什么？我们得先从他们的实验设计说起。因为好的实验设计。是结论可信的基础。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">和之前的LoRA研究相比。他们的实验有两个很重要的特点。第一个是“不盯着具体任务。而是研究通用关系”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">很多研究只会选一两个数据集。比如只测情感分析。或者只测文本摘要，但是他们不一样。专门研究“训练集大小”和“LoRA参数数量”之间的通用规律。这样得出的结论。不管你是做医疗还是教育任务。都能参考。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第二个是采用了“对数损失（log loss）评估”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">之前很多研究采用的是“基于采样的评估”，比如让模型生成文本，再人工打分。但是这种方法主观性强。不同任务里标准还不一样。而对数损失能直接反映模型的预测误差。而且能够清晰地看到不同训练步骤、不同参数设置下的模型表现规律。通用性强得多。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_357.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">就是靠着这样的实验设计。他们得出了五个关键发现。这些发现也是LoRA能够媲美全量微调的核心条件。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第一个发现。在中小规模的指令微调或者推理数据集上。LoRA和全量微调的性能完全一样。比如他们用的Tulu3和OpenThoughts3数据集。只要规模没超过LoRA的容量。两者的损失曲线、收敛速度都几乎重合。这说明在大多数企业的落地场景里。LoRA完全够用。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第二个发现。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">如果数据集规模超过了LoRA的容量。LoRA就会比全量微调差。但不是“损失降到一定程度就降不下去”了。而是“训练效率变慢”了。具体会慢多少。要取决于LoRA的参数容量和数据集大小的比例。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如一个秩为32的LoRA。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">处理100万Token的数据集没问题。但是如果数据集涨到1亿个Token。它的训练效率就会明显下降。损失下降速度变慢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_419.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第三个发现。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">LoRA对“批量大小（batch size）”的容忍度比全量微调低。如果批量 size 太大。超过某个阈值后。LoRA的性能损失就会比全量微调大得多。而且这个问题。就算增大LoRA的秩也解决不了。因为这是LoRA“矩阵乘积参数化（BA）”的固有属性。它的优化动态和原权重矩阵（W）不一样。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">批量一大，这种动态差异就会被放大。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第四个发现。LoRA一定要应用到模型的所有层。尤其是MLP层或MoE层，效果才好。之前很多研究受到第一篇LoRA论文的影响。只把LoRA用到注意力层。但是约翰·舒尔曼团队发现。仅在注意力层使用LoRA的性能很差。而MLP层才是LoRA发挥作用的关键。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比德曼（Biderman）等人2024年的研究（《LoRA Learns Less and Forgets Less》）也得出了类似结论。进一步验证了这个发现。第五个发现，在强化学习的场景里。就算用很低秩的LoRA。也能和全量微调性能一致。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_482.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">他们用数学推理任务做实验。比如让模型去解初中数学题。以答案正确性为奖励，结果发现。哪怕LoRA的秩只有1。最终的准确率也和全量微调一样。这背后有个信息论的解释。那就是监督学习里。每一轮训练能给模型提供“O(Token数量)”个比特的信息。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如一句话有100个Token。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">就能提供100比特左右的信息；。但是强化学习不一样。政策梯度算法的学习。靠的是“优势函数（Advantage Function）”，每一轮只能提供“O(1)”比特的信息。简单来说。就是强化学习需要学的信息少。就算是低秩LoRA，容量也完全够。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">基于这些发现。他们提出了一个“低遗憾区间（low-regret regime）”的概念。只要LoRA的参数容量能覆盖数据集的信息需求。并且应用到所有层。就能和全量微调性能相当。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而这个区间。刚好覆盖了大多数企业的训练后处理场景。这也意味着，LoRA在很多实际应用里。完全可以替代全量微调。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_547.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">当然，光有结论不够。我们还得知道这些结论是怎么来的。也就是他们的实验细节。这能帮我们在自己做实验的时候少走弯路。我们挑几个部分来重点讲一下。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先，他们的实验设置非常严谨。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先是LoRA的秩。覆盖了1到512三个数量级。从最低的秩1。到能覆盖大部分场景的秩128、256。再到接近全量微调的秩512。这样能全面看到秩对性能的影响。然后是学习率（LR）。为了避免“因为学习率选得不好。导致LoRA性能差”的情况出现。他们对每个实验条件都做了“学习率扫描”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如对秩32的LoRA。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">测试0.0001到0.01的不同学习率。找到最佳值；。而且采用了“恒定学习率”，没有热身或冷却阶段。这样能够排除学习率调度的干扰。更准确对比LoRA和全量微调的差异。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_608.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">模型方面，他们用了两类主流模型。一类是Llama 3系列。另一类是Qwen3系列。还包括了混合专家（MoE）模型。因为MoE模型的激活效率高。现在很多大模型都用这种架构。所以加入MoE的实验，能让结论更全面。数据集方面。监督学习用了Tulu3和OpenThoughts3。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">Tulu3是艾维森（Ivison）等人在2024年提出的。侧重“指令遵循”，比如让模型根据用户指令生成邮件、写代码；。OpenThoughts3是古哈（Guha）等人在2025年提出的。侧重“推理”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如数学题、逻辑推理题。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这两个数据集在任务类型、数据结构上差异很大。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">强化学习实验则用了MATH数据集和GSM8K数据集。都是数学推理任务。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">以答案正确与否作为奖励信号。以便专注测试强化学习场景下的LoRA性能。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">先看“LoRA秩”的影响实验。他们在Tulu3和OpenThoughts3上训练了一个epoch。对每个数据集、每个模型大小。都扫描了不同的LoRA秩和学习率。结果发现，高秩LoRA，比如256、512。和全量微调的学习曲线几乎重合。损失都是“随步骤的对数线性下降”，也就是说。每训练10倍的步骤。损失就会按固定比例下降。这和全量微调的规律完全一样。而低秩LoRA，比如1、4。在训练初期还能跟上。但是到了某个步骤阈值后。就会“偏离最小损失曲线”。这就是之前说的“容量耗尽”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_707.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">低秩矩阵存不下更多的任务信息。所以学习速度变慢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">更重要的是“学习率”的发现。他们绘制了“学习率和最终损失”的曲线。发现高秩LoRA的最小损失和全量微调几乎一样。但是最佳学习率是全量微调的10倍。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如全量微调的最佳学习率是0.001。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">LoRA就需要0.01。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比德曼（Biderman）等人2024年的研究也发现了类似的10倍比例。这说明这个规律不是偶然的。而是LoRA的固有属性。而且不同秩的LoRA。最佳学习率很接近。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如秩4和秩512的最佳学习率差异不到2倍。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这为我们设置LoRA学习率提供了很大便利。不用频繁调整。按全量微调的10倍开始试就行。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">再看“批量大小效应”的实验。他们用了OpenThoughts3里一个1万样本的子集。测试不同批量大小的影响。结果很明显，当批量大小比较小。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_768.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如32的时候。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">LoRA和全量微调的性能差距很小。而且随着训练推进，差距还会缩小；。但当批量大小变大。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如256、512的时候。LoRA的损失会明显比全量微调高。而且这个差距会一直存在。不会随训练步骤消失。更关键的是。这个差距和LoRA的秩无关。就算用秩512的LoRA。大批次下还是比全量微调差。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">他们分析。这是因为LoRA的“矩阵乘积（BA）”参数化。和原权重矩阵（W）的优化动态不同。批量一大。原矩阵能更好地利用批次信息更新。而BA矩阵的更新效率会下降。导致性能差距。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">不过好在，不管是LoRA还是全量微调。都是“小批量下性能更好”，所以实际应用里。只要别用太大的批量。这个问题就能够缓解。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">接下来。我们来重点说一说跟前面第4点发现。“LoRA该应用到哪些层”有关的实验过程。这是很多人调参时容易踩坑的地方。之前很多人习惯只把LoRA用到注意力层。觉得注意力层负责“理解上下文”，所以是关键，但是实验结果刚好相反。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_838.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">约翰·舒尔曼团队做了三组对比实验。第一组只在注意力层用LoRA。第二组只在MLP层用LoRA。第三组在所有层。也就是注意力+MLP+MoE都用了LoRA。结果发现。仅注意力层的LoRA性能最差。就算用更高的秩。参数数量和仅MLP层的秩128差不多。性能还是差一截。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如在Llama-3.1-8B模型上，仅MLP层。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">秩128，0.24B参数的损失。比仅注意力层。秩256，0.25B参数低了0.15左右。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">要知道，在语言模型里。损失降低0.1已经是很明显的提升了。而“所有层用LoRA”和“仅MLP层用LoRA”的性能几乎一样。这说明MLP层才是LoRA发挥作用的核心。注意力层加不加LoRA。对最终性能影响不大。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">他们还在MoE模型上做了实验。对Qwen3-30B-A3B-Base的每个专家都单独训练LoRA。秩设为“总秩除以活跃专家数量”，这样能保证MoE层的LoRA参数比例和其他层一致。结果和稠密模型一样。仅MLP层的LoRA优于仅注意力层。所有层和仅MLP层相当。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_918.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">为什么会这样？这就要用到马拉迪（Malladi）等人在2022年提出的“经验神经正切核（eNTK）”理论了。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">eNTK是用来描述模型微调初期学习动态的工具。它的核心是“梯度的点积”。比如对每个Token的预测。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">梯度g_i是损失对参数的偏导。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">eNTK的核函数K(i。j)就是g_i和g_j的点积。这个核函数决定了模型怎么“学习”数据里的模式。而参数越多的层。对核函数的影响越大。马拉迪等人的研究指出。当LoRA应用到所有层的时候。它的eNTK和全量微调的eNTK几乎一样。所以学习动态也相似；。但是如果只应用到注意力层。而忽略了参数更多的MLP层。eNTK就会和全量微调有明显差异。学习速度自然慢下来。这也解释了为什么仅注意力层的LoRA效果差。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">聊完实验。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">我们最后再来总结一下这篇研究的主要结论和背后的深层意义。它不止告诉了我们LoRA怎么用。还帮我们深化了对大模型微调的理解。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_986.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先是“LoRA匹配全量微调的两个核心条件”，一是要应用到所有层。尤其是MLP/MoE层。这样才能保证eNTK和全量微调的一致；。二是LoRA的容量要能覆盖数据集的信息需求。也就是“非容量约束”。只要满足这两个条件。LoRA就能和全量微调性能相当。这为我们选择微调方法提供了明确的判断标准。如果你的数据集是中小规模。模型有MLP层。就用LoRA；。如果数据集特别大。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如接近预训练规模。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">再考虑全量微调。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">其次是“监督学习和RL的容量需求的差异”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">监督学习需要的信息多。每个token 1比特。所以需要更高秩的LoRA；。强化学习需要的信息少，每轮次1比特。低秩LoRA就够。这个差异能帮我们节省资源。也就是做强化学习任务时。不用追求高秩。秩1、8就够，参数更少，训练更快。然后是“LoRA的计算效率优势”，他们算了一笔账。LoRA每轮训练的FLOPs。是全量微调的2/3左右。具体怎么算的大家可以去看一下这一段。这里我们就不一一推导了。简单来说，对于一个N×N的权重矩阵W。LoRA的总FLOPs是2N²。全量微调是3N²，所以比例就是2/3。这意味着，同样的硬件。LoRA能比全量微调多训练50%的样本。计算效率更高。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_1079.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">当然，研究也提出了一些“开放问题”，这些也是未来值得关注的方向。比如怎么更精准地预测LoRA的性能。不用每次都做实验；。为什么LoRA的最佳学习率是全量微调的10倍。目前还没有完整的理论解释；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">LoRA的变体在这种评估方法下表现如何；。还有MoE层的LoRA。怎么和张量并行、专家并行这些技术结合。适配更大的模型，等等等等。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">最后，约翰·舒尔曼团队在结语里提到。他们研究LoRA的目标。不只是为了省资源。更是为了“让大模型的微调能力更易获取”，毕竟不是每个团队都有能力做全量微调。但是LoRA能让普通团队也能用大模型来解决具体的问题。而且通过研究LoRA。他们也深化了对“模型容量”、“数据集复杂度”、“样本效率”这些基础问题的理解。这其实也是AI研究的魅力。一个实用的技术。背后往往藏着对核心原理的新认知。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_1140.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">好了，今天的内容就到这里。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">如果你在做LoRA相关的工作。希望今天的拆解能帮你少走弯路；。如果你只是感兴趣。也希望能够帮助你增加对LoRA的了解。感谢大家观看本期视频。我们下期再见。</p>

<hr style="clear:both" />
=============
<p><a href="https://www.youtube.com/watch?v=MlqXL6o--2M">https://www.youtube.com/watch?v=MlqXL6o--2M</a></p><p>Thinking Machines Lab最近发布的一篇博客，文章的标题是《LoRA无悔（LoRA Without Regret）》，作者是约翰·舒尔曼（John Schulman）和他的团队。文章不仅回答了“LoRA能不能媲美全量微调”的问题，还给出了具体的操作条件，甚至连超参数怎么设都讲清楚了。不管你是想入门LoRA，还是已经在落地中遇到问题，这篇内容都值得你仔细阅读。</p><p><a href="https://thinkingmachines.ai/blog/lora/">https://thinkingmachines.ai/blog/lora/</a></p>]]></content:encoded>
      <itunes:image href="https://i.ytimg.com/vi/MlqXL6o--2M/hqdefault.jpg"/>
      <pubDate>2025-10-02T09:00:39.000Z</pubDate>
    </item></channel>
</rss>