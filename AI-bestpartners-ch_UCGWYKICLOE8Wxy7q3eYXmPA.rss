<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:itunes="http://www.itunes.com/dtds/podcast-1.0.dtd" xmlns:googleplay="http://www.google.com/schemas/play-podcasts/1.0" xmlns:media="http://www.rssboard.org/media-rss" version="2.0">
  <channel>
    <title><![CDATA[AI-最佳拍檔[YT+]]]></title>
    <link>http://www.youtube.com/feeds/videos.xml?channel_id=UCGWYKICLOE8Wxy7q3eYXmPA</link>
    <image>
      <url>https://yt3.googleusercontent.com/GpvF9XbD-stx6HR3BySXKvMqm_AySlczqmJKdkdZsloYQ9-rnoaLdCpOn0irmvqi3QYroccHNg=s900-b50-c-k-c0x008A95A5-no-rj</url>
      <title>AI-最佳拍檔[YT+]</title>
      <link>http://www.youtube.com/feeds/videos.xml?channel_id=UCGWYKICLOE8Wxy7q3eYXmPA</link>
    </image>
    <language>en-us</language>
    <atom:link href="https://www.youtube.com/feeds/videos.xml?channel_id=UCGWYKICLOE8Wxy7q3eYXmPA" rel="self" type="application/rss+xml"/>
    <copyright><![CDATA[AI-最佳拍檔[YT+]]]></copyright>
    <itunes:author><![CDATA[AI-最佳拍檔[YT+]]]></itunes:author>
    <itunes:summary>
      <![CDATA[
      <a href="https://www.youtube.com/channel/UCGWYKICLOE8Wxy7q3eYXmPA" target="_blank">https://www.youtube.com/channel/UCGWYKICLOE8Wxy7q3eYXmPA</a><br />
<br />
<a href="https://www.youtube.com/feeds/videos.xml?channel_id=UCGWYKICLOE8Wxy7q3eYXmPA" target="_blank">https://www.youtube.com/feeds/videos.xml?channel_id=UCGWYKICLOE8Wxy7q3eYXmPA</a>
      ]]>
    </itunes:summary>
    <description>
      <![CDATA[
      <a href="https://www.youtube.com/channel/UCGWYKICLOE8Wxy7q3eYXmPA" target="_blank">https://www.youtube.com/channel/UCGWYKICLOE8Wxy7q3eYXmPA</a><br />
<br />
<a href="https://www.youtube.com/feeds/videos.xml?channel_id=UCGWYKICLOE8Wxy7q3eYXmPA" target="_blank">https://www.youtube.com/feeds/videos.xml?channel_id=UCGWYKICLOE8Wxy7q3eYXmPA</a>
      ]]>
    </description>
    <itunes:owner>
      <itunes:name><![CDATA[AI-最佳拍檔[YT+]]]></itunes:name>
    </itunes:owner>
    <itunes:image href="https://yt3.googleusercontent.com/GpvF9XbD-stx6HR3BySXKvMqm_AySlczqmJKdkdZsloYQ9-rnoaLdCpOn0irmvqi3QYroccHNg=s900-b50-c-k-c0x008A95A5-no-rj"/>
<item>
      <title><![CDATA[【人工智能】Lora无悔 | Thinking Machines最新研究 | John Schulman | 全量微调 | 什么时候可以放心用Lora | MLP层 | 学习率LR | eNTK理论]]></title>
      <link>https://www.youtube.com/watch?v=MlqXL6o--2M</link>
      <itunes:title><![CDATA[【人工智能】Lora无悔 | Thinking Machines最新研究 | John Schulman | 全量微调 | 什么时候可以放心用Lora | MLP层 | 学习率LR | eNTK理论]]></itunes:title>
      <itunes:author><![CDATA[最佳拍档]]></itunes:author>
      <itunes:summary>
        <![CDATA[<hr style="clear:both" />

<p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://img.youtube.com/vi/MlqXL6o--2M/maxresdefault.jpg" /></p><p><a href="https://www.youtube.com/watch?v=MlqXL6o--2M">https://www.youtube.com/watch?v=MlqXL6o--2M</a></p><h1>值得閱讀的理由</h1> <ul> <li>深入理解<strong>LoRA</strong>與<strong>全量微調（FullFT）</strong>在資源消耗與性能表現之間的平衡，為大模型微調提供決策依據。</li> <li>掌握<strong>約翰·舒爾曼（John Schulman）</strong>團隊提出的關鍵條件和最佳實踐，有效提升LoRA的微調效果。</li> <li>探索LoRA在監督學習與強化學習場景下的應用差異，以及未來大模型微調研究的開放性問題。</li> </ul> <hr /> <h1>摘要</h1> <p>本影片由影片的講者深入探討了當今大模型微調領域中一個核心問題：究竟該選擇<strong>低秩適應（LoRA）</strong>還是<strong>全量微調（FullFT）</strong>？這個問題的本質在於<strong>「資源」</strong>與<strong>「效果」</strong>的權衡。影片指出，儘管全量微調能最大化模型性能，但其對硬體資源的龐大需求（如儲存原始權重、梯度、優化器動量，且需使用FP32精度）使得一般團隊難以承受。相較之下，LoRA作為一種參數高效微調的主流方法，以其節省記憶體和算力的優勢吸引了廣泛關注，但其性能能否媲美全量微調一直是業界普遍的疑慮。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_90.jpg" /></p> <p>影片接著介紹了由<strong>約翰·舒爾曼（John Schulman）及其團隊</strong>發表的深度分析文章《LoRA無悔（LoRA Without Regret）》，該文章不僅回應了LoRA能否匹敵全量微調的疑問，更提出了具體的操作條件和超參數設定建議。影片強調，理解LoRA的重要性源於當前主流大型語言模型（如Llama 3、Qwen3）龐大的參數規模和預訓練資料量。在模型的「訓練後處理」階段，由於資料集規模變小且關注領域變窄，全量微調顯得過於浪費，因此催生了<strong>參數高效微調（PEFT）</strong>的發展，而LoRA正是其中最成熟的方法。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_180.jpg" /></p> <h2>LoRA的核心機制與優勢</h2> <p>影片詳細解釋了LoRA的核心思路：它不直接修改原模型的權重矩陣W，而是添加一個「低秩增量項」<strong>γBA</strong>。這裡的B和A是兩個低秩矩陣，它們的參數數量遠少於原始權重矩陣W，卻能「模擬」出原模型權重的更新效果。這種機制不僅實現了<strong>參數高效性</strong>，還帶來了多項實際優勢。首先是<strong>「多租戶服務」</strong>，由於LoRA只訓練輕量級的「適配器」，一個推理伺服器能夠同時載入多個適配器以處理不同任務，顯著節省硬體成本。其次是<strong>「訓練佈局優化」</strong>，LoRA更新的參數少，記憶體佔用小，使得訓練所需的硬體資源大幅降低，普通團隊也能用少量GPU進行訓練。最後是<strong>「載入傳輸方便」</strong>，LoRA適配器檔案體積小（僅幾MB到幾十MB），便於機器間傳輸和模型載入，提高了營運彈性。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_300.jpg" /></p> <h2>關鍵實驗發現與「低遺憾區間」</h2> <p>影片闡述了約翰·舒爾曼團隊實驗的核心目標：回答LoRA能否匹配全量微調的性能，以及需要滿足哪些條件。他們採取的實驗設計具有兩個顯著特點：一是<strong>「不聚焦特定任務，而是研究通用關係」</strong>，探討「訓練集大小」與「LoRA參數數量」之間的通用規律；二是採用<strong>「對數損失（log loss）評估」</strong>，能客觀反映模型的預測誤差和訓練表現。透過嚴謹的實驗設計，團隊得出了五個關鍵發現：</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_450.jpg" /></p> <p>1.  在中小規模的指令微調或推理資料集上，只要不超過LoRA的容量，<strong>LoRA與全量微調的性能幾乎完全一致</strong>。 2.  若資料集規模超出LoRA容量，LoRA會表現較差，但並非無法下降，而是<strong>「訓練效率變慢」</strong>。 3.  LoRA對<strong>「批量大小（batch size）」</strong>的容忍度低於全量微調，批次過大會導致LoRA性能明顯下降，且增大LoRA的秩也無法解決此問題。 4.  LoRA必須應用於模型<strong>「所有層」</strong>，尤其是<strong>MLP層或MoE層</strong>，效果才最佳；僅在注意力層使用LoRA的性能表現不佳。 5.  在<strong>強化學習（RL）</strong>場景中，即使使用<strong>極低秩的LoRA</strong>也能與全量微調性能一致，這歸因於強化學習每輪所需的資訊量較少（O(1)比特）。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_705.jpg" /></p> <p>基於這些發現，研究團隊提出了<strong>「低遺憾區間（low-regret regime）」</strong>的概念：只要LoRA的參數容量能覆蓋資料集所需的資訊，並且應用到所有層，就能與全量微調性能相當。這意味著在大多數企業的訓練後處理場景中，LoRA完全可以替代全量微調。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_750.jpg" /></p> <h2>實驗細節剖析</h2> <p>影片進一步深入探討了約翰·舒爾曼團隊的實驗細節，為實踐者提供了寶貴的參考。他們測試了從<strong>秩1到512</strong>的三個數量級LoRA秩，並對每個實驗條件進行了<strong>「學習率掃描」</strong>以找到最佳值，同時採用<strong>「恆定學習率」</strong>排除調度干擾。實驗模型涵蓋了<strong>Llama 3系列</strong>和<strong>Qwen3系列</strong>，以及<strong>混合專家（MoE）模型</strong>。資料集則選用了監督學習的<strong>Tulu3</strong>和<strong>OpenThoughts3</strong>，以及強化學習的<strong>MATH</strong>和<strong>GSM8K</strong>，確保了實驗的廣泛性和代表性。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_880.jpg" /></p> <p>關於<strong>LoRA秩的影響</strong>，高秩LoRA（如256、512）與全量微調的學習曲線幾乎重合，損失呈現「隨步驟的對數線性下降」；低秩LoRA（如1、4）在訓練初期尚可，但達到閾值後便會因「容量耗盡」而偏離最佳損失曲線。在<strong>學習率</strong>方面，研究發現LoRA的最佳學習率約為全量微調的10倍，且不同秩的LoRA之間最佳學習率差異不大，這為設定LoRA學習率提供了便利。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_990.jpg" /></p> <p><strong>批量大小效應</strong>實驗揭示，當批量大小較小（如32）時，LoRA與全量微調性能差距微小；但當批量大小增大（如256、512）時，LoRA的損失會顯著高於全量微調，且這種差距與LoRA的秩無關。這歸因於LoRA的「矩陣乘積參數化（BA）」與原權重矩陣（W）的優化動態不同，大批量會放大這種差異。幸運的是，在實際應用中，只要避免使用過大的批量，此問題便可緩解。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_1070.jpg" /></p> <p>影片特別強調了<strong>LoRA應該應用到哪些層</strong>的實驗結果。研究團隊通過三組對比實驗（僅注意力層、僅MLP層、所有層）發現，僅在注意力層使用LoRA的性能最差，而<strong>MLP層才是LoRA發揮作用的核心</strong>；將LoRA應用於所有層與僅MLP層的性能幾乎一致。這個發現與<strong>經驗神經正切核（eNTK）理論</strong>相符：參數越多的層對核函數影響越大，應用於所有層能使LoRA的eNTK與全量微調的eNTK保持一致，從而實現相似的學習動態。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_20.jpg" /></p> <h2>結論與深層意義</h2> <p>影片總結了該研究的深層意義。首先，LoRA匹配全量微調的核心條件有二：一是<strong>必須應用到所有層（尤其MLP/MoE層）</strong>，確保eNTK的一致性；二是<strong>LoRA的容量需能覆蓋資料集所需的資訊</strong>。其次，<strong>監督學習與強化學習對LoRA容量的需求存在顯著差異</strong>：監督學習需要更多資訊，適合高秩LoRA；強化學習每輪所需資訊少，低秩LoRA便足夠。這能幫助開發者在強化學習任務中節省資源，無需追求高秩。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_22.jpg" /></p> <p>研究還指出LoRA的<strong>計算效率優勢</strong>，每輪訓練的FLOPs約為全量微調的2/3，意味著在相同硬體下LoRA能多訓練50%的樣本。儘管如此，研究也提出了一些<strong>「開放問題」</strong>，例如如何更精準地預測LoRA性能、LoRA最佳學習率是全量微調10倍的理論解釋、LoRA變體的表現，以及LoE層LoRA如何與張量並行等技術結合以適配更大模型等，這些都是未來值得關注的研究方向。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_24.jpg" /></p> <p>最後，約翰·舒爾曼團隊的研究目標不僅是為了節省資源，更是為了<strong>「讓大模型的微調能力更易獲取」</strong>，使普通團隊也能利用大模型解決具體問題。透過對LoRA的深入研究，他們也深化了對「模型容量」、「資料集複雜度」、「樣本效率」等基礎問題的理解，展現了AI研究中實用技術背後蘊藏的核心原理新認知。</p> <hr /><hr />================================================</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">大家好，这里是最佳拍档，我是大飞。现在微调大模型。到底应该用LoRA还是全量微调呢？其实这个问题的核心。本质是“资源”和“效果”的平衡。全量微调（FullFT）虽然能把模型性能拉满。但是太“吃资源”了。一个万亿参数的模型。全量微调时不仅要存原权重。还要存梯度、优化器动量。而且这些数据得用FP32精度。比推理时的BF16多占一倍空间。普通团队根本扛不住硬件成本。而LoRA作为参数高效微调的主流方法。确实省内存、省算力。但是大家总是感觉没底。它的效果到底能不能跟全量微调比？会不会调了半天，性能差一截呢？而Thinking Machines Lab最近发布的一篇博客。恰恰把这个问题给讲透了。文章的标题是《LoRA无悔（LoRA Without Regret）》，作者是约翰·舒尔曼（John Schulman）和他的团队。文章不仅回答了“LoRA能不能媲美全量微调”的问题。还给出了具体的操作条件。甚至连超参数怎么设都讲清楚了。不管你是想入门LoRA。还是已经在落地中遇到问题。这篇内容都值得你仔细阅读。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_75.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先，我们得先搞清楚。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">为什么现在大家这么需要LoRA？这还得从大模型的现状说起。现在主流的语言模型。比如Llama 3、Qwen3。参数都在上万亿。预训练时用了数万亿个Token。这么大的规模。就是为了让模型学到人类书面知识里的所有模式。基础性能才能上去。但是到了“训练后处理”阶段。情况就发生了变化。不仅数据集变小了。关注的领域也变窄了。这时候如果还用全量微调。这就像用一辆卡车去拉一颗鸡蛋。太浪费了。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">就是因为这个“浪费”的问题。参数高效微调（PEFT）才发展起来。而低秩适应LoRA就是其中最成熟的方法。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">LoRA的核心思路很巧妙。它不直接改原模型的权重矩阵W。而是给W加了一个“低秩增量项”，公式是W' = W + γBA。这里的W'是微调后的权重。B和A是两个低秩矩阵。它们加起来的参数数量。比原矩阵W少得多。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_141.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">γ是个常数缩放因子。用来调整增量项的大小。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">简单说，LoRA就是用两个小矩阵。“模拟”出原模型权重的更新效果。这样既不用动原模型的海量参数。又能让模型适配新任务。这就是它“参数高效”的关键。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而且LoRA不止省参数。还有三个很实际的优势。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第一个是“多租户服务（Multi-tenant serving）”，因为LoRA只训练A和B这两个“适配器”，原模型权重不动。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">所以一个推理服务器可以在内存里存多个适配器。对应不同的任务。比如一个适配器处理客服对话。另一个处理产品摘要生成。还能批量处理这些请求。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">像陈（Chen）等人2023年提出的Punica框架。还有现在常用的vLLM、SGLang这些推理引擎。都已经支持这个功能。对企业来说能省不少硬件钱。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第二个优势是“训练布局优化”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">全量微调的时候。硬件布局得专门设计。你想，除了原权重。还要存梯度、优化器动量，精度还高。需要的加速器数量往往是推理时的10倍以上。但LoRA要更新的参数少。占用内存也少。训练时的硬件布局只比推理时稍微大一点。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_220.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">普通团队用几台GPU就能跑。门槛低多了。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第三个优势是“加载传输方便”，适配器矩阵的参数少。文件体积小。比如一个秩为128的LoRA适配器。可能就几MB或几十MB。不管是在机器间传。还是加载到模型里。都比传整个模型快得多，运维也灵活。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">正是这些优势。LoRA从2021年胡（Hu）等人发表的第一篇论文（《LoRA:。Low-Rank Adaptation of Large Language Models》）后。就越来越火。但是问题也来了，现有的研究里。LoRA和全量微调的性能对比一直不明确。大家公认的是。在“类似预训练”的场景里。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如数据集特别大。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">超过LoRA的参数存储极限的时候。LoRA性能会差一些；。但是在训练后处理的常见数据集规模下。LoRA虽然容量足够存下关键信息。可是没人能保证它的“样本效率”和“计算效率”，能跟全量微调相比。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">所以约翰·舒尔曼团队的实验。核心就是回答了。LoRA到底能不能匹配全量微调的性能？如果能，需要满足哪些条件呢？</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_289.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">他们的实验结论很明确。只要把几个关键细节做对。LoRA的样本效率和最终性能。就能和全量微调完全一致。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">那这些“关键细节”到底是什么？我们得先从他们的实验设计说起。因为好的实验设计。是结论可信的基础。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">和之前的LoRA研究相比。他们的实验有两个很重要的特点。第一个是“不盯着具体任务。而是研究通用关系”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">很多研究只会选一两个数据集。比如只测情感分析。或者只测文本摘要，但是他们不一样。专门研究“训练集大小”和“LoRA参数数量”之间的通用规律。这样得出的结论。不管你是做医疗还是教育任务。都能参考。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第二个是采用了“对数损失（log loss）评估”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">之前很多研究采用的是“基于采样的评估”，比如让模型生成文本，再人工打分。但是这种方法主观性强。不同任务里标准还不一样。而对数损失能直接反映模型的预测误差。而且能够清晰地看到不同训练步骤、不同参数设置下的模型表现规律。通用性强得多。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_357.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">就是靠着这样的实验设计。他们得出了五个关键发现。这些发现也是LoRA能够媲美全量微调的核心条件。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第一个发现。在中小规模的指令微调或者推理数据集上。LoRA和全量微调的性能完全一样。比如他们用的Tulu3和OpenThoughts3数据集。只要规模没超过LoRA的容量。两者的损失曲线、收敛速度都几乎重合。这说明在大多数企业的落地场景里。LoRA完全够用。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第二个发现。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">如果数据集规模超过了LoRA的容量。LoRA就会比全量微调差。但不是“损失降到一定程度就降不下去”了。而是“训练效率变慢”了。具体会慢多少。要取决于LoRA的参数容量和数据集大小的比例。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如一个秩为32的LoRA。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">处理100万Token的数据集没问题。但是如果数据集涨到1亿个Token。它的训练效率就会明显下降。损失下降速度变慢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_419.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第三个发现。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">LoRA对“批量大小（batch size）”的容忍度比全量微调低。如果批量 size 太大。超过某个阈值后。LoRA的性能损失就会比全量微调大得多。而且这个问题。就算增大LoRA的秩也解决不了。因为这是LoRA“矩阵乘积参数化（BA）”的固有属性。它的优化动态和原权重矩阵（W）不一样。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">批量一大，这种动态差异就会被放大。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第四个发现。LoRA一定要应用到模型的所有层。尤其是MLP层或MoE层，效果才好。之前很多研究受到第一篇LoRA论文的影响。只把LoRA用到注意力层。但是约翰·舒尔曼团队发现。仅在注意力层使用LoRA的性能很差。而MLP层才是LoRA发挥作用的关键。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比德曼（Biderman）等人2024年的研究（《LoRA Learns Less and Forgets Less》）也得出了类似结论。进一步验证了这个发现。第五个发现，在强化学习的场景里。就算用很低秩的LoRA。也能和全量微调性能一致。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_482.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">他们用数学推理任务做实验。比如让模型去解初中数学题。以答案正确性为奖励，结果发现。哪怕LoRA的秩只有1。最终的准确率也和全量微调一样。这背后有个信息论的解释。那就是监督学习里。每一轮训练能给模型提供“O(Token数量)”个比特的信息。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如一句话有100个Token。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">就能提供100比特左右的信息；。但是强化学习不一样。政策梯度算法的学习。靠的是“优势函数（Advantage Function）”，每一轮只能提供“O(1)”比特的信息。简单来说。就是强化学习需要学的信息少。就算是低秩LoRA，容量也完全够。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">基于这些发现。他们提出了一个“低遗憾区间（low-regret regime）”的概念。只要LoRA的参数容量能覆盖数据集的信息需求。并且应用到所有层。就能和全量微调性能相当。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而这个区间。刚好覆盖了大多数企业的训练后处理场景。这也意味着，LoRA在很多实际应用里。完全可以替代全量微调。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_547.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">当然，光有结论不够。我们还得知道这些结论是怎么来的。也就是他们的实验细节。这能帮我们在自己做实验的时候少走弯路。我们挑几个部分来重点讲一下。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先，他们的实验设置非常严谨。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先是LoRA的秩。覆盖了1到512三个数量级。从最低的秩1。到能覆盖大部分场景的秩128、256。再到接近全量微调的秩512。这样能全面看到秩对性能的影响。然后是学习率（LR）。为了避免“因为学习率选得不好。导致LoRA性能差”的情况出现。他们对每个实验条件都做了“学习率扫描”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如对秩32的LoRA。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">测试0.0001到0.01的不同学习率。找到最佳值；。而且采用了“恒定学习率”，没有热身或冷却阶段。这样能够排除学习率调度的干扰。更准确对比LoRA和全量微调的差异。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_608.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">模型方面，他们用了两类主流模型。一类是Llama 3系列。另一类是Qwen3系列。还包括了混合专家（MoE）模型。因为MoE模型的激活效率高。现在很多大模型都用这种架构。所以加入MoE的实验，能让结论更全面。数据集方面。监督学习用了Tulu3和OpenThoughts3。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">Tulu3是艾维森（Ivison）等人在2024年提出的。侧重“指令遵循”，比如让模型根据用户指令生成邮件、写代码；。OpenThoughts3是古哈（Guha）等人在2025年提出的。侧重“推理”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如数学题、逻辑推理题。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这两个数据集在任务类型、数据结构上差异很大。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">强化学习实验则用了MATH数据集和GSM8K数据集。都是数学推理任务。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">以答案正确与否作为奖励信号。以便专注测试强化学习场景下的LoRA性能。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">先看“LoRA秩”的影响实验。他们在Tulu3和OpenThoughts3上训练了一个epoch。对每个数据集、每个模型大小。都扫描了不同的LoRA秩和学习率。结果发现，高秩LoRA，比如256、512。和全量微调的学习曲线几乎重合。损失都是“随步骤的对数线性下降”，也就是说。每训练10倍的步骤。损失就会按固定比例下降。这和全量微调的规律完全一样。而低秩LoRA，比如1、4。在训练初期还能跟上。但是到了某个步骤阈值后。就会“偏离最小损失曲线”。这就是之前说的“容量耗尽”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_707.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">低秩矩阵存不下更多的任务信息。所以学习速度变慢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">更重要的是“学习率”的发现。他们绘制了“学习率和最终损失”的曲线。发现高秩LoRA的最小损失和全量微调几乎一样。但是最佳学习率是全量微调的10倍。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如全量微调的最佳学习率是0.001。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">LoRA就需要0.01。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比德曼（Biderman）等人2024年的研究也发现了类似的10倍比例。这说明这个规律不是偶然的。而是LoRA的固有属性。而且不同秩的LoRA。最佳学习率很接近。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如秩4和秩512的最佳学习率差异不到2倍。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这为我们设置LoRA学习率提供了很大便利。不用频繁调整。按全量微调的10倍开始试就行。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">再看“批量大小效应”的实验。他们用了OpenThoughts3里一个1万样本的子集。测试不同批量大小的影响。结果很明显，当批量大小比较小。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_768.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如32的时候。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">LoRA和全量微调的性能差距很小。而且随着训练推进，差距还会缩小；。但当批量大小变大。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如256、512的时候。LoRA的损失会明显比全量微调高。而且这个差距会一直存在。不会随训练步骤消失。更关键的是。这个差距和LoRA的秩无关。就算用秩512的LoRA。大批次下还是比全量微调差。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">他们分析。这是因为LoRA的“矩阵乘积（BA）”参数化。和原权重矩阵（W）的优化动态不同。批量一大。原矩阵能更好地利用批次信息更新。而BA矩阵的更新效率会下降。导致性能差距。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">不过好在，不管是LoRA还是全量微调。都是“小批量下性能更好”，所以实际应用里。只要别用太大的批量。这个问题就能够缓解。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">接下来。我们来重点说一说跟前面第4点发现。“LoRA该应用到哪些层”有关的实验过程。这是很多人调参时容易踩坑的地方。之前很多人习惯只把LoRA用到注意力层。觉得注意力层负责“理解上下文”，所以是关键，但是实验结果刚好相反。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_838.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">约翰·舒尔曼团队做了三组对比实验。第一组只在注意力层用LoRA。第二组只在MLP层用LoRA。第三组在所有层。也就是注意力+MLP+MoE都用了LoRA。结果发现。仅注意力层的LoRA性能最差。就算用更高的秩。参数数量和仅MLP层的秩128差不多。性能还是差一截。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如在Llama-3.1-8B模型上，仅MLP层。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">秩128，0.24B参数的损失。比仅注意力层。秩256，0.25B参数低了0.15左右。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">要知道，在语言模型里。损失降低0.1已经是很明显的提升了。而“所有层用LoRA”和“仅MLP层用LoRA”的性能几乎一样。这说明MLP层才是LoRA发挥作用的核心。注意力层加不加LoRA。对最终性能影响不大。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">他们还在MoE模型上做了实验。对Qwen3-30B-A3B-Base的每个专家都单独训练LoRA。秩设为“总秩除以活跃专家数量”，这样能保证MoE层的LoRA参数比例和其他层一致。结果和稠密模型一样。仅MLP层的LoRA优于仅注意力层。所有层和仅MLP层相当。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_918.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">为什么会这样？这就要用到马拉迪（Malladi）等人在2022年提出的“经验神经正切核（eNTK）”理论了。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">eNTK是用来描述模型微调初期学习动态的工具。它的核心是“梯度的点积”。比如对每个Token的预测。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">梯度g_i是损失对参数的偏导。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">eNTK的核函数K(i。j)就是g_i和g_j的点积。这个核函数决定了模型怎么“学习”数据里的模式。而参数越多的层。对核函数的影响越大。马拉迪等人的研究指出。当LoRA应用到所有层的时候。它的eNTK和全量微调的eNTK几乎一样。所以学习动态也相似；。但是如果只应用到注意力层。而忽略了参数更多的MLP层。eNTK就会和全量微调有明显差异。学习速度自然慢下来。这也解释了为什么仅注意力层的LoRA效果差。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">聊完实验。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">我们最后再来总结一下这篇研究的主要结论和背后的深层意义。它不止告诉了我们LoRA怎么用。还帮我们深化了对大模型微调的理解。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_986.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先是“LoRA匹配全量微调的两个核心条件”，一是要应用到所有层。尤其是MLP/MoE层。这样才能保证eNTK和全量微调的一致；。二是LoRA的容量要能覆盖数据集的信息需求。也就是“非容量约束”。只要满足这两个条件。LoRA就能和全量微调性能相当。这为我们选择微调方法提供了明确的判断标准。如果你的数据集是中小规模。模型有MLP层。就用LoRA；。如果数据集特别大。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如接近预训练规模。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">再考虑全量微调。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">其次是“监督学习和RL的容量需求的差异”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">监督学习需要的信息多。每个token 1比特。所以需要更高秩的LoRA；。强化学习需要的信息少，每轮次1比特。低秩LoRA就够。这个差异能帮我们节省资源。也就是做强化学习任务时。不用追求高秩。秩1、8就够，参数更少，训练更快。然后是“LoRA的计算效率优势”，他们算了一笔账。LoRA每轮训练的FLOPs。是全量微调的2/3左右。具体怎么算的大家可以去看一下这一段。这里我们就不一一推导了。简单来说，对于一个N×N的权重矩阵W。LoRA的总FLOPs是2N²。全量微调是3N²，所以比例就是2/3。这意味着，同样的硬件。LoRA能比全量微调多训练50%的样本。计算效率更高。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_1079.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">当然，研究也提出了一些“开放问题”，这些也是未来值得关注的方向。比如怎么更精准地预测LoRA的性能。不用每次都做实验；。为什么LoRA的最佳学习率是全量微调的10倍。目前还没有完整的理论解释；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">LoRA的变体在这种评估方法下表现如何；。还有MoE层的LoRA。怎么和张量并行、专家并行这些技术结合。适配更大的模型，等等等等。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">最后，约翰·舒尔曼团队在结语里提到。他们研究LoRA的目标。不只是为了省资源。更是为了“让大模型的微调能力更易获取”，毕竟不是每个团队都有能力做全量微调。但是LoRA能让普通团队也能用大模型来解决具体的问题。而且通过研究LoRA。他们也深化了对“模型容量”、“数据集复杂度”、“样本效率”这些基础问题的理解。这其实也是AI研究的魅力。一个实用的技术。背后往往藏着对核心原理的新认知。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_1140.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">好了，今天的内容就到这里。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">如果你在做LoRA相关的工作。希望今天的拆解能帮你少走弯路；。如果你只是感兴趣。也希望能够帮助你增加对LoRA的了解。感谢大家观看本期视频。我们下期再见。</p>

<hr style="clear:both" />
=============
<p><a href="https://www.youtube.com/watch?v=MlqXL6o--2M">https://www.youtube.com/watch?v=MlqXL6o--2M</a></p><p>Thinking Machines Lab最近发布的一篇博客，文章的标题是《LoRA无悔（LoRA Without Regret）》，作者是约翰·舒尔曼（John Schulman）和他的团队。文章不仅回答了“LoRA能不能媲美全量微调”的问题，还给出了具体的操作条件，甚至连超参数怎么设都讲清楚了。不管你是想入门LoRA，还是已经在落地中遇到问题，这篇内容都值得你仔细阅读。</p><p><a href="https://thinkingmachines.ai/blog/lora/">https://thinkingmachines.ai/blog/lora/</a></p>]]>
      </itunes:summary>
      <description>
        <![CDATA[<hr style="clear:both" />

<p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://img.youtube.com/vi/MlqXL6o--2M/maxresdefault.jpg" /></p><p><a href="https://www.youtube.com/watch?v=MlqXL6o--2M">https://www.youtube.com/watch?v=MlqXL6o--2M</a></p><h1>值得閱讀的理由</h1> <ul> <li>深入理解<strong>LoRA</strong>與<strong>全量微調（FullFT）</strong>在資源消耗與性能表現之間的平衡，為大模型微調提供決策依據。</li> <li>掌握<strong>約翰·舒爾曼（John Schulman）</strong>團隊提出的關鍵條件和最佳實踐，有效提升LoRA的微調效果。</li> <li>探索LoRA在監督學習與強化學習場景下的應用差異，以及未來大模型微調研究的開放性問題。</li> </ul> <hr /> <h1>摘要</h1> <p>本影片由影片的講者深入探討了當今大模型微調領域中一個核心問題：究竟該選擇<strong>低秩適應（LoRA）</strong>還是<strong>全量微調（FullFT）</strong>？這個問題的本質在於<strong>「資源」</strong>與<strong>「效果」</strong>的權衡。影片指出，儘管全量微調能最大化模型性能，但其對硬體資源的龐大需求（如儲存原始權重、梯度、優化器動量，且需使用FP32精度）使得一般團隊難以承受。相較之下，LoRA作為一種參數高效微調的主流方法，以其節省記憶體和算力的優勢吸引了廣泛關注，但其性能能否媲美全量微調一直是業界普遍的疑慮。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_90.jpg" /></p> <p>影片接著介紹了由<strong>約翰·舒爾曼（John Schulman）及其團隊</strong>發表的深度分析文章《LoRA無悔（LoRA Without Regret）》，該文章不僅回應了LoRA能否匹敵全量微調的疑問，更提出了具體的操作條件和超參數設定建議。影片強調，理解LoRA的重要性源於當前主流大型語言模型（如Llama 3、Qwen3）龐大的參數規模和預訓練資料量。在模型的「訓練後處理」階段，由於資料集規模變小且關注領域變窄，全量微調顯得過於浪費，因此催生了<strong>參數高效微調（PEFT）</strong>的發展，而LoRA正是其中最成熟的方法。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_180.jpg" /></p> <h2>LoRA的核心機制與優勢</h2> <p>影片詳細解釋了LoRA的核心思路：它不直接修改原模型的權重矩陣W，而是添加一個「低秩增量項」<strong>γBA</strong>。這裡的B和A是兩個低秩矩陣，它們的參數數量遠少於原始權重矩陣W，卻能「模擬」出原模型權重的更新效果。這種機制不僅實現了<strong>參數高效性</strong>，還帶來了多項實際優勢。首先是<strong>「多租戶服務」</strong>，由於LoRA只訓練輕量級的「適配器」，一個推理伺服器能夠同時載入多個適配器以處理不同任務，顯著節省硬體成本。其次是<strong>「訓練佈局優化」</strong>，LoRA更新的參數少，記憶體佔用小，使得訓練所需的硬體資源大幅降低，普通團隊也能用少量GPU進行訓練。最後是<strong>「載入傳輸方便」</strong>，LoRA適配器檔案體積小（僅幾MB到幾十MB），便於機器間傳輸和模型載入，提高了營運彈性。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_300.jpg" /></p> <h2>關鍵實驗發現與「低遺憾區間」</h2> <p>影片闡述了約翰·舒爾曼團隊實驗的核心目標：回答LoRA能否匹配全量微調的性能，以及需要滿足哪些條件。他們採取的實驗設計具有兩個顯著特點：一是<strong>「不聚焦特定任務，而是研究通用關係」</strong>，探討「訓練集大小」與「LoRA參數數量」之間的通用規律；二是採用<strong>「對數損失（log loss）評估」</strong>，能客觀反映模型的預測誤差和訓練表現。透過嚴謹的實驗設計，團隊得出了五個關鍵發現：</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_450.jpg" /></p> <p>1.  在中小規模的指令微調或推理資料集上，只要不超過LoRA的容量，<strong>LoRA與全量微調的性能幾乎完全一致</strong>。 2.  若資料集規模超出LoRA容量，LoRA會表現較差，但並非無法下降，而是<strong>「訓練效率變慢」</strong>。 3.  LoRA對<strong>「批量大小（batch size）」</strong>的容忍度低於全量微調，批次過大會導致LoRA性能明顯下降，且增大LoRA的秩也無法解決此問題。 4.  LoRA必須應用於模型<strong>「所有層」</strong>，尤其是<strong>MLP層或MoE層</strong>，效果才最佳；僅在注意力層使用LoRA的性能表現不佳。 5.  在<strong>強化學習（RL）</strong>場景中，即使使用<strong>極低秩的LoRA</strong>也能與全量微調性能一致，這歸因於強化學習每輪所需的資訊量較少（O(1)比特）。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_705.jpg" /></p> <p>基於這些發現，研究團隊提出了<strong>「低遺憾區間（low-regret regime）」</strong>的概念：只要LoRA的參數容量能覆蓋資料集所需的資訊，並且應用到所有層，就能與全量微調性能相當。這意味著在大多數企業的訓練後處理場景中，LoRA完全可以替代全量微調。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_750.jpg" /></p> <h2>實驗細節剖析</h2> <p>影片進一步深入探討了約翰·舒爾曼團隊的實驗細節，為實踐者提供了寶貴的參考。他們測試了從<strong>秩1到512</strong>的三個數量級LoRA秩，並對每個實驗條件進行了<strong>「學習率掃描」</strong>以找到最佳值，同時採用<strong>「恆定學習率」</strong>排除調度干擾。實驗模型涵蓋了<strong>Llama 3系列</strong>和<strong>Qwen3系列</strong>，以及<strong>混合專家（MoE）模型</strong>。資料集則選用了監督學習的<strong>Tulu3</strong>和<strong>OpenThoughts3</strong>，以及強化學習的<strong>MATH</strong>和<strong>GSM8K</strong>，確保了實驗的廣泛性和代表性。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_880.jpg" /></p> <p>關於<strong>LoRA秩的影響</strong>，高秩LoRA（如256、512）與全量微調的學習曲線幾乎重合，損失呈現「隨步驟的對數線性下降」；低秩LoRA（如1、4）在訓練初期尚可，但達到閾值後便會因「容量耗盡」而偏離最佳損失曲線。在<strong>學習率</strong>方面，研究發現LoRA的最佳學習率約為全量微調的10倍，且不同秩的LoRA之間最佳學習率差異不大，這為設定LoRA學習率提供了便利。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_990.jpg" /></p> <p><strong>批量大小效應</strong>實驗揭示，當批量大小較小（如32）時，LoRA與全量微調性能差距微小；但當批量大小增大（如256、512）時，LoRA的損失會顯著高於全量微調，且這種差距與LoRA的秩無關。這歸因於LoRA的「矩陣乘積參數化（BA）」與原權重矩陣（W）的優化動態不同，大批量會放大這種差異。幸運的是，在實際應用中，只要避免使用過大的批量，此問題便可緩解。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_1070.jpg" /></p> <p>影片特別強調了<strong>LoRA應該應用到哪些層</strong>的實驗結果。研究團隊通過三組對比實驗（僅注意力層、僅MLP層、所有層）發現，僅在注意力層使用LoRA的性能最差，而<strong>MLP層才是LoRA發揮作用的核心</strong>；將LoRA應用於所有層與僅MLP層的性能幾乎一致。這個發現與<strong>經驗神經正切核（eNTK）理論</strong>相符：參數越多的層對核函數影響越大，應用於所有層能使LoRA的eNTK與全量微調的eNTK保持一致，從而實現相似的學習動態。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_20.jpg" /></p> <h2>結論與深層意義</h2> <p>影片總結了該研究的深層意義。首先，LoRA匹配全量微調的核心條件有二：一是<strong>必須應用到所有層（尤其MLP/MoE層）</strong>，確保eNTK的一致性；二是<strong>LoRA的容量需能覆蓋資料集所需的資訊</strong>。其次，<strong>監督學習與強化學習對LoRA容量的需求存在顯著差異</strong>：監督學習需要更多資訊，適合高秩LoRA；強化學習每輪所需資訊少，低秩LoRA便足夠。這能幫助開發者在強化學習任務中節省資源，無需追求高秩。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_22.jpg" /></p> <p>研究還指出LoRA的<strong>計算效率優勢</strong>，每輪訓練的FLOPs約為全量微調的2/3，意味著在相同硬體下LoRA能多訓練50%的樣本。儘管如此，研究也提出了一些<strong>「開放問題」</strong>，例如如何更精準地預測LoRA性能、LoRA最佳學習率是全量微調10倍的理論解釋、LoRA變體的表現，以及LoE層LoRA如何與張量並行等技術結合以適配更大模型等，這些都是未來值得關注的研究方向。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_24.jpg" /></p> <p>最後，約翰·舒爾曼團隊的研究目標不僅是為了節省資源，更是為了<strong>「讓大模型的微調能力更易獲取」</strong>，使普通團隊也能利用大模型解決具體問題。透過對LoRA的深入研究，他們也深化了對「模型容量」、「資料集複雜度」、「樣本效率」等基礎問題的理解，展現了AI研究中實用技術背後蘊藏的核心原理新認知。</p> <hr /><hr />================================================</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">大家好，这里是最佳拍档，我是大飞。现在微调大模型。到底应该用LoRA还是全量微调呢？其实这个问题的核心。本质是“资源”和“效果”的平衡。全量微调（FullFT）虽然能把模型性能拉满。但是太“吃资源”了。一个万亿参数的模型。全量微调时不仅要存原权重。还要存梯度、优化器动量。而且这些数据得用FP32精度。比推理时的BF16多占一倍空间。普通团队根本扛不住硬件成本。而LoRA作为参数高效微调的主流方法。确实省内存、省算力。但是大家总是感觉没底。它的效果到底能不能跟全量微调比？会不会调了半天，性能差一截呢？而Thinking Machines Lab最近发布的一篇博客。恰恰把这个问题给讲透了。文章的标题是《LoRA无悔（LoRA Without Regret）》，作者是约翰·舒尔曼（John Schulman）和他的团队。文章不仅回答了“LoRA能不能媲美全量微调”的问题。还给出了具体的操作条件。甚至连超参数怎么设都讲清楚了。不管你是想入门LoRA。还是已经在落地中遇到问题。这篇内容都值得你仔细阅读。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_75.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先，我们得先搞清楚。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">为什么现在大家这么需要LoRA？这还得从大模型的现状说起。现在主流的语言模型。比如Llama 3、Qwen3。参数都在上万亿。预训练时用了数万亿个Token。这么大的规模。就是为了让模型学到人类书面知识里的所有模式。基础性能才能上去。但是到了“训练后处理”阶段。情况就发生了变化。不仅数据集变小了。关注的领域也变窄了。这时候如果还用全量微调。这就像用一辆卡车去拉一颗鸡蛋。太浪费了。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">就是因为这个“浪费”的问题。参数高效微调（PEFT）才发展起来。而低秩适应LoRA就是其中最成熟的方法。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">LoRA的核心思路很巧妙。它不直接改原模型的权重矩阵W。而是给W加了一个“低秩增量项”，公式是W' = W + γBA。这里的W'是微调后的权重。B和A是两个低秩矩阵。它们加起来的参数数量。比原矩阵W少得多。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_141.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">γ是个常数缩放因子。用来调整增量项的大小。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">简单说，LoRA就是用两个小矩阵。“模拟”出原模型权重的更新效果。这样既不用动原模型的海量参数。又能让模型适配新任务。这就是它“参数高效”的关键。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而且LoRA不止省参数。还有三个很实际的优势。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第一个是“多租户服务（Multi-tenant serving）”，因为LoRA只训练A和B这两个“适配器”，原模型权重不动。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">所以一个推理服务器可以在内存里存多个适配器。对应不同的任务。比如一个适配器处理客服对话。另一个处理产品摘要生成。还能批量处理这些请求。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">像陈（Chen）等人2023年提出的Punica框架。还有现在常用的vLLM、SGLang这些推理引擎。都已经支持这个功能。对企业来说能省不少硬件钱。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第二个优势是“训练布局优化”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">全量微调的时候。硬件布局得专门设计。你想，除了原权重。还要存梯度、优化器动量，精度还高。需要的加速器数量往往是推理时的10倍以上。但LoRA要更新的参数少。占用内存也少。训练时的硬件布局只比推理时稍微大一点。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_220.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">普通团队用几台GPU就能跑。门槛低多了。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第三个优势是“加载传输方便”，适配器矩阵的参数少。文件体积小。比如一个秩为128的LoRA适配器。可能就几MB或几十MB。不管是在机器间传。还是加载到模型里。都比传整个模型快得多，运维也灵活。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">正是这些优势。LoRA从2021年胡（Hu）等人发表的第一篇论文（《LoRA:。Low-Rank Adaptation of Large Language Models》）后。就越来越火。但是问题也来了，现有的研究里。LoRA和全量微调的性能对比一直不明确。大家公认的是。在“类似预训练”的场景里。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如数据集特别大。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">超过LoRA的参数存储极限的时候。LoRA性能会差一些；。但是在训练后处理的常见数据集规模下。LoRA虽然容量足够存下关键信息。可是没人能保证它的“样本效率”和“计算效率”，能跟全量微调相比。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">所以约翰·舒尔曼团队的实验。核心就是回答了。LoRA到底能不能匹配全量微调的性能？如果能，需要满足哪些条件呢？</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_289.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">他们的实验结论很明确。只要把几个关键细节做对。LoRA的样本效率和最终性能。就能和全量微调完全一致。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">那这些“关键细节”到底是什么？我们得先从他们的实验设计说起。因为好的实验设计。是结论可信的基础。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">和之前的LoRA研究相比。他们的实验有两个很重要的特点。第一个是“不盯着具体任务。而是研究通用关系”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">很多研究只会选一两个数据集。比如只测情感分析。或者只测文本摘要，但是他们不一样。专门研究“训练集大小”和“LoRA参数数量”之间的通用规律。这样得出的结论。不管你是做医疗还是教育任务。都能参考。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第二个是采用了“对数损失（log loss）评估”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">之前很多研究采用的是“基于采样的评估”，比如让模型生成文本，再人工打分。但是这种方法主观性强。不同任务里标准还不一样。而对数损失能直接反映模型的预测误差。而且能够清晰地看到不同训练步骤、不同参数设置下的模型表现规律。通用性强得多。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_357.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">就是靠着这样的实验设计。他们得出了五个关键发现。这些发现也是LoRA能够媲美全量微调的核心条件。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第一个发现。在中小规模的指令微调或者推理数据集上。LoRA和全量微调的性能完全一样。比如他们用的Tulu3和OpenThoughts3数据集。只要规模没超过LoRA的容量。两者的损失曲线、收敛速度都几乎重合。这说明在大多数企业的落地场景里。LoRA完全够用。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第二个发现。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">如果数据集规模超过了LoRA的容量。LoRA就会比全量微调差。但不是“损失降到一定程度就降不下去”了。而是“训练效率变慢”了。具体会慢多少。要取决于LoRA的参数容量和数据集大小的比例。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如一个秩为32的LoRA。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">处理100万Token的数据集没问题。但是如果数据集涨到1亿个Token。它的训练效率就会明显下降。损失下降速度变慢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_419.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第三个发现。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">LoRA对“批量大小（batch size）”的容忍度比全量微调低。如果批量 size 太大。超过某个阈值后。LoRA的性能损失就会比全量微调大得多。而且这个问题。就算增大LoRA的秩也解决不了。因为这是LoRA“矩阵乘积参数化（BA）”的固有属性。它的优化动态和原权重矩阵（W）不一样。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">批量一大，这种动态差异就会被放大。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第四个发现。LoRA一定要应用到模型的所有层。尤其是MLP层或MoE层，效果才好。之前很多研究受到第一篇LoRA论文的影响。只把LoRA用到注意力层。但是约翰·舒尔曼团队发现。仅在注意力层使用LoRA的性能很差。而MLP层才是LoRA发挥作用的关键。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比德曼（Biderman）等人2024年的研究（《LoRA Learns Less and Forgets Less》）也得出了类似结论。进一步验证了这个发现。第五个发现，在强化学习的场景里。就算用很低秩的LoRA。也能和全量微调性能一致。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_482.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">他们用数学推理任务做实验。比如让模型去解初中数学题。以答案正确性为奖励，结果发现。哪怕LoRA的秩只有1。最终的准确率也和全量微调一样。这背后有个信息论的解释。那就是监督学习里。每一轮训练能给模型提供“O(Token数量)”个比特的信息。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如一句话有100个Token。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">就能提供100比特左右的信息；。但是强化学习不一样。政策梯度算法的学习。靠的是“优势函数（Advantage Function）”，每一轮只能提供“O(1)”比特的信息。简单来说。就是强化学习需要学的信息少。就算是低秩LoRA，容量也完全够。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">基于这些发现。他们提出了一个“低遗憾区间（low-regret regime）”的概念。只要LoRA的参数容量能覆盖数据集的信息需求。并且应用到所有层。就能和全量微调性能相当。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而这个区间。刚好覆盖了大多数企业的训练后处理场景。这也意味着，LoRA在很多实际应用里。完全可以替代全量微调。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_547.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">当然，光有结论不够。我们还得知道这些结论是怎么来的。也就是他们的实验细节。这能帮我们在自己做实验的时候少走弯路。我们挑几个部分来重点讲一下。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先，他们的实验设置非常严谨。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先是LoRA的秩。覆盖了1到512三个数量级。从最低的秩1。到能覆盖大部分场景的秩128、256。再到接近全量微调的秩512。这样能全面看到秩对性能的影响。然后是学习率（LR）。为了避免“因为学习率选得不好。导致LoRA性能差”的情况出现。他们对每个实验条件都做了“学习率扫描”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如对秩32的LoRA。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">测试0.0001到0.01的不同学习率。找到最佳值；。而且采用了“恒定学习率”，没有热身或冷却阶段。这样能够排除学习率调度的干扰。更准确对比LoRA和全量微调的差异。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_608.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">模型方面，他们用了两类主流模型。一类是Llama 3系列。另一类是Qwen3系列。还包括了混合专家（MoE）模型。因为MoE模型的激活效率高。现在很多大模型都用这种架构。所以加入MoE的实验，能让结论更全面。数据集方面。监督学习用了Tulu3和OpenThoughts3。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">Tulu3是艾维森（Ivison）等人在2024年提出的。侧重“指令遵循”，比如让模型根据用户指令生成邮件、写代码；。OpenThoughts3是古哈（Guha）等人在2025年提出的。侧重“推理”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如数学题、逻辑推理题。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这两个数据集在任务类型、数据结构上差异很大。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">强化学习实验则用了MATH数据集和GSM8K数据集。都是数学推理任务。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">以答案正确与否作为奖励信号。以便专注测试强化学习场景下的LoRA性能。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">先看“LoRA秩”的影响实验。他们在Tulu3和OpenThoughts3上训练了一个epoch。对每个数据集、每个模型大小。都扫描了不同的LoRA秩和学习率。结果发现，高秩LoRA，比如256、512。和全量微调的学习曲线几乎重合。损失都是“随步骤的对数线性下降”，也就是说。每训练10倍的步骤。损失就会按固定比例下降。这和全量微调的规律完全一样。而低秩LoRA，比如1、4。在训练初期还能跟上。但是到了某个步骤阈值后。就会“偏离最小损失曲线”。这就是之前说的“容量耗尽”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_707.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">低秩矩阵存不下更多的任务信息。所以学习速度变慢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">更重要的是“学习率”的发现。他们绘制了“学习率和最终损失”的曲线。发现高秩LoRA的最小损失和全量微调几乎一样。但是最佳学习率是全量微调的10倍。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如全量微调的最佳学习率是0.001。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">LoRA就需要0.01。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比德曼（Biderman）等人2024年的研究也发现了类似的10倍比例。这说明这个规律不是偶然的。而是LoRA的固有属性。而且不同秩的LoRA。最佳学习率很接近。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如秩4和秩512的最佳学习率差异不到2倍。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这为我们设置LoRA学习率提供了很大便利。不用频繁调整。按全量微调的10倍开始试就行。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">再看“批量大小效应”的实验。他们用了OpenThoughts3里一个1万样本的子集。测试不同批量大小的影响。结果很明显，当批量大小比较小。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_768.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如32的时候。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">LoRA和全量微调的性能差距很小。而且随着训练推进，差距还会缩小；。但当批量大小变大。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如256、512的时候。LoRA的损失会明显比全量微调高。而且这个差距会一直存在。不会随训练步骤消失。更关键的是。这个差距和LoRA的秩无关。就算用秩512的LoRA。大批次下还是比全量微调差。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">他们分析。这是因为LoRA的“矩阵乘积（BA）”参数化。和原权重矩阵（W）的优化动态不同。批量一大。原矩阵能更好地利用批次信息更新。而BA矩阵的更新效率会下降。导致性能差距。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">不过好在，不管是LoRA还是全量微调。都是“小批量下性能更好”，所以实际应用里。只要别用太大的批量。这个问题就能够缓解。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">接下来。我们来重点说一说跟前面第4点发现。“LoRA该应用到哪些层”有关的实验过程。这是很多人调参时容易踩坑的地方。之前很多人习惯只把LoRA用到注意力层。觉得注意力层负责“理解上下文”，所以是关键，但是实验结果刚好相反。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_838.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">约翰·舒尔曼团队做了三组对比实验。第一组只在注意力层用LoRA。第二组只在MLP层用LoRA。第三组在所有层。也就是注意力+MLP+MoE都用了LoRA。结果发现。仅注意力层的LoRA性能最差。就算用更高的秩。参数数量和仅MLP层的秩128差不多。性能还是差一截。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如在Llama-3.1-8B模型上，仅MLP层。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">秩128，0.24B参数的损失。比仅注意力层。秩256，0.25B参数低了0.15左右。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">要知道，在语言模型里。损失降低0.1已经是很明显的提升了。而“所有层用LoRA”和“仅MLP层用LoRA”的性能几乎一样。这说明MLP层才是LoRA发挥作用的核心。注意力层加不加LoRA。对最终性能影响不大。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">他们还在MoE模型上做了实验。对Qwen3-30B-A3B-Base的每个专家都单独训练LoRA。秩设为“总秩除以活跃专家数量”，这样能保证MoE层的LoRA参数比例和其他层一致。结果和稠密模型一样。仅MLP层的LoRA优于仅注意力层。所有层和仅MLP层相当。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_918.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">为什么会这样？这就要用到马拉迪（Malladi）等人在2022年提出的“经验神经正切核（eNTK）”理论了。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">eNTK是用来描述模型微调初期学习动态的工具。它的核心是“梯度的点积”。比如对每个Token的预测。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">梯度g_i是损失对参数的偏导。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">eNTK的核函数K(i。j)就是g_i和g_j的点积。这个核函数决定了模型怎么“学习”数据里的模式。而参数越多的层。对核函数的影响越大。马拉迪等人的研究指出。当LoRA应用到所有层的时候。它的eNTK和全量微调的eNTK几乎一样。所以学习动态也相似；。但是如果只应用到注意力层。而忽略了参数更多的MLP层。eNTK就会和全量微调有明显差异。学习速度自然慢下来。这也解释了为什么仅注意力层的LoRA效果差。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">聊完实验。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">我们最后再来总结一下这篇研究的主要结论和背后的深层意义。它不止告诉了我们LoRA怎么用。还帮我们深化了对大模型微调的理解。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_986.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先是“LoRA匹配全量微调的两个核心条件”，一是要应用到所有层。尤其是MLP/MoE层。这样才能保证eNTK和全量微调的一致；。二是LoRA的容量要能覆盖数据集的信息需求。也就是“非容量约束”。只要满足这两个条件。LoRA就能和全量微调性能相当。这为我们选择微调方法提供了明确的判断标准。如果你的数据集是中小规模。模型有MLP层。就用LoRA；。如果数据集特别大。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如接近预训练规模。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">再考虑全量微调。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">其次是“监督学习和RL的容量需求的差异”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">监督学习需要的信息多。每个token 1比特。所以需要更高秩的LoRA；。强化学习需要的信息少，每轮次1比特。低秩LoRA就够。这个差异能帮我们节省资源。也就是做强化学习任务时。不用追求高秩。秩1、8就够，参数更少，训练更快。然后是“LoRA的计算效率优势”，他们算了一笔账。LoRA每轮训练的FLOPs。是全量微调的2/3左右。具体怎么算的大家可以去看一下这一段。这里我们就不一一推导了。简单来说，对于一个N×N的权重矩阵W。LoRA的总FLOPs是2N²。全量微调是3N²，所以比例就是2/3。这意味着，同样的硬件。LoRA能比全量微调多训练50%的样本。计算效率更高。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_1079.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">当然，研究也提出了一些“开放问题”，这些也是未来值得关注的方向。比如怎么更精准地预测LoRA的性能。不用每次都做实验；。为什么LoRA的最佳学习率是全量微调的10倍。目前还没有完整的理论解释；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">LoRA的变体在这种评估方法下表现如何；。还有MoE层的LoRA。怎么和张量并行、专家并行这些技术结合。适配更大的模型，等等等等。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">最后，约翰·舒尔曼团队在结语里提到。他们研究LoRA的目标。不只是为了省资源。更是为了“让大模型的微调能力更易获取”，毕竟不是每个团队都有能力做全量微调。但是LoRA能让普通团队也能用大模型来解决具体的问题。而且通过研究LoRA。他们也深化了对“模型容量”、“数据集复杂度”、“样本效率”这些基础问题的理解。这其实也是AI研究的魅力。一个实用的技术。背后往往藏着对核心原理的新认知。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_1140.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">好了，今天的内容就到这里。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">如果你在做LoRA相关的工作。希望今天的拆解能帮你少走弯路；。如果你只是感兴趣。也希望能够帮助你增加对LoRA的了解。感谢大家观看本期视频。我们下期再见。</p>

<hr style="clear:both" />
=============
<p><a href="https://www.youtube.com/watch?v=MlqXL6o--2M">https://www.youtube.com/watch?v=MlqXL6o--2M</a></p><p>Thinking Machines Lab最近发布的一篇博客，文章的标题是《LoRA无悔（LoRA Without Regret）》，作者是约翰·舒尔曼（John Schulman）和他的团队。文章不仅回答了“LoRA能不能媲美全量微调”的问题，还给出了具体的操作条件，甚至连超参数怎么设都讲清楚了。不管你是想入门LoRA，还是已经在落地中遇到问题，这篇内容都值得你仔细阅读。</p><p><a href="https://thinkingmachines.ai/blog/lora/">https://thinkingmachines.ai/blog/lora/</a></p>]]>
      </description>
      <content:encoded><![CDATA[<hr style="clear:both" />

<p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://img.youtube.com/vi/MlqXL6o--2M/maxresdefault.jpg" /></p><p><a href="https://www.youtube.com/watch?v=MlqXL6o--2M">https://www.youtube.com/watch?v=MlqXL6o--2M</a></p><h1>值得閱讀的理由</h1> <ul> <li>深入理解<strong>LoRA</strong>與<strong>全量微調（FullFT）</strong>在資源消耗與性能表現之間的平衡，為大模型微調提供決策依據。</li> <li>掌握<strong>約翰·舒爾曼（John Schulman）</strong>團隊提出的關鍵條件和最佳實踐，有效提升LoRA的微調效果。</li> <li>探索LoRA在監督學習與強化學習場景下的應用差異，以及未來大模型微調研究的開放性問題。</li> </ul> <hr /> <h1>摘要</h1> <p>本影片由影片的講者深入探討了當今大模型微調領域中一個核心問題：究竟該選擇<strong>低秩適應（LoRA）</strong>還是<strong>全量微調（FullFT）</strong>？這個問題的本質在於<strong>「資源」</strong>與<strong>「效果」</strong>的權衡。影片指出，儘管全量微調能最大化模型性能，但其對硬體資源的龐大需求（如儲存原始權重、梯度、優化器動量，且需使用FP32精度）使得一般團隊難以承受。相較之下，LoRA作為一種參數高效微調的主流方法，以其節省記憶體和算力的優勢吸引了廣泛關注，但其性能能否媲美全量微調一直是業界普遍的疑慮。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_90.jpg" /></p> <p>影片接著介紹了由<strong>約翰·舒爾曼（John Schulman）及其團隊</strong>發表的深度分析文章《LoRA無悔（LoRA Without Regret）》，該文章不僅回應了LoRA能否匹敵全量微調的疑問，更提出了具體的操作條件和超參數設定建議。影片強調，理解LoRA的重要性源於當前主流大型語言模型（如Llama 3、Qwen3）龐大的參數規模和預訓練資料量。在模型的「訓練後處理」階段，由於資料集規模變小且關注領域變窄，全量微調顯得過於浪費，因此催生了<strong>參數高效微調（PEFT）</strong>的發展，而LoRA正是其中最成熟的方法。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_180.jpg" /></p> <h2>LoRA的核心機制與優勢</h2> <p>影片詳細解釋了LoRA的核心思路：它不直接修改原模型的權重矩陣W，而是添加一個「低秩增量項」<strong>γBA</strong>。這裡的B和A是兩個低秩矩陣，它們的參數數量遠少於原始權重矩陣W，卻能「模擬」出原模型權重的更新效果。這種機制不僅實現了<strong>參數高效性</strong>，還帶來了多項實際優勢。首先是<strong>「多租戶服務」</strong>，由於LoRA只訓練輕量級的「適配器」，一個推理伺服器能夠同時載入多個適配器以處理不同任務，顯著節省硬體成本。其次是<strong>「訓練佈局優化」</strong>，LoRA更新的參數少，記憶體佔用小，使得訓練所需的硬體資源大幅降低，普通團隊也能用少量GPU進行訓練。最後是<strong>「載入傳輸方便」</strong>，LoRA適配器檔案體積小（僅幾MB到幾十MB），便於機器間傳輸和模型載入，提高了營運彈性。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_300.jpg" /></p> <h2>關鍵實驗發現與「低遺憾區間」</h2> <p>影片闡述了約翰·舒爾曼團隊實驗的核心目標：回答LoRA能否匹配全量微調的性能，以及需要滿足哪些條件。他們採取的實驗設計具有兩個顯著特點：一是<strong>「不聚焦特定任務，而是研究通用關係」</strong>，探討「訓練集大小」與「LoRA參數數量」之間的通用規律；二是採用<strong>「對數損失（log loss）評估」</strong>，能客觀反映模型的預測誤差和訓練表現。透過嚴謹的實驗設計，團隊得出了五個關鍵發現：</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_450.jpg" /></p> <p>1.  在中小規模的指令微調或推理資料集上，只要不超過LoRA的容量，<strong>LoRA與全量微調的性能幾乎完全一致</strong>。 2.  若資料集規模超出LoRA容量，LoRA會表現較差，但並非無法下降，而是<strong>「訓練效率變慢」</strong>。 3.  LoRA對<strong>「批量大小（batch size）」</strong>的容忍度低於全量微調，批次過大會導致LoRA性能明顯下降，且增大LoRA的秩也無法解決此問題。 4.  LoRA必須應用於模型<strong>「所有層」</strong>，尤其是<strong>MLP層或MoE層</strong>，效果才最佳；僅在注意力層使用LoRA的性能表現不佳。 5.  在<strong>強化學習（RL）</strong>場景中，即使使用<strong>極低秩的LoRA</strong>也能與全量微調性能一致，這歸因於強化學習每輪所需的資訊量較少（O(1)比特）。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_705.jpg" /></p> <p>基於這些發現，研究團隊提出了<strong>「低遺憾區間（low-regret regime）」</strong>的概念：只要LoRA的參數容量能覆蓋資料集所需的資訊，並且應用到所有層，就能與全量微調性能相當。這意味著在大多數企業的訓練後處理場景中，LoRA完全可以替代全量微調。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_750.jpg" /></p> <h2>實驗細節剖析</h2> <p>影片進一步深入探討了約翰·舒爾曼團隊的實驗細節，為實踐者提供了寶貴的參考。他們測試了從<strong>秩1到512</strong>的三個數量級LoRA秩，並對每個實驗條件進行了<strong>「學習率掃描」</strong>以找到最佳值，同時採用<strong>「恆定學習率」</strong>排除調度干擾。實驗模型涵蓋了<strong>Llama 3系列</strong>和<strong>Qwen3系列</strong>，以及<strong>混合專家（MoE）模型</strong>。資料集則選用了監督學習的<strong>Tulu3</strong>和<strong>OpenThoughts3</strong>，以及強化學習的<strong>MATH</strong>和<strong>GSM8K</strong>，確保了實驗的廣泛性和代表性。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_880.jpg" /></p> <p>關於<strong>LoRA秩的影響</strong>，高秩LoRA（如256、512）與全量微調的學習曲線幾乎重合，損失呈現「隨步驟的對數線性下降」；低秩LoRA（如1、4）在訓練初期尚可，但達到閾值後便會因「容量耗盡」而偏離最佳損失曲線。在<strong>學習率</strong>方面，研究發現LoRA的最佳學習率約為全量微調的10倍，且不同秩的LoRA之間最佳學習率差異不大，這為設定LoRA學習率提供了便利。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_990.jpg" /></p> <p><strong>批量大小效應</strong>實驗揭示，當批量大小較小（如32）時，LoRA與全量微調性能差距微小；但當批量大小增大（如256、512）時，LoRA的損失會顯著高於全量微調，且這種差距與LoRA的秩無關。這歸因於LoRA的「矩陣乘積參數化（BA）」與原權重矩陣（W）的優化動態不同，大批量會放大這種差異。幸運的是，在實際應用中，只要避免使用過大的批量，此問題便可緩解。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_1070.jpg" /></p> <p>影片特別強調了<strong>LoRA應該應用到哪些層</strong>的實驗結果。研究團隊通過三組對比實驗（僅注意力層、僅MLP層、所有層）發現，僅在注意力層使用LoRA的性能最差，而<strong>MLP層才是LoRA發揮作用的核心</strong>；將LoRA應用於所有層與僅MLP層的性能幾乎一致。這個發現與<strong>經驗神經正切核（eNTK）理論</strong>相符：參數越多的層對核函數影響越大，應用於所有層能使LoRA的eNTK與全量微調的eNTK保持一致，從而實現相似的學習動態。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_20.jpg" /></p> <h2>結論與深層意義</h2> <p>影片總結了該研究的深層意義。首先，LoRA匹配全量微調的核心條件有二：一是<strong>必須應用到所有層（尤其MLP/MoE層）</strong>，確保eNTK的一致性；二是<strong>LoRA的容量需能覆蓋資料集所需的資訊</strong>。其次，<strong>監督學習與強化學習對LoRA容量的需求存在顯著差異</strong>：監督學習需要更多資訊，適合高秩LoRA；強化學習每輪所需資訊少，低秩LoRA便足夠。這能幫助開發者在強化學習任務中節省資源，無需追求高秩。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_22.jpg" /></p> <p>研究還指出LoRA的<strong>計算效率優勢</strong>，每輪訓練的FLOPs約為全量微調的2/3，意味著在相同硬體下LoRA能多訓練50%的樣本。儘管如此，研究也提出了一些<strong>「開放問題」</strong>，例如如何更精準地預測LoRA性能、LoRA最佳學習率是全量微調10倍的理論解釋、LoRA變體的表現，以及LoE層LoRA如何與張量並行等技術結合以適配更大模型等，這些都是未來值得關注的研究方向。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_24.jpg" /></p> <p>最後，約翰·舒爾曼團隊的研究目標不僅是為了節省資源，更是為了<strong>「讓大模型的微調能力更易獲取」</strong>，使普通團隊也能利用大模型解決具體問題。透過對LoRA的深入研究，他們也深化了對「模型容量」、「資料集複雜度」、「樣本效率」等基礎問題的理解，展現了AI研究中實用技術背後蘊藏的核心原理新認知。</p> <hr /><hr />================================================</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">大家好，这里是最佳拍档，我是大飞。现在微调大模型。到底应该用LoRA还是全量微调呢？其实这个问题的核心。本质是“资源”和“效果”的平衡。全量微调（FullFT）虽然能把模型性能拉满。但是太“吃资源”了。一个万亿参数的模型。全量微调时不仅要存原权重。还要存梯度、优化器动量。而且这些数据得用FP32精度。比推理时的BF16多占一倍空间。普通团队根本扛不住硬件成本。而LoRA作为参数高效微调的主流方法。确实省内存、省算力。但是大家总是感觉没底。它的效果到底能不能跟全量微调比？会不会调了半天，性能差一截呢？而Thinking Machines Lab最近发布的一篇博客。恰恰把这个问题给讲透了。文章的标题是《LoRA无悔（LoRA Without Regret）》，作者是约翰·舒尔曼（John Schulman）和他的团队。文章不仅回答了“LoRA能不能媲美全量微调”的问题。还给出了具体的操作条件。甚至连超参数怎么设都讲清楚了。不管你是想入门LoRA。还是已经在落地中遇到问题。这篇内容都值得你仔细阅读。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_75.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先，我们得先搞清楚。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">为什么现在大家这么需要LoRA？这还得从大模型的现状说起。现在主流的语言模型。比如Llama 3、Qwen3。参数都在上万亿。预训练时用了数万亿个Token。这么大的规模。就是为了让模型学到人类书面知识里的所有模式。基础性能才能上去。但是到了“训练后处理”阶段。情况就发生了变化。不仅数据集变小了。关注的领域也变窄了。这时候如果还用全量微调。这就像用一辆卡车去拉一颗鸡蛋。太浪费了。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">就是因为这个“浪费”的问题。参数高效微调（PEFT）才发展起来。而低秩适应LoRA就是其中最成熟的方法。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">LoRA的核心思路很巧妙。它不直接改原模型的权重矩阵W。而是给W加了一个“低秩增量项”，公式是W' = W + γBA。这里的W'是微调后的权重。B和A是两个低秩矩阵。它们加起来的参数数量。比原矩阵W少得多。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_141.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">γ是个常数缩放因子。用来调整增量项的大小。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">简单说，LoRA就是用两个小矩阵。“模拟”出原模型权重的更新效果。这样既不用动原模型的海量参数。又能让模型适配新任务。这就是它“参数高效”的关键。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而且LoRA不止省参数。还有三个很实际的优势。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第一个是“多租户服务（Multi-tenant serving）”，因为LoRA只训练A和B这两个“适配器”，原模型权重不动。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">所以一个推理服务器可以在内存里存多个适配器。对应不同的任务。比如一个适配器处理客服对话。另一个处理产品摘要生成。还能批量处理这些请求。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">像陈（Chen）等人2023年提出的Punica框架。还有现在常用的vLLM、SGLang这些推理引擎。都已经支持这个功能。对企业来说能省不少硬件钱。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第二个优势是“训练布局优化”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">全量微调的时候。硬件布局得专门设计。你想，除了原权重。还要存梯度、优化器动量，精度还高。需要的加速器数量往往是推理时的10倍以上。但LoRA要更新的参数少。占用内存也少。训练时的硬件布局只比推理时稍微大一点。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_220.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">普通团队用几台GPU就能跑。门槛低多了。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第三个优势是“加载传输方便”，适配器矩阵的参数少。文件体积小。比如一个秩为128的LoRA适配器。可能就几MB或几十MB。不管是在机器间传。还是加载到模型里。都比传整个模型快得多，运维也灵活。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">正是这些优势。LoRA从2021年胡（Hu）等人发表的第一篇论文（《LoRA:。Low-Rank Adaptation of Large Language Models》）后。就越来越火。但是问题也来了，现有的研究里。LoRA和全量微调的性能对比一直不明确。大家公认的是。在“类似预训练”的场景里。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如数据集特别大。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">超过LoRA的参数存储极限的时候。LoRA性能会差一些；。但是在训练后处理的常见数据集规模下。LoRA虽然容量足够存下关键信息。可是没人能保证它的“样本效率”和“计算效率”，能跟全量微调相比。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">所以约翰·舒尔曼团队的实验。核心就是回答了。LoRA到底能不能匹配全量微调的性能？如果能，需要满足哪些条件呢？</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_289.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">他们的实验结论很明确。只要把几个关键细节做对。LoRA的样本效率和最终性能。就能和全量微调完全一致。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">那这些“关键细节”到底是什么？我们得先从他们的实验设计说起。因为好的实验设计。是结论可信的基础。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">和之前的LoRA研究相比。他们的实验有两个很重要的特点。第一个是“不盯着具体任务。而是研究通用关系”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">很多研究只会选一两个数据集。比如只测情感分析。或者只测文本摘要，但是他们不一样。专门研究“训练集大小”和“LoRA参数数量”之间的通用规律。这样得出的结论。不管你是做医疗还是教育任务。都能参考。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第二个是采用了“对数损失（log loss）评估”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">之前很多研究采用的是“基于采样的评估”，比如让模型生成文本，再人工打分。但是这种方法主观性强。不同任务里标准还不一样。而对数损失能直接反映模型的预测误差。而且能够清晰地看到不同训练步骤、不同参数设置下的模型表现规律。通用性强得多。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_357.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">就是靠着这样的实验设计。他们得出了五个关键发现。这些发现也是LoRA能够媲美全量微调的核心条件。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第一个发现。在中小规模的指令微调或者推理数据集上。LoRA和全量微调的性能完全一样。比如他们用的Tulu3和OpenThoughts3数据集。只要规模没超过LoRA的容量。两者的损失曲线、收敛速度都几乎重合。这说明在大多数企业的落地场景里。LoRA完全够用。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第二个发现。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">如果数据集规模超过了LoRA的容量。LoRA就会比全量微调差。但不是“损失降到一定程度就降不下去”了。而是“训练效率变慢”了。具体会慢多少。要取决于LoRA的参数容量和数据集大小的比例。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如一个秩为32的LoRA。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">处理100万Token的数据集没问题。但是如果数据集涨到1亿个Token。它的训练效率就会明显下降。损失下降速度变慢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_419.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第三个发现。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">LoRA对“批量大小（batch size）”的容忍度比全量微调低。如果批量 size 太大。超过某个阈值后。LoRA的性能损失就会比全量微调大得多。而且这个问题。就算增大LoRA的秩也解决不了。因为这是LoRA“矩阵乘积参数化（BA）”的固有属性。它的优化动态和原权重矩阵（W）不一样。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">批量一大，这种动态差异就会被放大。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第四个发现。LoRA一定要应用到模型的所有层。尤其是MLP层或MoE层，效果才好。之前很多研究受到第一篇LoRA论文的影响。只把LoRA用到注意力层。但是约翰·舒尔曼团队发现。仅在注意力层使用LoRA的性能很差。而MLP层才是LoRA发挥作用的关键。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比德曼（Biderman）等人2024年的研究（《LoRA Learns Less and Forgets Less》）也得出了类似结论。进一步验证了这个发现。第五个发现，在强化学习的场景里。就算用很低秩的LoRA。也能和全量微调性能一致。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_482.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">他们用数学推理任务做实验。比如让模型去解初中数学题。以答案正确性为奖励，结果发现。哪怕LoRA的秩只有1。最终的准确率也和全量微调一样。这背后有个信息论的解释。那就是监督学习里。每一轮训练能给模型提供“O(Token数量)”个比特的信息。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如一句话有100个Token。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">就能提供100比特左右的信息；。但是强化学习不一样。政策梯度算法的学习。靠的是“优势函数（Advantage Function）”，每一轮只能提供“O(1)”比特的信息。简单来说。就是强化学习需要学的信息少。就算是低秩LoRA，容量也完全够。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">基于这些发现。他们提出了一个“低遗憾区间（low-regret regime）”的概念。只要LoRA的参数容量能覆盖数据集的信息需求。并且应用到所有层。就能和全量微调性能相当。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而这个区间。刚好覆盖了大多数企业的训练后处理场景。这也意味着，LoRA在很多实际应用里。完全可以替代全量微调。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_547.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">当然，光有结论不够。我们还得知道这些结论是怎么来的。也就是他们的实验细节。这能帮我们在自己做实验的时候少走弯路。我们挑几个部分来重点讲一下。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先，他们的实验设置非常严谨。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先是LoRA的秩。覆盖了1到512三个数量级。从最低的秩1。到能覆盖大部分场景的秩128、256。再到接近全量微调的秩512。这样能全面看到秩对性能的影响。然后是学习率（LR）。为了避免“因为学习率选得不好。导致LoRA性能差”的情况出现。他们对每个实验条件都做了“学习率扫描”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如对秩32的LoRA。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">测试0.0001到0.01的不同学习率。找到最佳值；。而且采用了“恒定学习率”，没有热身或冷却阶段。这样能够排除学习率调度的干扰。更准确对比LoRA和全量微调的差异。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_608.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">模型方面，他们用了两类主流模型。一类是Llama 3系列。另一类是Qwen3系列。还包括了混合专家（MoE）模型。因为MoE模型的激活效率高。现在很多大模型都用这种架构。所以加入MoE的实验，能让结论更全面。数据集方面。监督学习用了Tulu3和OpenThoughts3。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">Tulu3是艾维森（Ivison）等人在2024年提出的。侧重“指令遵循”，比如让模型根据用户指令生成邮件、写代码；。OpenThoughts3是古哈（Guha）等人在2025年提出的。侧重“推理”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如数学题、逻辑推理题。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这两个数据集在任务类型、数据结构上差异很大。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">强化学习实验则用了MATH数据集和GSM8K数据集。都是数学推理任务。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">以答案正确与否作为奖励信号。以便专注测试强化学习场景下的LoRA性能。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">先看“LoRA秩”的影响实验。他们在Tulu3和OpenThoughts3上训练了一个epoch。对每个数据集、每个模型大小。都扫描了不同的LoRA秩和学习率。结果发现，高秩LoRA，比如256、512。和全量微调的学习曲线几乎重合。损失都是“随步骤的对数线性下降”，也就是说。每训练10倍的步骤。损失就会按固定比例下降。这和全量微调的规律完全一样。而低秩LoRA，比如1、4。在训练初期还能跟上。但是到了某个步骤阈值后。就会“偏离最小损失曲线”。这就是之前说的“容量耗尽”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_707.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">低秩矩阵存不下更多的任务信息。所以学习速度变慢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">更重要的是“学习率”的发现。他们绘制了“学习率和最终损失”的曲线。发现高秩LoRA的最小损失和全量微调几乎一样。但是最佳学习率是全量微调的10倍。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如全量微调的最佳学习率是0.001。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">LoRA就需要0.01。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比德曼（Biderman）等人2024年的研究也发现了类似的10倍比例。这说明这个规律不是偶然的。而是LoRA的固有属性。而且不同秩的LoRA。最佳学习率很接近。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如秩4和秩512的最佳学习率差异不到2倍。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这为我们设置LoRA学习率提供了很大便利。不用频繁调整。按全量微调的10倍开始试就行。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">再看“批量大小效应”的实验。他们用了OpenThoughts3里一个1万样本的子集。测试不同批量大小的影响。结果很明显，当批量大小比较小。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_768.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如32的时候。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">LoRA和全量微调的性能差距很小。而且随着训练推进，差距还会缩小；。但当批量大小变大。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如256、512的时候。LoRA的损失会明显比全量微调高。而且这个差距会一直存在。不会随训练步骤消失。更关键的是。这个差距和LoRA的秩无关。就算用秩512的LoRA。大批次下还是比全量微调差。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">他们分析。这是因为LoRA的“矩阵乘积（BA）”参数化。和原权重矩阵（W）的优化动态不同。批量一大。原矩阵能更好地利用批次信息更新。而BA矩阵的更新效率会下降。导致性能差距。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">不过好在，不管是LoRA还是全量微调。都是“小批量下性能更好”，所以实际应用里。只要别用太大的批量。这个问题就能够缓解。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">接下来。我们来重点说一说跟前面第4点发现。“LoRA该应用到哪些层”有关的实验过程。这是很多人调参时容易踩坑的地方。之前很多人习惯只把LoRA用到注意力层。觉得注意力层负责“理解上下文”，所以是关键，但是实验结果刚好相反。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_838.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">约翰·舒尔曼团队做了三组对比实验。第一组只在注意力层用LoRA。第二组只在MLP层用LoRA。第三组在所有层。也就是注意力+MLP+MoE都用了LoRA。结果发现。仅注意力层的LoRA性能最差。就算用更高的秩。参数数量和仅MLP层的秩128差不多。性能还是差一截。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如在Llama-3.1-8B模型上，仅MLP层。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">秩128，0.24B参数的损失。比仅注意力层。秩256，0.25B参数低了0.15左右。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">要知道，在语言模型里。损失降低0.1已经是很明显的提升了。而“所有层用LoRA”和“仅MLP层用LoRA”的性能几乎一样。这说明MLP层才是LoRA发挥作用的核心。注意力层加不加LoRA。对最终性能影响不大。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">他们还在MoE模型上做了实验。对Qwen3-30B-A3B-Base的每个专家都单独训练LoRA。秩设为“总秩除以活跃专家数量”，这样能保证MoE层的LoRA参数比例和其他层一致。结果和稠密模型一样。仅MLP层的LoRA优于仅注意力层。所有层和仅MLP层相当。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_918.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">为什么会这样？这就要用到马拉迪（Malladi）等人在2022年提出的“经验神经正切核（eNTK）”理论了。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">eNTK是用来描述模型微调初期学习动态的工具。它的核心是“梯度的点积”。比如对每个Token的预测。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">梯度g_i是损失对参数的偏导。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">eNTK的核函数K(i。j)就是g_i和g_j的点积。这个核函数决定了模型怎么“学习”数据里的模式。而参数越多的层。对核函数的影响越大。马拉迪等人的研究指出。当LoRA应用到所有层的时候。它的eNTK和全量微调的eNTK几乎一样。所以学习动态也相似；。但是如果只应用到注意力层。而忽略了参数更多的MLP层。eNTK就会和全量微调有明显差异。学习速度自然慢下来。这也解释了为什么仅注意力层的LoRA效果差。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">聊完实验。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">我们最后再来总结一下这篇研究的主要结论和背后的深层意义。它不止告诉了我们LoRA怎么用。还帮我们深化了对大模型微调的理解。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_986.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先是“LoRA匹配全量微调的两个核心条件”，一是要应用到所有层。尤其是MLP/MoE层。这样才能保证eNTK和全量微调的一致；。二是LoRA的容量要能覆盖数据集的信息需求。也就是“非容量约束”。只要满足这两个条件。LoRA就能和全量微调性能相当。这为我们选择微调方法提供了明确的判断标准。如果你的数据集是中小规模。模型有MLP层。就用LoRA；。如果数据集特别大。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如接近预训练规模。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">再考虑全量微调。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">其次是“监督学习和RL的容量需求的差异”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">监督学习需要的信息多。每个token 1比特。所以需要更高秩的LoRA；。强化学习需要的信息少，每轮次1比特。低秩LoRA就够。这个差异能帮我们节省资源。也就是做强化学习任务时。不用追求高秩。秩1、8就够，参数更少，训练更快。然后是“LoRA的计算效率优势”，他们算了一笔账。LoRA每轮训练的FLOPs。是全量微调的2/3左右。具体怎么算的大家可以去看一下这一段。这里我们就不一一推导了。简单来说，对于一个N×N的权重矩阵W。LoRA的总FLOPs是2N²。全量微调是3N²，所以比例就是2/3。这意味着，同样的硬件。LoRA能比全量微调多训练50%的样本。计算效率更高。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_1079.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">当然，研究也提出了一些“开放问题”，这些也是未来值得关注的方向。比如怎么更精准地预测LoRA的性能。不用每次都做实验；。为什么LoRA的最佳学习率是全量微调的10倍。目前还没有完整的理论解释；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">LoRA的变体在这种评估方法下表现如何；。还有MoE层的LoRA。怎么和张量并行、专家并行这些技术结合。适配更大的模型，等等等等。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">最后，约翰·舒尔曼团队在结语里提到。他们研究LoRA的目标。不只是为了省资源。更是为了“让大模型的微调能力更易获取”，毕竟不是每个团队都有能力做全量微调。但是LoRA能让普通团队也能用大模型来解决具体的问题。而且通过研究LoRA。他们也深化了对“模型容量”、“数据集复杂度”、“样本效率”这些基础问题的理解。这其实也是AI研究的魅力。一个实用的技术。背后往往藏着对核心原理的新认知。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/MlqXL6o--2M_1140.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">好了，今天的内容就到这里。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">如果你在做LoRA相关的工作。希望今天的拆解能帮你少走弯路；。如果你只是感兴趣。也希望能够帮助你增加对LoRA的了解。感谢大家观看本期视频。我们下期再见。</p>

<hr style="clear:both" />
=============
<p><a href="https://www.youtube.com/watch?v=MlqXL6o--2M">https://www.youtube.com/watch?v=MlqXL6o--2M</a></p><p>Thinking Machines Lab最近发布的一篇博客，文章的标题是《LoRA无悔（LoRA Without Regret）》，作者是约翰·舒尔曼（John Schulman）和他的团队。文章不仅回答了“LoRA能不能媲美全量微调”的问题，还给出了具体的操作条件，甚至连超参数怎么设都讲清楚了。不管你是想入门LoRA，还是已经在落地中遇到问题，这篇内容都值得你仔细阅读。</p><p><a href="https://thinkingmachines.ai/blog/lora/">https://thinkingmachines.ai/blog/lora/</a></p>]]></content:encoded>
      <itunes:image href="https://i.ytimg.com/vi/MlqXL6o--2M/hqdefault.jpg"/>
      <pubDate>2025-10-02T09:00:39.000Z</pubDate>
    </item><item>
      <title><![CDATA[【人工智能】DeepSeek发布新模型V3.2-Exp | 全新DSA稀疏注意力机制 | 闪电索引器 | 长上下文效率优化 | 细粒度的token选择 | MLA架构 | 密集预热+稀疏训练 | 蒸馏]]></title>
      <link>https://www.youtube.com/watch?v=CA2lzW9INrQ</link>
      <itunes:title><![CDATA[【人工智能】DeepSeek发布新模型V3.2-Exp | 全新DSA稀疏注意力机制 | 闪电索引器 | 长上下文效率优化 | 细粒度的token选择 | MLA架构 | 密集预热+稀疏训练 | 蒸馏]]></itunes:title>
      <itunes:author><![CDATA[最佳拍档]]></itunes:author>
      <itunes:summary>
        <![CDATA[<hr style="clear:both" />

<p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://img.youtube.com/vi/CA2lzW9INrQ/maxresdefault.jpg" /></p><p><a href="https://www.youtube.com/watch?v=CA2lzW9INrQ">https://www.youtube.com/watch?v=CA2lzW9INrQ</a></p><h1>值得閱讀的理由</h1> <ul> <li>深入了解DeepSeek-V3.2-Exp如何透過其獨特的<strong>稀疏注意力機制（DSA）</strong>，在處理<strong>長上下文</strong>任務時實現顯著的效率提升。</li> <li>探索模型的<strong>架構細節</strong>，包括<strong>閃電索引器</strong>與<strong>細粒度token選擇機制</strong>如何協同運作，以及其訓練流程中<strong>密集預熱</strong>與<strong>稀疏訓練</strong>階段的精妙設計。</li> <li>掌握該模型在<strong>能力評估</strong>與<strong>推理成本</strong>上的實際表現，了解DeepSeek團隊如何平衡<strong>性能</strong>與<strong>效率</strong>，並為未來的AI應用提供更具經濟效益的解決方案。</li> </ul> <hr /> <h1>摘要</h1> <p>在大飛的介紹下，DeepSeek團隊推出了其最新實驗性模型<strong>DeepSeek-V3.2-Exp</strong>。該模型在DeepSeek-V3.1-Terminus的基礎上，透過持續訓練引入了全新的<strong>稀疏注意力機制（DeepSeek Sparse Attention, DSA）</strong>。結合高效的<strong>閃電索引器</strong>，V3.2-Exp在訓練和推理效率上實現了顯著提升，尤其在處理<strong>長上下文</strong>場景時，其效率優勢更為突出。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_50.jpg" /></p> <h2>架構分析</h2> <p>DeepSeek-V3.2-Exp的架構核心是DSA，主要包含兩個關鍵組件：<strong>閃電索引器</strong>和<strong>細粒度的token選擇機制</strong>。<strong>閃電索引器</strong>負責計算查詢token與前面token之間的索引分數，並根據分數決定哪些token進行後續注意力計算。為提升計算效率，索引器採用ReLU激活函數，且頭數較少並支援FP8精度，確保其計算成本極低。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_160.jpg" /></p> <p>在索引器篩選出關鍵token後，<strong>細粒度的token選擇機制</strong>會根據索引分數，只挑選出排名前k位的token所對應的鍵值對進行注意力運算。這使得模型無需對所有token都進行計算，大幅降低了計算量。DSA是基於DeepSeek在2024年提出的<strong>MLA（Multi-Query Attention, 多查詢注意力）架構</strong>實現的，以確保與V3.1的訓練連貫性，並支援鍵值對在多個查詢間共享，進一步提升計算效率。詳細的架構和開源實現可供進一步參考。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_320.jpg" /></p> <h2>訓練流程</h2> <p>DeepSeek-V3.2-Exp的訓練始於DeepSeek-V3.1-Terminus的基礎檢查點，其上下文長度已擴展至128K。整個訓練過程分為兩個主要階段：<strong>持續預訓練</strong>和<strong>後訓練</strong>。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_350.jpg" /></p> <h3>持續預訓練階段</h3> <p>持續預訓練包含<strong>密集預熱階段</strong>和<strong>稀疏訓練階段</strong>，兩者使用與V3.1擴展長上下文時相同的數據分佈。在<strong>密集預熱階段</strong>，模型仍採密集注意力計算，只初始化<strong>閃電索引器</strong>，而模型其他參數凍結。此階段的目標是透過最小化索引器輸出與主注意力分佈之間的<strong>KL散度損失</strong>，讓索引器的輸出與主注意力分布保持一致。此階段訓練量約為2.1B個token，旨在短時間內完成索引器初始化。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_495.jpg" /></p> <p>完成預熱後，進入<strong>稀疏訓練階段</strong>。此時模型啟用DSA的稀疏模式，並優化所有參數。KL散度損失函數調整為僅計算經過篩選的token。值得一提的是，索引器的輸入與主模型的計算圖分離，索引器僅依賴KL散度損失優化，而主模型則依據語言建模損失，確保兩者優化過程相互獨立。此階段訓練量約為943.7B個token，確保模型充分適應稀疏模式下的性能。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_640.jpg" /></p> <h3>後訓練階段</h3> <p>後訓練階段旨在打造最終的DeepSeek-V3.2-Exp模型，並保持與稀疏持續預訓練階段相同的稀疏注意力使用方式。為嚴謹評估DSA的影響，後訓練流水線、算法和數據均與DeepSeek-V3.1-Terminus完全相同。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_690.jpg" /></p> <p>此階段包含<strong>專家蒸餾</strong>和<strong>混合RL訓練</strong>。<strong>專家蒸餾</strong>透過從同一預訓練基礎檢查點微調，開發出多個專注於不同領域（如數學、編程、邏輯推理等）的<strong>專家模型</strong>。這些專家模型用於生成兩種訓練數據：<strong>長鏈式推理（思考模式）</strong>和<strong>直接響應生成（非思考模式）</strong>。最終，這些蒸餾後的數據用於訓練DeepSeek-V3.2-Exp模型，使其在多個領域具備接近專家模型的能力，而性能差距可透過後續RL訓練消除。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_840.jpg" /></p> <p><strong>混合RL訓練</strong>階段，DeepSeek-V3.2-Exp採用<strong>分組相對策略優化（GRPO）</strong>，並將推理訓練、智能體訓練和人類對齊訓練<strong>合併為單一RL階段</strong>。這種合併不僅在不同領域間實現了更好的性能平衡，還成功規避了多階段訓練中常見的「<strong>災難性遺忘</strong>」問題。獎勵設計也十分細緻：推理和智能體任務採用基於規則的結果獎勵、長度懲罰和語言一致性獎勵；通用任務則採用生成式獎勵模型及評估標準。獎勵設計著重平衡「長度與準確性」以及「語言一致性與準確性」。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_1060.jpg" /></p> <h2>評估結果</h2> <p>DeepSeek-V3.2-Exp的評估涵蓋模型能力、推理成本及未來驗證計劃。在<strong>模型能力</strong>方面，與DeepSeek-V3.1-Terminus進行了全面基準測試對比。結果顯示，儘管效率顯著提升，V3.2-Exp在短、長上下文任務中均<strong>未出現明顯性能下降</strong>。在部分基準測試中，微小的性能下降主要是由於生成token數量減少，若生成token數與V3.1相近，性能差距便會消失。此外，兩款模型的強化學習訓練曲線趨勢高度接近，證明DSA機制不影響模型的訓練穩定性。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_20.jpg" /></p> <p><strong>推理成本</strong>是V3.2-Exp的核心優勢。理論上，傳統密集注意力的核心計算複雜度為O(L²)，而引入DSA後降至O(L×k)，實現從平方級到線性級的顯著降低。雖然閃電索引器本身的計算複雜度仍為O(L²)，但由於其頭數少且支援FP8精度，實際計算成本極低。在H800 GPU集群上的實際服務測試顯示，DeepSeek-V3.2-Exp的每百萬token成本始終低於DeepSeek-V3.1-Terminus，尤其在128K等<strong>長上下文</strong>場景下，成本優勢更為明顯。為優化短序列處理效率，團隊還引入了掩碼MHA模式。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_24.jpg" /></p> <p>最後，DeepSeek團隊計劃在<strong>真實世界場景中進行更大規模的測試</strong>，以發現並解決稀疏注意力架構可能存在的潛在局限，確保DeepSeek-V3.2-Exp在實際應用中具備更穩定、更出色的表現。總體而言，DeepSeek-V3.2-Exp透過引入DSA，成功在長上下文處理效率上取得突破，同時保持了與前一版本相當的性能水平，為AI任務提供了更高效、更經濟的選擇。</p> <hr /><hr />================================================</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">大家好，这里是最佳拍档，我是大飞。昨天。DeepSeek团队推出了最新模型DeepSeek-V3.2-Exp。在之前 DeepSeek-V3.1-Terminus 的基础上。通过持续训练集成了一种全新的稀疏注意力机制。也就是DeepSeek 稀疏注意力。简称DSA。摇身一变成为了一款实验性的稀疏注意力模型。再搭配上一个高效的闪电索引器。V3.2-Exp在训练和推理两个关键环节都实现了显著的效率提升。尤其是在处理长上下文场景的时候。这种效率优势会更加明显。这期视频我们就从架构、训练、评估这几个核心维度。给大家拆解一下这款模型的细节。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先来看架构部分。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这次DeepSeek-V3.2-Exp在架构上唯一的修改。就是通过持续训练。引入了DSA。那这个DSA到底是由什么构成的呢？它主要包含两个关键组件。一个是闪电索引器。另一个是细粒度的token选择机制。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_62.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">我们先来了解一下闪电索引器。它的核心作用是计算查询token（query token）和前面的某个token之间的索引分数It,s。然后通过这个分数来决定。查询token要选择哪些token进行后续的注意力计算。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这个索引分数的计算公式是这样的。这里面有几个关键参数需要跟大家解释清楚。其中Hᴵ代表的是索引器头（indexer heads）的数量；。qt,jᴵ和wt,jᴵ是一个实数。是从查询token ht中推导出来的；。而ksᴵ是从前面的token hs中推导出来的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">可能有朋友会问。为什么选择ReLU作为激活函数呢？这里主要是出于吞吐量的考虑。ReLU函数在计算效率上有一定的优势。能够更好地配合闪电索引器实现高效计算。而且还有一个很重要的点。闪电索引器的头数比较少。同时还可以用FP8精度来实现。这就使得它的计算效率非常突出。不会因为引入新的组件而导致计算成本大幅增加。这也是后续模型整体效率提升的一个重要基础。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_130.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">有了闪电索引器计算出的索引分数之后。接下来就要靠细粒度的token选择机制来发挥作用了。对于每个查询token ht。这个机制会根据索引分数。只筛选出索引分数排在前k位的token所对应的键值对（key-value entries）。然后，注意力输出ut的计算。就是让查询token ht和这些经过稀疏筛选后的键值对。用{cs}表示，进行注意力机制的运算。具体公式是这样的。这样一来。模型就不需要对所有的token都进行注意力计算。只针对筛选后的关键token进行处理。从而在保证效果的前提下。大幅降低计算量。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">不过，DSA并不是孤立存在的。它是在DeepSeek在2024年提出的MLA架构的基础上实例化来的。为什么要选择这样的方式呢？主要是考虑到DeepSeek-V3.2-Exp是从DeepSeek-V3.1-Terminus进行持续训练得到的。基于MLA来实例化DSA。能够更好地保证训练的连贯性和兼容性。而且在kernel层面。为了提升计算效率。每个键值对都必须在多个查询之间共享。这一点是袁等人在2025年的研究中提到的关键结论。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_203.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">基于这样的需求。研发团队选择了在MLA的MQA模式。也就是多查询注意力（Multi-Query Attention）的基础上实现DSA。这种MQA模式是Transformer作者之一诺姆·沙泽尔（N。Shazeer）在2019年提出的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在这种模式下，MLA中的每个潜在向量。都会在查询token的所有查询头（query heads）之间共享。这样就能很好地满足键值对共享的需求。进一步提升计算效率。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">关于DSA基于MLA的具体架构。大家可以参考原文中的图1。里面清晰地展示了整个注意力架构的细节。尤其是绿色部分。直观地呈现了DSA如何根据索引器筛选出前k个键值对的过程。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">另外。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">为了让大家能够更清晰地了解具体的实现细节。研发团队还提供了DeepSeek-V3.2-Exp的开源实现。大家如果想深入研究代码层面的逻辑。这个地址会非常有帮助。同时。原文的附录A还详细说明了MLA的MQA模式和MHA模式（Multi-Head Attention。多头注意力）之间的区别。感兴趣的朋友也可以去附录部分进一步阅读。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_265.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">讲完了架构，咱们再来看训练流程。DeepSeek-V3.2-Exp的训练是从DeepSeek-V3.1-Terminus的基础checkpoint开始的。这个基础checkpoint的上下文长度已经扩展到了128K。为后续处理长上下文任务打下了很好的基础。整个训练过程分为两个主要阶段。持续预训练和后训练。每个阶段又有各自细分的步骤和目标。咱们一步步来看。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先是持续预训练阶段。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这个阶段又分为两个训练步骤。分别是密集预热阶段和稀疏训练阶段。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">需要先说明的是。这两个阶段所使用的训练数据分布。是完全和DeepSeek-V3.1-Terminus在扩展128K长上下文时所用的数据保持一致的。这样做的目的是为了保证数据的连贯性。减少因数据分布差异对模型训练效果产生的干扰。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第一个步骤是密集预热阶段。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这个阶段的核心目标是初始化闪电索引器。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在这个阶段。模型仍然保持着密集注意力的计算方式。也就是说。此时还没有启用稀疏筛选的机制。同时。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_332.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">除了闪电索引器之外。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">模型的其他所有参数都处于冻结状态。不进行更新。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">那为什么要这么做呢？主要是为了让索引器的输出能够和主注意力的分布保持一致。为后续的稀疏训练做好铺垫。具体怎么实现这种对齐呢？对于第t个查询token。研发团队首先会将主注意力的分数在所有注意力头上进行求和。然后沿着序列维度对这个求和结果进行L1归一化。得到一个目标分布p。然后，基于这个目标分布p。团队设置了KL散度损失作为闪电索引器的训练目标。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">具体的损失函数公式是这样的。其中Softmax函数是为了将索引器计算出的索引分数转换为概率分布。以便和目标分布p进行比较。通过最小化两者之间的KL散度。让索引器的输出尽可能接近主注意力的分布。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在这个预热阶段。所使用的学习率是10的负3次方。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">训练步数非常少，只训练了1000步。每一步包含16个序列。每个序列有128K个token。咱们可以算一下。整个密集预热阶段总共训练的token数量是1000步 × 16序列/步 × 128K token/序列。约等于2.1B个token。这样的训练量能够在短时间内完成索引器的初始化。同时避免过度训练导致其他问题。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_411.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">完成了密集预热之后。就进入到了持续预训练的第二个步骤。稀疏训练阶段。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在这个阶段。研发团队引入了前面提到的细粒度token选择机制。开始让模型适应DSA的稀疏模式。并且此时会对模型的所有参数进行优化更新。而不再是只训练索引器。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在这个阶段。仍然需要保持索引器的输出和主注意力分布的对齐。但和密集预热阶段不同的是。此时只考虑经过筛选后的token集合Sₜ。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这个集合St的定义是所有满足It,s属于It,:中前k个分数的s所构成的集合。因此。KL散度损失函数也相应地进行了调整。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">也就是只计算筛选后的token在目标分布和索引器输出分布之间的KL散度。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这里有一个非常关键的技术细节。研发团队将索引器的输入从计算图中分离了出来。进行单独的优化。这意味着什么呢？简单来说。索引器的训练信号只来自于刚才提到的KL散度损失。而主模型的优化则完全依据语言建模损失（language modeling loss）。两者的优化过程相互独立。这样做能够更好地平衡索引器和主模型的训练。避免相互干扰。保证各自训练目标的实现。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_485.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在稀疏训练阶段。所使用的学习率是7.3×10⁻⁶。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">每个查询token会选择2048个键值对token进行后续的注意力计算。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">训练步数比预热阶段多很多。总共训练了15000步。每一步包含480个序列。每个序列同样是128K个token。咱们再来计算一下这个阶段的总训练token量。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">15000步 × 480序列/步 × 128K token/序列。约为943.7B个token。如此大的训练量能够让模型充分适应稀疏模式。保证模型在稀疏注意力机制下的性能。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">持续预训练完成之后。就进入到了后训练阶段。这个阶段的目标是打造最终的DeepSeek-V3.2-Exp模型。需要强调的是，在后训练阶段。模型仍然采用和稀疏持续预训练阶段相同的方式来使用稀疏注意力。这样可以保证训练过程的一致性。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">为了严谨地评估引入DSA之后对模型的影响。研发团队在DeepSeek-V3.2-Exp的后训练过程中。保持了和DeepSeek-V3.1-Terminus完全相同的后训练流水线、算法以及数据。这样一来。两者在后训练阶段的差异就只来源于是否引入了DSA。从而能够更准确地判断DSA对模型性能和效率的影响。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_565.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">后训练阶段主要包括两个核心环节。分别是专家蒸馏（Specialist Distillation）和混合RL训练（Mixed RL Training）。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">先来看专家蒸馏环节。这个环节的思路是。针对每个特定的任务或领域。先开发一个专门的模型。也就是“专家模型”，这些专家模型都只专注于自己所对应的领域。所有的专家模型都是从同一个预训练的DeepSeek-V3.2基础checkpoint进行微调得到的。这样可以保证专家模型之间的基础能力处于同一水平。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">具体涵盖了哪些领域呢？除了常见的写作任务和通用问答任务之外。还包括五个专门的领域。包括数学、竞赛编程（competitive programming）、通用逻辑推理（general logical reasoning）、智能体编程（agentic coding）以及智能体搜索（agentic search）。每个专家模型的训练都投入了大规模的强化学习（RL）计算资源。确保专家模型在各自领域内能够达到较高的性能水平。而且。为了让训练数据更贴合不同的推理和生成需求。研发团队还使用了不同的模型来生成两种类型的训练数据。一种是用于长链式推理（long chain-of-thought reasoning）的训练数据。对应的是“思考模式”（thinking mode）；。另一种是用于直接响应生成（direct response generation）的训练数据。对应的是“非思考模式”（non-thinking mode）。当这些专家模型训练完成之后。它们就会被用来生成特定领域的数据。这些数据会用于最终DeepSeek-V3.2-Exp模型checkpoint的训练。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_643.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">从实验结果来看。使用这些蒸馏后的数据训练出来的模型。性能只比那些专门的领域专家模型略低一点。而且通过后续的RL训练。这个性能差距能够被有效的消除。这就意味着，通过专家蒸馏。DeepSeek-V3.2-Exp能够在多个领域同时具备接近专家模型的能力。而不需要为每个领域单独训练一个模型。大大提升了模型的通用性和效率。接下来呢。是混合RL训练环节。在DeepSeek V3.2 EXP的后训练中。研发团队仍然采用了分组相对策略优化GRPO作为RL训练的算法。不过，和之前的DeepSeek模型相比。这里有一个重要的改进。那就是之前的模型采用的是多阶段强化学习训练。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而DeepSeek-V3.2-Exp则将推理训练、智能体训练以及人类对齐训练这三个原本分开的阶段合并成了一个RL阶段。这种合并带来了两个显著的好处。一方面。它能够在不同的领域之间实现更好的性能平衡。避免某个领域因为训练阶段的侧重不同而导致性能过强或过弱；。另一方面。它成功规避了多阶段训练范式中常见的“灾难性遗忘”问题。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_715.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">所谓灾难性遗忘。就是模型在学习新任务或新领域知识的时候。会忘记之前已经学会的知识。而单阶段训练则很好地缓解了这个问题。保证了模型在各个领域知识的稳定性。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在奖励设计方面。研发团队也考虑得非常细致。针对不同类型的任务设置了不同的奖励机制。对于推理任务和智能体任务。采用了基于规则的结果奖励（rule-based outcome reward）、长度惩罚（length penalty）以及语言一致性奖励（language consistency reward）。基于规则的结果奖励主要是根据任务的客观结果来判断模型输出是否正确。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如推理任务是否得出正确答案。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">智能体任务是否完成指定目标；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">长度惩罚则是为了避免模型生成过长但无意义的内容。鼓励简洁有效的输出；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">语言一致性奖励则是保证模型输出的语言在逻辑、风格等方面保持一致。提升输出质量。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而对于通用任务。研发团队采用了生成式奖励模型（generative reward model）。并且为每个提示词都设置了专门的评估标准（rubrics）。这种奖励模型能够更灵活地评估通用任务中模型输出的质量。因为通用任务的评价标准往往不像推理或智能体任务那样客观明确。需要更细致的评估维度。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_787.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在整个奖励设计过程中。研发团队重点平衡了两个关键的权衡关系。第一个是“长度与准确性”的权衡。也就是既要避免模型为了追求准确性而生成过长的内容。也要防止为了缩短长度而牺牲准确性；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第二个是“语言一致性与准确性”的权衡。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">即保证语言一致性的同时。不影响模型输出的准确性。确保模型在多个评估维度上都能有出色的表现。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">讲完了训练流程。咱们再来看大家最关心的评估结果。毕竟一款模型的好坏。最终还是要靠实际的评估数据来证明。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">DeepSeek-V3.2-Exp的评估主要从三个方面展开。分别是模型能力、推理成本。以及未来的真实场景验证计划。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先是模型能力的评估。研发团队选择了一系列涵盖不同能力的基准测试（benchmark）。将DeepSeek-V3.2-Exp和DeepSeek-V3.1-Terminus进行了全面的对比。具体的对比数据大家可以参考这张表。从整体结果来看。尽管DeepSeek-V3.2-Exp在长序列处理的计算效率上有了显著提升。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_852.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">但是无论是在短上下文任务还是长上下文任务中。它都没有出现明显的性能下降。这一点非常关键。因为很多时候模型效率的提升往往会伴随着性能的损失。而DeepSeek-V3.2-Exp很好地兼顾了效率和性能。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">不过，在个别基准测试中。DeepSeek-V3.2-Exp的性能确实比DeepSeek-V3.1-Terminus略低一些。比如在GPQA-Diamond（Pass@1）、HLE（Pass@1）以及HMMT 2025（Pass@1）这几项测试中。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">那是什么原因导致了这种性能差异呢？研发团队经过分析发现。主要是因为DeepSeek-V3.2-Exp生成的推理token数量更少。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而有趣的是。当使用那些生成的token数量。与DeepSeek-V3.1-Terminus相近的中间checkpoint进行测试时。这种性能差距就消失了。这说明。性能差异并不是因为DSA机制本身导致的。而是和模型生成内容的长度相关。只要调整好生成token的数量。DeepSeek-V3.2-Exp完全能够达到和之前版本相当的性能水平。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_919.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">除了基准测试的静态结果之外。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">研发团队还对比了两款模型的强化学习训练曲线。其中图（a）是BrowseComp任务的训练曲线。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">图（b）是SWE Verified任务的训练曲线。从曲线可以看出，在整个训练过程中。DeepSeek-V3.1-Terminus和DeepSeek-V3.2-Exp在这两项任务上的准确率都在稳步提升。而且两条曲线的走势非常接近。几乎是同步上升的。同时。图中的虚线代表模型输出的平均token数量。两款模型的输出token数量也保持着相似的变化趋势。这种接近的训练曲线充分说明了DSA机制并没有影响模型的训练稳定性。DeepSeek-V3.2-Exp在训练过程中能够像之前的版本一样稳定地学习和提升性能。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">接下来是推理成本的评估。这也是DeepSeek-V3.2-Exp的核心优势所在。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">咱们先从理论层面来看。DSA机制到底是如何降低推理成本的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在传统的密集注意力机制中。主模型的核心注意力计算复杂度是O(L²)，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_981.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">其中L是序列的长度。这意味着当序列长度大幅增加时。计算量会呈平方级增长。这也是长上下文处理效率低的主要原因之一。而引入DSA之后。核心注意力计算复杂度降低到了O(L×k)，其中k是每个查询token选择的键值对token数量。而且k远小于L（k≪L）。这样一来，当处理长序列时。计算量的增长速度就从平方级变成了线性级。大幅降低了计算成本。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">可能有朋友会问。那闪电索引器的计算复杂度呢？确实。闪电索引器的计算复杂度仍然是O(L²)，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">但是它的计算量和DeepSeek-V3.1-Terminus中MLA的计算量相比。要少得多。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">一方面是因为闪电索引器的头数少。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">另一方面是因为它可以用FP8精度实现。这些都使得闪电索引器的实际计算成本非常低。再加上研发团队针对DSA做的优化实现。最终使得DeepSeek-V3.2-Exp在长上下文场景下实现了显著的端到端加速。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_1045.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">为了让大家更直观地感受到这种成本差异。研发团队在H800 GPU集群上部署了实际的服务。并且对两款模型的推理成本进行了 benchmark 测试。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">测试时GPU的租赁价格按照每小时2美元计算。具体的成本对比数据可以参考这张图。其中图（a）是预填充（Prefilling）阶段的成本。图（b）是解码（Decoding）阶段的成本。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">从图中可以清晰地看到。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">随着序列中token位置的增加。比如从0K到128K。DeepSeek-V3.2-Exp的每百万token成本始终低于DeepSeek-V3.1-Terminus。而且在token位置达到128K这种长上下文场景下。成本优势更加明显。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">另外。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">还有一个细节需要跟大家提一下。在处理短序列预填充的时候。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">研发团队专门实现了一种掩码MHA（masked MHA）模式来模拟DSA的效果。这样做的目的是在短上下文条件下。也能让模型达到更高的效率。避免在短序列场景下因为DSA的稀疏机制反而导致效率下降。充分考虑了不同序列长度场景下的效率优化。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_1107.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">最后，关于未来的验证计划。研发团队也表现出了非常严谨的态度。虽然目前的内部评估结果显示DeepSeek-V3.2-Exp的表现很有前景。但是团队并没有就此止步。而是计划在真实世界场景中进行更大规模的测试。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">为什么要这么做呢？因为实验室环境下的基准测试往往无法完全覆盖真实场景中的各种复杂情况。真实场景中可能会遇到各种意想不到的问题。比如不同类型的长上下文数据、不同的推理任务需求等。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">通过大规模的真实场景测试。研发团队希望能够发现稀疏注意力架构可能存在的潜在局限。从而进一步优化模型。让DeepSeek-V3.2-Exp在实际应用中能够有更稳定、更出色的表现。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">到这里。DeepSeek-V3.2-Exp的核心内容就给大家介绍得差不多了。这款模型通过引入DSA稀疏注意力机制。成功在长上下文处理的效率上实现了突破。同时又保持了和之前版本相当的性能水平。为长上下文AI任务的应用提供了一个更高效、更经济的选择。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_1170.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">无论是架构设计中的细节考量。还是训练过程中的严谨优化。再到评估阶段的全面验证。都能看出DeepSeek-AI团队在这款模型上投入的心血和专业能力。当然。AI模型的发展是一个持续迭代的过程。DeepSeek-V3.2-Exp作为一款实验性模型。未来还有很大的优化空间。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如进一步提升稀疏筛选的准确性、在更多真实场景中验证性能等。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">相信随着后续的不断改进。这款模型会在更多领域发挥重要作用。也期待DeepSeek团队能够带来更多像这样兼顾性能和效率的优秀模型。感谢收看本期视频，我们下期再见。</p>

<hr style="clear:both" />
=============
<p><a href="https://www.youtube.com/watch?v=CA2lzW9INrQ">https://www.youtube.com/watch?v=CA2lzW9INrQ</a></p><p>昨天，DeepSeek团队推出了最新模型DeepSeek-V3.2-Exp，在之前 DeepSeek-V3.1-Terminus 的基础上，通过持续训练集成了一种全新的稀疏注意力机制，也就是DeepSeek 稀疏注意力，简称DSA，摇身一变成为了一款实验性的稀疏注意力模型。再搭配上一个高效的闪电索引器，V3.2-Exp在训练和推理两个关键环节都实现了显著的效率提升，尤其是在处理长上下文场景的时候，这种效率优势会更加明显。这期视频我们就从架构、训练、评估这几个核心维度，给大家拆解一下这款模型的细节。</p><p><a href="https://api-docs.deepseek.com/zh-cn/news/news250929">https://api-docs.deepseek.com/zh-cn/news/news250929</a></p><p><a href="https://github.com/deepseek-ai/DeepSeek-V3.2-Exp/blob/main/DeepSeek_V3_2.pdf">https://github.com/deepseek-ai/DeepSeek-V3.2-Exp/blob/main/DeepSeek_V3_2.pdf</a></p><p><a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Exp/tree/main/inference">https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Exp/tree/main/inference</a></p>]]>
      </itunes:summary>
      <description>
        <![CDATA[<hr style="clear:both" />

<p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://img.youtube.com/vi/CA2lzW9INrQ/maxresdefault.jpg" /></p><p><a href="https://www.youtube.com/watch?v=CA2lzW9INrQ">https://www.youtube.com/watch?v=CA2lzW9INrQ</a></p><h1>值得閱讀的理由</h1> <ul> <li>深入了解DeepSeek-V3.2-Exp如何透過其獨特的<strong>稀疏注意力機制（DSA）</strong>，在處理<strong>長上下文</strong>任務時實現顯著的效率提升。</li> <li>探索模型的<strong>架構細節</strong>，包括<strong>閃電索引器</strong>與<strong>細粒度token選擇機制</strong>如何協同運作，以及其訓練流程中<strong>密集預熱</strong>與<strong>稀疏訓練</strong>階段的精妙設計。</li> <li>掌握該模型在<strong>能力評估</strong>與<strong>推理成本</strong>上的實際表現，了解DeepSeek團隊如何平衡<strong>性能</strong>與<strong>效率</strong>，並為未來的AI應用提供更具經濟效益的解決方案。</li> </ul> <hr /> <h1>摘要</h1> <p>在大飛的介紹下，DeepSeek團隊推出了其最新實驗性模型<strong>DeepSeek-V3.2-Exp</strong>。該模型在DeepSeek-V3.1-Terminus的基礎上，透過持續訓練引入了全新的<strong>稀疏注意力機制（DeepSeek Sparse Attention, DSA）</strong>。結合高效的<strong>閃電索引器</strong>，V3.2-Exp在訓練和推理效率上實現了顯著提升，尤其在處理<strong>長上下文</strong>場景時，其效率優勢更為突出。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_50.jpg" /></p> <h2>架構分析</h2> <p>DeepSeek-V3.2-Exp的架構核心是DSA，主要包含兩個關鍵組件：<strong>閃電索引器</strong>和<strong>細粒度的token選擇機制</strong>。<strong>閃電索引器</strong>負責計算查詢token與前面token之間的索引分數，並根據分數決定哪些token進行後續注意力計算。為提升計算效率，索引器採用ReLU激活函數，且頭數較少並支援FP8精度，確保其計算成本極低。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_160.jpg" /></p> <p>在索引器篩選出關鍵token後，<strong>細粒度的token選擇機制</strong>會根據索引分數，只挑選出排名前k位的token所對應的鍵值對進行注意力運算。這使得模型無需對所有token都進行計算，大幅降低了計算量。DSA是基於DeepSeek在2024年提出的<strong>MLA（Multi-Query Attention, 多查詢注意力）架構</strong>實現的，以確保與V3.1的訓練連貫性，並支援鍵值對在多個查詢間共享，進一步提升計算效率。詳細的架構和開源實現可供進一步參考。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_320.jpg" /></p> <h2>訓練流程</h2> <p>DeepSeek-V3.2-Exp的訓練始於DeepSeek-V3.1-Terminus的基礎檢查點，其上下文長度已擴展至128K。整個訓練過程分為兩個主要階段：<strong>持續預訓練</strong>和<strong>後訓練</strong>。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_350.jpg" /></p> <h3>持續預訓練階段</h3> <p>持續預訓練包含<strong>密集預熱階段</strong>和<strong>稀疏訓練階段</strong>，兩者使用與V3.1擴展長上下文時相同的數據分佈。在<strong>密集預熱階段</strong>，模型仍採密集注意力計算，只初始化<strong>閃電索引器</strong>，而模型其他參數凍結。此階段的目標是透過最小化索引器輸出與主注意力分佈之間的<strong>KL散度損失</strong>，讓索引器的輸出與主注意力分布保持一致。此階段訓練量約為2.1B個token，旨在短時間內完成索引器初始化。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_495.jpg" /></p> <p>完成預熱後，進入<strong>稀疏訓練階段</strong>。此時模型啟用DSA的稀疏模式，並優化所有參數。KL散度損失函數調整為僅計算經過篩選的token。值得一提的是，索引器的輸入與主模型的計算圖分離，索引器僅依賴KL散度損失優化，而主模型則依據語言建模損失，確保兩者優化過程相互獨立。此階段訓練量約為943.7B個token，確保模型充分適應稀疏模式下的性能。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_640.jpg" /></p> <h3>後訓練階段</h3> <p>後訓練階段旨在打造最終的DeepSeek-V3.2-Exp模型，並保持與稀疏持續預訓練階段相同的稀疏注意力使用方式。為嚴謹評估DSA的影響，後訓練流水線、算法和數據均與DeepSeek-V3.1-Terminus完全相同。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_690.jpg" /></p> <p>此階段包含<strong>專家蒸餾</strong>和<strong>混合RL訓練</strong>。<strong>專家蒸餾</strong>透過從同一預訓練基礎檢查點微調，開發出多個專注於不同領域（如數學、編程、邏輯推理等）的<strong>專家模型</strong>。這些專家模型用於生成兩種訓練數據：<strong>長鏈式推理（思考模式）</strong>和<strong>直接響應生成（非思考模式）</strong>。最終，這些蒸餾後的數據用於訓練DeepSeek-V3.2-Exp模型，使其在多個領域具備接近專家模型的能力，而性能差距可透過後續RL訓練消除。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_840.jpg" /></p> <p><strong>混合RL訓練</strong>階段，DeepSeek-V3.2-Exp採用<strong>分組相對策略優化（GRPO）</strong>，並將推理訓練、智能體訓練和人類對齊訓練<strong>合併為單一RL階段</strong>。這種合併不僅在不同領域間實現了更好的性能平衡，還成功規避了多階段訓練中常見的「<strong>災難性遺忘</strong>」問題。獎勵設計也十分細緻：推理和智能體任務採用基於規則的結果獎勵、長度懲罰和語言一致性獎勵；通用任務則採用生成式獎勵模型及評估標準。獎勵設計著重平衡「長度與準確性」以及「語言一致性與準確性」。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_1060.jpg" /></p> <h2>評估結果</h2> <p>DeepSeek-V3.2-Exp的評估涵蓋模型能力、推理成本及未來驗證計劃。在<strong>模型能力</strong>方面，與DeepSeek-V3.1-Terminus進行了全面基準測試對比。結果顯示，儘管效率顯著提升，V3.2-Exp在短、長上下文任務中均<strong>未出現明顯性能下降</strong>。在部分基準測試中，微小的性能下降主要是由於生成token數量減少，若生成token數與V3.1相近，性能差距便會消失。此外，兩款模型的強化學習訓練曲線趨勢高度接近，證明DSA機制不影響模型的訓練穩定性。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_20.jpg" /></p> <p><strong>推理成本</strong>是V3.2-Exp的核心優勢。理論上，傳統密集注意力的核心計算複雜度為O(L²)，而引入DSA後降至O(L×k)，實現從平方級到線性級的顯著降低。雖然閃電索引器本身的計算複雜度仍為O(L²)，但由於其頭數少且支援FP8精度，實際計算成本極低。在H800 GPU集群上的實際服務測試顯示，DeepSeek-V3.2-Exp的每百萬token成本始終低於DeepSeek-V3.1-Terminus，尤其在128K等<strong>長上下文</strong>場景下，成本優勢更為明顯。為優化短序列處理效率，團隊還引入了掩碼MHA模式。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_24.jpg" /></p> <p>最後，DeepSeek團隊計劃在<strong>真實世界場景中進行更大規模的測試</strong>，以發現並解決稀疏注意力架構可能存在的潛在局限，確保DeepSeek-V3.2-Exp在實際應用中具備更穩定、更出色的表現。總體而言，DeepSeek-V3.2-Exp透過引入DSA，成功在長上下文處理效率上取得突破，同時保持了與前一版本相當的性能水平，為AI任務提供了更高效、更經濟的選擇。</p> <hr /><hr />================================================</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">大家好，这里是最佳拍档，我是大飞。昨天。DeepSeek团队推出了最新模型DeepSeek-V3.2-Exp。在之前 DeepSeek-V3.1-Terminus 的基础上。通过持续训练集成了一种全新的稀疏注意力机制。也就是DeepSeek 稀疏注意力。简称DSA。摇身一变成为了一款实验性的稀疏注意力模型。再搭配上一个高效的闪电索引器。V3.2-Exp在训练和推理两个关键环节都实现了显著的效率提升。尤其是在处理长上下文场景的时候。这种效率优势会更加明显。这期视频我们就从架构、训练、评估这几个核心维度。给大家拆解一下这款模型的细节。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先来看架构部分。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这次DeepSeek-V3.2-Exp在架构上唯一的修改。就是通过持续训练。引入了DSA。那这个DSA到底是由什么构成的呢？它主要包含两个关键组件。一个是闪电索引器。另一个是细粒度的token选择机制。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_62.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">我们先来了解一下闪电索引器。它的核心作用是计算查询token（query token）和前面的某个token之间的索引分数It,s。然后通过这个分数来决定。查询token要选择哪些token进行后续的注意力计算。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这个索引分数的计算公式是这样的。这里面有几个关键参数需要跟大家解释清楚。其中Hᴵ代表的是索引器头（indexer heads）的数量；。qt,jᴵ和wt,jᴵ是一个实数。是从查询token ht中推导出来的；。而ksᴵ是从前面的token hs中推导出来的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">可能有朋友会问。为什么选择ReLU作为激活函数呢？这里主要是出于吞吐量的考虑。ReLU函数在计算效率上有一定的优势。能够更好地配合闪电索引器实现高效计算。而且还有一个很重要的点。闪电索引器的头数比较少。同时还可以用FP8精度来实现。这就使得它的计算效率非常突出。不会因为引入新的组件而导致计算成本大幅增加。这也是后续模型整体效率提升的一个重要基础。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_130.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">有了闪电索引器计算出的索引分数之后。接下来就要靠细粒度的token选择机制来发挥作用了。对于每个查询token ht。这个机制会根据索引分数。只筛选出索引分数排在前k位的token所对应的键值对（key-value entries）。然后，注意力输出ut的计算。就是让查询token ht和这些经过稀疏筛选后的键值对。用{cs}表示，进行注意力机制的运算。具体公式是这样的。这样一来。模型就不需要对所有的token都进行注意力计算。只针对筛选后的关键token进行处理。从而在保证效果的前提下。大幅降低计算量。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">不过，DSA并不是孤立存在的。它是在DeepSeek在2024年提出的MLA架构的基础上实例化来的。为什么要选择这样的方式呢？主要是考虑到DeepSeek-V3.2-Exp是从DeepSeek-V3.1-Terminus进行持续训练得到的。基于MLA来实例化DSA。能够更好地保证训练的连贯性和兼容性。而且在kernel层面。为了提升计算效率。每个键值对都必须在多个查询之间共享。这一点是袁等人在2025年的研究中提到的关键结论。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_203.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">基于这样的需求。研发团队选择了在MLA的MQA模式。也就是多查询注意力（Multi-Query Attention）的基础上实现DSA。这种MQA模式是Transformer作者之一诺姆·沙泽尔（N。Shazeer）在2019年提出的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在这种模式下，MLA中的每个潜在向量。都会在查询token的所有查询头（query heads）之间共享。这样就能很好地满足键值对共享的需求。进一步提升计算效率。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">关于DSA基于MLA的具体架构。大家可以参考原文中的图1。里面清晰地展示了整个注意力架构的细节。尤其是绿色部分。直观地呈现了DSA如何根据索引器筛选出前k个键值对的过程。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">另外。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">为了让大家能够更清晰地了解具体的实现细节。研发团队还提供了DeepSeek-V3.2-Exp的开源实现。大家如果想深入研究代码层面的逻辑。这个地址会非常有帮助。同时。原文的附录A还详细说明了MLA的MQA模式和MHA模式（Multi-Head Attention。多头注意力）之间的区别。感兴趣的朋友也可以去附录部分进一步阅读。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_265.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">讲完了架构，咱们再来看训练流程。DeepSeek-V3.2-Exp的训练是从DeepSeek-V3.1-Terminus的基础checkpoint开始的。这个基础checkpoint的上下文长度已经扩展到了128K。为后续处理长上下文任务打下了很好的基础。整个训练过程分为两个主要阶段。持续预训练和后训练。每个阶段又有各自细分的步骤和目标。咱们一步步来看。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先是持续预训练阶段。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这个阶段又分为两个训练步骤。分别是密集预热阶段和稀疏训练阶段。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">需要先说明的是。这两个阶段所使用的训练数据分布。是完全和DeepSeek-V3.1-Terminus在扩展128K长上下文时所用的数据保持一致的。这样做的目的是为了保证数据的连贯性。减少因数据分布差异对模型训练效果产生的干扰。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第一个步骤是密集预热阶段。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这个阶段的核心目标是初始化闪电索引器。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在这个阶段。模型仍然保持着密集注意力的计算方式。也就是说。此时还没有启用稀疏筛选的机制。同时。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_332.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">除了闪电索引器之外。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">模型的其他所有参数都处于冻结状态。不进行更新。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">那为什么要这么做呢？主要是为了让索引器的输出能够和主注意力的分布保持一致。为后续的稀疏训练做好铺垫。具体怎么实现这种对齐呢？对于第t个查询token。研发团队首先会将主注意力的分数在所有注意力头上进行求和。然后沿着序列维度对这个求和结果进行L1归一化。得到一个目标分布p。然后，基于这个目标分布p。团队设置了KL散度损失作为闪电索引器的训练目标。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">具体的损失函数公式是这样的。其中Softmax函数是为了将索引器计算出的索引分数转换为概率分布。以便和目标分布p进行比较。通过最小化两者之间的KL散度。让索引器的输出尽可能接近主注意力的分布。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在这个预热阶段。所使用的学习率是10的负3次方。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">训练步数非常少，只训练了1000步。每一步包含16个序列。每个序列有128K个token。咱们可以算一下。整个密集预热阶段总共训练的token数量是1000步 × 16序列/步 × 128K token/序列。约等于2.1B个token。这样的训练量能够在短时间内完成索引器的初始化。同时避免过度训练导致其他问题。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_411.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">完成了密集预热之后。就进入到了持续预训练的第二个步骤。稀疏训练阶段。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在这个阶段。研发团队引入了前面提到的细粒度token选择机制。开始让模型适应DSA的稀疏模式。并且此时会对模型的所有参数进行优化更新。而不再是只训练索引器。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在这个阶段。仍然需要保持索引器的输出和主注意力分布的对齐。但和密集预热阶段不同的是。此时只考虑经过筛选后的token集合Sₜ。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这个集合St的定义是所有满足It,s属于It,:中前k个分数的s所构成的集合。因此。KL散度损失函数也相应地进行了调整。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">也就是只计算筛选后的token在目标分布和索引器输出分布之间的KL散度。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这里有一个非常关键的技术细节。研发团队将索引器的输入从计算图中分离了出来。进行单独的优化。这意味着什么呢？简单来说。索引器的训练信号只来自于刚才提到的KL散度损失。而主模型的优化则完全依据语言建模损失（language modeling loss）。两者的优化过程相互独立。这样做能够更好地平衡索引器和主模型的训练。避免相互干扰。保证各自训练目标的实现。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_485.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在稀疏训练阶段。所使用的学习率是7.3×10⁻⁶。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">每个查询token会选择2048个键值对token进行后续的注意力计算。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">训练步数比预热阶段多很多。总共训练了15000步。每一步包含480个序列。每个序列同样是128K个token。咱们再来计算一下这个阶段的总训练token量。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">15000步 × 480序列/步 × 128K token/序列。约为943.7B个token。如此大的训练量能够让模型充分适应稀疏模式。保证模型在稀疏注意力机制下的性能。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">持续预训练完成之后。就进入到了后训练阶段。这个阶段的目标是打造最终的DeepSeek-V3.2-Exp模型。需要强调的是，在后训练阶段。模型仍然采用和稀疏持续预训练阶段相同的方式来使用稀疏注意力。这样可以保证训练过程的一致性。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">为了严谨地评估引入DSA之后对模型的影响。研发团队在DeepSeek-V3.2-Exp的后训练过程中。保持了和DeepSeek-V3.1-Terminus完全相同的后训练流水线、算法以及数据。这样一来。两者在后训练阶段的差异就只来源于是否引入了DSA。从而能够更准确地判断DSA对模型性能和效率的影响。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_565.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">后训练阶段主要包括两个核心环节。分别是专家蒸馏（Specialist Distillation）和混合RL训练（Mixed RL Training）。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">先来看专家蒸馏环节。这个环节的思路是。针对每个特定的任务或领域。先开发一个专门的模型。也就是“专家模型”，这些专家模型都只专注于自己所对应的领域。所有的专家模型都是从同一个预训练的DeepSeek-V3.2基础checkpoint进行微调得到的。这样可以保证专家模型之间的基础能力处于同一水平。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">具体涵盖了哪些领域呢？除了常见的写作任务和通用问答任务之外。还包括五个专门的领域。包括数学、竞赛编程（competitive programming）、通用逻辑推理（general logical reasoning）、智能体编程（agentic coding）以及智能体搜索（agentic search）。每个专家模型的训练都投入了大规模的强化学习（RL）计算资源。确保专家模型在各自领域内能够达到较高的性能水平。而且。为了让训练数据更贴合不同的推理和生成需求。研发团队还使用了不同的模型来生成两种类型的训练数据。一种是用于长链式推理（long chain-of-thought reasoning）的训练数据。对应的是“思考模式”（thinking mode）；。另一种是用于直接响应生成（direct response generation）的训练数据。对应的是“非思考模式”（non-thinking mode）。当这些专家模型训练完成之后。它们就会被用来生成特定领域的数据。这些数据会用于最终DeepSeek-V3.2-Exp模型checkpoint的训练。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_643.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">从实验结果来看。使用这些蒸馏后的数据训练出来的模型。性能只比那些专门的领域专家模型略低一点。而且通过后续的RL训练。这个性能差距能够被有效的消除。这就意味着，通过专家蒸馏。DeepSeek-V3.2-Exp能够在多个领域同时具备接近专家模型的能力。而不需要为每个领域单独训练一个模型。大大提升了模型的通用性和效率。接下来呢。是混合RL训练环节。在DeepSeek V3.2 EXP的后训练中。研发团队仍然采用了分组相对策略优化GRPO作为RL训练的算法。不过，和之前的DeepSeek模型相比。这里有一个重要的改进。那就是之前的模型采用的是多阶段强化学习训练。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而DeepSeek-V3.2-Exp则将推理训练、智能体训练以及人类对齐训练这三个原本分开的阶段合并成了一个RL阶段。这种合并带来了两个显著的好处。一方面。它能够在不同的领域之间实现更好的性能平衡。避免某个领域因为训练阶段的侧重不同而导致性能过强或过弱；。另一方面。它成功规避了多阶段训练范式中常见的“灾难性遗忘”问题。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_715.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">所谓灾难性遗忘。就是模型在学习新任务或新领域知识的时候。会忘记之前已经学会的知识。而单阶段训练则很好地缓解了这个问题。保证了模型在各个领域知识的稳定性。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在奖励设计方面。研发团队也考虑得非常细致。针对不同类型的任务设置了不同的奖励机制。对于推理任务和智能体任务。采用了基于规则的结果奖励（rule-based outcome reward）、长度惩罚（length penalty）以及语言一致性奖励（language consistency reward）。基于规则的结果奖励主要是根据任务的客观结果来判断模型输出是否正确。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如推理任务是否得出正确答案。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">智能体任务是否完成指定目标；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">长度惩罚则是为了避免模型生成过长但无意义的内容。鼓励简洁有效的输出；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">语言一致性奖励则是保证模型输出的语言在逻辑、风格等方面保持一致。提升输出质量。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而对于通用任务。研发团队采用了生成式奖励模型（generative reward model）。并且为每个提示词都设置了专门的评估标准（rubrics）。这种奖励模型能够更灵活地评估通用任务中模型输出的质量。因为通用任务的评价标准往往不像推理或智能体任务那样客观明确。需要更细致的评估维度。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_787.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在整个奖励设计过程中。研发团队重点平衡了两个关键的权衡关系。第一个是“长度与准确性”的权衡。也就是既要避免模型为了追求准确性而生成过长的内容。也要防止为了缩短长度而牺牲准确性；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第二个是“语言一致性与准确性”的权衡。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">即保证语言一致性的同时。不影响模型输出的准确性。确保模型在多个评估维度上都能有出色的表现。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">讲完了训练流程。咱们再来看大家最关心的评估结果。毕竟一款模型的好坏。最终还是要靠实际的评估数据来证明。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">DeepSeek-V3.2-Exp的评估主要从三个方面展开。分别是模型能力、推理成本。以及未来的真实场景验证计划。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先是模型能力的评估。研发团队选择了一系列涵盖不同能力的基准测试（benchmark）。将DeepSeek-V3.2-Exp和DeepSeek-V3.1-Terminus进行了全面的对比。具体的对比数据大家可以参考这张表。从整体结果来看。尽管DeepSeek-V3.2-Exp在长序列处理的计算效率上有了显著提升。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_852.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">但是无论是在短上下文任务还是长上下文任务中。它都没有出现明显的性能下降。这一点非常关键。因为很多时候模型效率的提升往往会伴随着性能的损失。而DeepSeek-V3.2-Exp很好地兼顾了效率和性能。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">不过，在个别基准测试中。DeepSeek-V3.2-Exp的性能确实比DeepSeek-V3.1-Terminus略低一些。比如在GPQA-Diamond（Pass@1）、HLE（Pass@1）以及HMMT 2025（Pass@1）这几项测试中。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">那是什么原因导致了这种性能差异呢？研发团队经过分析发现。主要是因为DeepSeek-V3.2-Exp生成的推理token数量更少。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而有趣的是。当使用那些生成的token数量。与DeepSeek-V3.1-Terminus相近的中间checkpoint进行测试时。这种性能差距就消失了。这说明。性能差异并不是因为DSA机制本身导致的。而是和模型生成内容的长度相关。只要调整好生成token的数量。DeepSeek-V3.2-Exp完全能够达到和之前版本相当的性能水平。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_919.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">除了基准测试的静态结果之外。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">研发团队还对比了两款模型的强化学习训练曲线。其中图（a）是BrowseComp任务的训练曲线。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">图（b）是SWE Verified任务的训练曲线。从曲线可以看出，在整个训练过程中。DeepSeek-V3.1-Terminus和DeepSeek-V3.2-Exp在这两项任务上的准确率都在稳步提升。而且两条曲线的走势非常接近。几乎是同步上升的。同时。图中的虚线代表模型输出的平均token数量。两款模型的输出token数量也保持着相似的变化趋势。这种接近的训练曲线充分说明了DSA机制并没有影响模型的训练稳定性。DeepSeek-V3.2-Exp在训练过程中能够像之前的版本一样稳定地学习和提升性能。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">接下来是推理成本的评估。这也是DeepSeek-V3.2-Exp的核心优势所在。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">咱们先从理论层面来看。DSA机制到底是如何降低推理成本的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在传统的密集注意力机制中。主模型的核心注意力计算复杂度是O(L²)，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_981.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">其中L是序列的长度。这意味着当序列长度大幅增加时。计算量会呈平方级增长。这也是长上下文处理效率低的主要原因之一。而引入DSA之后。核心注意力计算复杂度降低到了O(L×k)，其中k是每个查询token选择的键值对token数量。而且k远小于L（k≪L）。这样一来，当处理长序列时。计算量的增长速度就从平方级变成了线性级。大幅降低了计算成本。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">可能有朋友会问。那闪电索引器的计算复杂度呢？确实。闪电索引器的计算复杂度仍然是O(L²)，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">但是它的计算量和DeepSeek-V3.1-Terminus中MLA的计算量相比。要少得多。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">一方面是因为闪电索引器的头数少。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">另一方面是因为它可以用FP8精度实现。这些都使得闪电索引器的实际计算成本非常低。再加上研发团队针对DSA做的优化实现。最终使得DeepSeek-V3.2-Exp在长上下文场景下实现了显著的端到端加速。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_1045.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">为了让大家更直观地感受到这种成本差异。研发团队在H800 GPU集群上部署了实际的服务。并且对两款模型的推理成本进行了 benchmark 测试。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">测试时GPU的租赁价格按照每小时2美元计算。具体的成本对比数据可以参考这张图。其中图（a）是预填充（Prefilling）阶段的成本。图（b）是解码（Decoding）阶段的成本。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">从图中可以清晰地看到。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">随着序列中token位置的增加。比如从0K到128K。DeepSeek-V3.2-Exp的每百万token成本始终低于DeepSeek-V3.1-Terminus。而且在token位置达到128K这种长上下文场景下。成本优势更加明显。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">另外。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">还有一个细节需要跟大家提一下。在处理短序列预填充的时候。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">研发团队专门实现了一种掩码MHA（masked MHA）模式来模拟DSA的效果。这样做的目的是在短上下文条件下。也能让模型达到更高的效率。避免在短序列场景下因为DSA的稀疏机制反而导致效率下降。充分考虑了不同序列长度场景下的效率优化。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_1107.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">最后，关于未来的验证计划。研发团队也表现出了非常严谨的态度。虽然目前的内部评估结果显示DeepSeek-V3.2-Exp的表现很有前景。但是团队并没有就此止步。而是计划在真实世界场景中进行更大规模的测试。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">为什么要这么做呢？因为实验室环境下的基准测试往往无法完全覆盖真实场景中的各种复杂情况。真实场景中可能会遇到各种意想不到的问题。比如不同类型的长上下文数据、不同的推理任务需求等。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">通过大规模的真实场景测试。研发团队希望能够发现稀疏注意力架构可能存在的潜在局限。从而进一步优化模型。让DeepSeek-V3.2-Exp在实际应用中能够有更稳定、更出色的表现。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">到这里。DeepSeek-V3.2-Exp的核心内容就给大家介绍得差不多了。这款模型通过引入DSA稀疏注意力机制。成功在长上下文处理的效率上实现了突破。同时又保持了和之前版本相当的性能水平。为长上下文AI任务的应用提供了一个更高效、更经济的选择。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_1170.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">无论是架构设计中的细节考量。还是训练过程中的严谨优化。再到评估阶段的全面验证。都能看出DeepSeek-AI团队在这款模型上投入的心血和专业能力。当然。AI模型的发展是一个持续迭代的过程。DeepSeek-V3.2-Exp作为一款实验性模型。未来还有很大的优化空间。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如进一步提升稀疏筛选的准确性、在更多真实场景中验证性能等。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">相信随着后续的不断改进。这款模型会在更多领域发挥重要作用。也期待DeepSeek团队能够带来更多像这样兼顾性能和效率的优秀模型。感谢收看本期视频，我们下期再见。</p>

<hr style="clear:both" />
=============
<p><a href="https://www.youtube.com/watch?v=CA2lzW9INrQ">https://www.youtube.com/watch?v=CA2lzW9INrQ</a></p><p>昨天，DeepSeek团队推出了最新模型DeepSeek-V3.2-Exp，在之前 DeepSeek-V3.1-Terminus 的基础上，通过持续训练集成了一种全新的稀疏注意力机制，也就是DeepSeek 稀疏注意力，简称DSA，摇身一变成为了一款实验性的稀疏注意力模型。再搭配上一个高效的闪电索引器，V3.2-Exp在训练和推理两个关键环节都实现了显著的效率提升，尤其是在处理长上下文场景的时候，这种效率优势会更加明显。这期视频我们就从架构、训练、评估这几个核心维度，给大家拆解一下这款模型的细节。</p><p><a href="https://api-docs.deepseek.com/zh-cn/news/news250929">https://api-docs.deepseek.com/zh-cn/news/news250929</a></p><p><a href="https://github.com/deepseek-ai/DeepSeek-V3.2-Exp/blob/main/DeepSeek_V3_2.pdf">https://github.com/deepseek-ai/DeepSeek-V3.2-Exp/blob/main/DeepSeek_V3_2.pdf</a></p><p><a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Exp/tree/main/inference">https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Exp/tree/main/inference</a></p>]]>
      </description>
      <content:encoded><![CDATA[<hr style="clear:both" />

<p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://img.youtube.com/vi/CA2lzW9INrQ/maxresdefault.jpg" /></p><p><a href="https://www.youtube.com/watch?v=CA2lzW9INrQ">https://www.youtube.com/watch?v=CA2lzW9INrQ</a></p><h1>值得閱讀的理由</h1> <ul> <li>深入了解DeepSeek-V3.2-Exp如何透過其獨特的<strong>稀疏注意力機制（DSA）</strong>，在處理<strong>長上下文</strong>任務時實現顯著的效率提升。</li> <li>探索模型的<strong>架構細節</strong>，包括<strong>閃電索引器</strong>與<strong>細粒度token選擇機制</strong>如何協同運作，以及其訓練流程中<strong>密集預熱</strong>與<strong>稀疏訓練</strong>階段的精妙設計。</li> <li>掌握該模型在<strong>能力評估</strong>與<strong>推理成本</strong>上的實際表現，了解DeepSeek團隊如何平衡<strong>性能</strong>與<strong>效率</strong>，並為未來的AI應用提供更具經濟效益的解決方案。</li> </ul> <hr /> <h1>摘要</h1> <p>在大飛的介紹下，DeepSeek團隊推出了其最新實驗性模型<strong>DeepSeek-V3.2-Exp</strong>。該模型在DeepSeek-V3.1-Terminus的基礎上，透過持續訓練引入了全新的<strong>稀疏注意力機制（DeepSeek Sparse Attention, DSA）</strong>。結合高效的<strong>閃電索引器</strong>，V3.2-Exp在訓練和推理效率上實現了顯著提升，尤其在處理<strong>長上下文</strong>場景時，其效率優勢更為突出。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_50.jpg" /></p> <h2>架構分析</h2> <p>DeepSeek-V3.2-Exp的架構核心是DSA，主要包含兩個關鍵組件：<strong>閃電索引器</strong>和<strong>細粒度的token選擇機制</strong>。<strong>閃電索引器</strong>負責計算查詢token與前面token之間的索引分數，並根據分數決定哪些token進行後續注意力計算。為提升計算效率，索引器採用ReLU激活函數，且頭數較少並支援FP8精度，確保其計算成本極低。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_160.jpg" /></p> <p>在索引器篩選出關鍵token後，<strong>細粒度的token選擇機制</strong>會根據索引分數，只挑選出排名前k位的token所對應的鍵值對進行注意力運算。這使得模型無需對所有token都進行計算，大幅降低了計算量。DSA是基於DeepSeek在2024年提出的<strong>MLA（Multi-Query Attention, 多查詢注意力）架構</strong>實現的，以確保與V3.1的訓練連貫性，並支援鍵值對在多個查詢間共享，進一步提升計算效率。詳細的架構和開源實現可供進一步參考。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_320.jpg" /></p> <h2>訓練流程</h2> <p>DeepSeek-V3.2-Exp的訓練始於DeepSeek-V3.1-Terminus的基礎檢查點，其上下文長度已擴展至128K。整個訓練過程分為兩個主要階段：<strong>持續預訓練</strong>和<strong>後訓練</strong>。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_350.jpg" /></p> <h3>持續預訓練階段</h3> <p>持續預訓練包含<strong>密集預熱階段</strong>和<strong>稀疏訓練階段</strong>，兩者使用與V3.1擴展長上下文時相同的數據分佈。在<strong>密集預熱階段</strong>，模型仍採密集注意力計算，只初始化<strong>閃電索引器</strong>，而模型其他參數凍結。此階段的目標是透過最小化索引器輸出與主注意力分佈之間的<strong>KL散度損失</strong>，讓索引器的輸出與主注意力分布保持一致。此階段訓練量約為2.1B個token，旨在短時間內完成索引器初始化。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_495.jpg" /></p> <p>完成預熱後，進入<strong>稀疏訓練階段</strong>。此時模型啟用DSA的稀疏模式，並優化所有參數。KL散度損失函數調整為僅計算經過篩選的token。值得一提的是，索引器的輸入與主模型的計算圖分離，索引器僅依賴KL散度損失優化，而主模型則依據語言建模損失，確保兩者優化過程相互獨立。此階段訓練量約為943.7B個token，確保模型充分適應稀疏模式下的性能。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_640.jpg" /></p> <h3>後訓練階段</h3> <p>後訓練階段旨在打造最終的DeepSeek-V3.2-Exp模型，並保持與稀疏持續預訓練階段相同的稀疏注意力使用方式。為嚴謹評估DSA的影響，後訓練流水線、算法和數據均與DeepSeek-V3.1-Terminus完全相同。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_690.jpg" /></p> <p>此階段包含<strong>專家蒸餾</strong>和<strong>混合RL訓練</strong>。<strong>專家蒸餾</strong>透過從同一預訓練基礎檢查點微調，開發出多個專注於不同領域（如數學、編程、邏輯推理等）的<strong>專家模型</strong>。這些專家模型用於生成兩種訓練數據：<strong>長鏈式推理（思考模式）</strong>和<strong>直接響應生成（非思考模式）</strong>。最終，這些蒸餾後的數據用於訓練DeepSeek-V3.2-Exp模型，使其在多個領域具備接近專家模型的能力，而性能差距可透過後續RL訓練消除。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_840.jpg" /></p> <p><strong>混合RL訓練</strong>階段，DeepSeek-V3.2-Exp採用<strong>分組相對策略優化（GRPO）</strong>，並將推理訓練、智能體訓練和人類對齊訓練<strong>合併為單一RL階段</strong>。這種合併不僅在不同領域間實現了更好的性能平衡，還成功規避了多階段訓練中常見的「<strong>災難性遺忘</strong>」問題。獎勵設計也十分細緻：推理和智能體任務採用基於規則的結果獎勵、長度懲罰和語言一致性獎勵；通用任務則採用生成式獎勵模型及評估標準。獎勵設計著重平衡「長度與準確性」以及「語言一致性與準確性」。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_1060.jpg" /></p> <h2>評估結果</h2> <p>DeepSeek-V3.2-Exp的評估涵蓋模型能力、推理成本及未來驗證計劃。在<strong>模型能力</strong>方面，與DeepSeek-V3.1-Terminus進行了全面基準測試對比。結果顯示，儘管效率顯著提升，V3.2-Exp在短、長上下文任務中均<strong>未出現明顯性能下降</strong>。在部分基準測試中，微小的性能下降主要是由於生成token數量減少，若生成token數與V3.1相近，性能差距便會消失。此外，兩款模型的強化學習訓練曲線趨勢高度接近，證明DSA機制不影響模型的訓練穩定性。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_20.jpg" /></p> <p><strong>推理成本</strong>是V3.2-Exp的核心優勢。理論上，傳統密集注意力的核心計算複雜度為O(L²)，而引入DSA後降至O(L×k)，實現從平方級到線性級的顯著降低。雖然閃電索引器本身的計算複雜度仍為O(L²)，但由於其頭數少且支援FP8精度，實際計算成本極低。在H800 GPU集群上的實際服務測試顯示，DeepSeek-V3.2-Exp的每百萬token成本始終低於DeepSeek-V3.1-Terminus，尤其在128K等<strong>長上下文</strong>場景下，成本優勢更為明顯。為優化短序列處理效率，團隊還引入了掩碼MHA模式。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_24.jpg" /></p> <p>最後，DeepSeek團隊計劃在<strong>真實世界場景中進行更大規模的測試</strong>，以發現並解決稀疏注意力架構可能存在的潛在局限，確保DeepSeek-V3.2-Exp在實際應用中具備更穩定、更出色的表現。總體而言，DeepSeek-V3.2-Exp透過引入DSA，成功在長上下文處理效率上取得突破，同時保持了與前一版本相當的性能水平，為AI任務提供了更高效、更經濟的選擇。</p> <hr /><hr />================================================</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">大家好，这里是最佳拍档，我是大飞。昨天。DeepSeek团队推出了最新模型DeepSeek-V3.2-Exp。在之前 DeepSeek-V3.1-Terminus 的基础上。通过持续训练集成了一种全新的稀疏注意力机制。也就是DeepSeek 稀疏注意力。简称DSA。摇身一变成为了一款实验性的稀疏注意力模型。再搭配上一个高效的闪电索引器。V3.2-Exp在训练和推理两个关键环节都实现了显著的效率提升。尤其是在处理长上下文场景的时候。这种效率优势会更加明显。这期视频我们就从架构、训练、评估这几个核心维度。给大家拆解一下这款模型的细节。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先来看架构部分。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这次DeepSeek-V3.2-Exp在架构上唯一的修改。就是通过持续训练。引入了DSA。那这个DSA到底是由什么构成的呢？它主要包含两个关键组件。一个是闪电索引器。另一个是细粒度的token选择机制。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_62.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">我们先来了解一下闪电索引器。它的核心作用是计算查询token（query token）和前面的某个token之间的索引分数It,s。然后通过这个分数来决定。查询token要选择哪些token进行后续的注意力计算。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这个索引分数的计算公式是这样的。这里面有几个关键参数需要跟大家解释清楚。其中Hᴵ代表的是索引器头（indexer heads）的数量；。qt,jᴵ和wt,jᴵ是一个实数。是从查询token ht中推导出来的；。而ksᴵ是从前面的token hs中推导出来的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">可能有朋友会问。为什么选择ReLU作为激活函数呢？这里主要是出于吞吐量的考虑。ReLU函数在计算效率上有一定的优势。能够更好地配合闪电索引器实现高效计算。而且还有一个很重要的点。闪电索引器的头数比较少。同时还可以用FP8精度来实现。这就使得它的计算效率非常突出。不会因为引入新的组件而导致计算成本大幅增加。这也是后续模型整体效率提升的一个重要基础。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_130.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">有了闪电索引器计算出的索引分数之后。接下来就要靠细粒度的token选择机制来发挥作用了。对于每个查询token ht。这个机制会根据索引分数。只筛选出索引分数排在前k位的token所对应的键值对（key-value entries）。然后，注意力输出ut的计算。就是让查询token ht和这些经过稀疏筛选后的键值对。用{cs}表示，进行注意力机制的运算。具体公式是这样的。这样一来。模型就不需要对所有的token都进行注意力计算。只针对筛选后的关键token进行处理。从而在保证效果的前提下。大幅降低计算量。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">不过，DSA并不是孤立存在的。它是在DeepSeek在2024年提出的MLA架构的基础上实例化来的。为什么要选择这样的方式呢？主要是考虑到DeepSeek-V3.2-Exp是从DeepSeek-V3.1-Terminus进行持续训练得到的。基于MLA来实例化DSA。能够更好地保证训练的连贯性和兼容性。而且在kernel层面。为了提升计算效率。每个键值对都必须在多个查询之间共享。这一点是袁等人在2025年的研究中提到的关键结论。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_203.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">基于这样的需求。研发团队选择了在MLA的MQA模式。也就是多查询注意力（Multi-Query Attention）的基础上实现DSA。这种MQA模式是Transformer作者之一诺姆·沙泽尔（N。Shazeer）在2019年提出的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在这种模式下，MLA中的每个潜在向量。都会在查询token的所有查询头（query heads）之间共享。这样就能很好地满足键值对共享的需求。进一步提升计算效率。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">关于DSA基于MLA的具体架构。大家可以参考原文中的图1。里面清晰地展示了整个注意力架构的细节。尤其是绿色部分。直观地呈现了DSA如何根据索引器筛选出前k个键值对的过程。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">另外。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">为了让大家能够更清晰地了解具体的实现细节。研发团队还提供了DeepSeek-V3.2-Exp的开源实现。大家如果想深入研究代码层面的逻辑。这个地址会非常有帮助。同时。原文的附录A还详细说明了MLA的MQA模式和MHA模式（Multi-Head Attention。多头注意力）之间的区别。感兴趣的朋友也可以去附录部分进一步阅读。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_265.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">讲完了架构，咱们再来看训练流程。DeepSeek-V3.2-Exp的训练是从DeepSeek-V3.1-Terminus的基础checkpoint开始的。这个基础checkpoint的上下文长度已经扩展到了128K。为后续处理长上下文任务打下了很好的基础。整个训练过程分为两个主要阶段。持续预训练和后训练。每个阶段又有各自细分的步骤和目标。咱们一步步来看。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先是持续预训练阶段。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这个阶段又分为两个训练步骤。分别是密集预热阶段和稀疏训练阶段。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">需要先说明的是。这两个阶段所使用的训练数据分布。是完全和DeepSeek-V3.1-Terminus在扩展128K长上下文时所用的数据保持一致的。这样做的目的是为了保证数据的连贯性。减少因数据分布差异对模型训练效果产生的干扰。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第一个步骤是密集预热阶段。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这个阶段的核心目标是初始化闪电索引器。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在这个阶段。模型仍然保持着密集注意力的计算方式。也就是说。此时还没有启用稀疏筛选的机制。同时。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_332.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">除了闪电索引器之外。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">模型的其他所有参数都处于冻结状态。不进行更新。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">那为什么要这么做呢？主要是为了让索引器的输出能够和主注意力的分布保持一致。为后续的稀疏训练做好铺垫。具体怎么实现这种对齐呢？对于第t个查询token。研发团队首先会将主注意力的分数在所有注意力头上进行求和。然后沿着序列维度对这个求和结果进行L1归一化。得到一个目标分布p。然后，基于这个目标分布p。团队设置了KL散度损失作为闪电索引器的训练目标。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">具体的损失函数公式是这样的。其中Softmax函数是为了将索引器计算出的索引分数转换为概率分布。以便和目标分布p进行比较。通过最小化两者之间的KL散度。让索引器的输出尽可能接近主注意力的分布。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在这个预热阶段。所使用的学习率是10的负3次方。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">训练步数非常少，只训练了1000步。每一步包含16个序列。每个序列有128K个token。咱们可以算一下。整个密集预热阶段总共训练的token数量是1000步 × 16序列/步 × 128K token/序列。约等于2.1B个token。这样的训练量能够在短时间内完成索引器的初始化。同时避免过度训练导致其他问题。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_411.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">完成了密集预热之后。就进入到了持续预训练的第二个步骤。稀疏训练阶段。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在这个阶段。研发团队引入了前面提到的细粒度token选择机制。开始让模型适应DSA的稀疏模式。并且此时会对模型的所有参数进行优化更新。而不再是只训练索引器。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在这个阶段。仍然需要保持索引器的输出和主注意力分布的对齐。但和密集预热阶段不同的是。此时只考虑经过筛选后的token集合Sₜ。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这个集合St的定义是所有满足It,s属于It,:中前k个分数的s所构成的集合。因此。KL散度损失函数也相应地进行了调整。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">也就是只计算筛选后的token在目标分布和索引器输出分布之间的KL散度。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这里有一个非常关键的技术细节。研发团队将索引器的输入从计算图中分离了出来。进行单独的优化。这意味着什么呢？简单来说。索引器的训练信号只来自于刚才提到的KL散度损失。而主模型的优化则完全依据语言建模损失（language modeling loss）。两者的优化过程相互独立。这样做能够更好地平衡索引器和主模型的训练。避免相互干扰。保证各自训练目标的实现。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_485.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在稀疏训练阶段。所使用的学习率是7.3×10⁻⁶。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">每个查询token会选择2048个键值对token进行后续的注意力计算。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">训练步数比预热阶段多很多。总共训练了15000步。每一步包含480个序列。每个序列同样是128K个token。咱们再来计算一下这个阶段的总训练token量。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">15000步 × 480序列/步 × 128K token/序列。约为943.7B个token。如此大的训练量能够让模型充分适应稀疏模式。保证模型在稀疏注意力机制下的性能。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">持续预训练完成之后。就进入到了后训练阶段。这个阶段的目标是打造最终的DeepSeek-V3.2-Exp模型。需要强调的是，在后训练阶段。模型仍然采用和稀疏持续预训练阶段相同的方式来使用稀疏注意力。这样可以保证训练过程的一致性。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">为了严谨地评估引入DSA之后对模型的影响。研发团队在DeepSeek-V3.2-Exp的后训练过程中。保持了和DeepSeek-V3.1-Terminus完全相同的后训练流水线、算法以及数据。这样一来。两者在后训练阶段的差异就只来源于是否引入了DSA。从而能够更准确地判断DSA对模型性能和效率的影响。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_565.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">后训练阶段主要包括两个核心环节。分别是专家蒸馏（Specialist Distillation）和混合RL训练（Mixed RL Training）。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">先来看专家蒸馏环节。这个环节的思路是。针对每个特定的任务或领域。先开发一个专门的模型。也就是“专家模型”，这些专家模型都只专注于自己所对应的领域。所有的专家模型都是从同一个预训练的DeepSeek-V3.2基础checkpoint进行微调得到的。这样可以保证专家模型之间的基础能力处于同一水平。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">具体涵盖了哪些领域呢？除了常见的写作任务和通用问答任务之外。还包括五个专门的领域。包括数学、竞赛编程（competitive programming）、通用逻辑推理（general logical reasoning）、智能体编程（agentic coding）以及智能体搜索（agentic search）。每个专家模型的训练都投入了大规模的强化学习（RL）计算资源。确保专家模型在各自领域内能够达到较高的性能水平。而且。为了让训练数据更贴合不同的推理和生成需求。研发团队还使用了不同的模型来生成两种类型的训练数据。一种是用于长链式推理（long chain-of-thought reasoning）的训练数据。对应的是“思考模式”（thinking mode）；。另一种是用于直接响应生成（direct response generation）的训练数据。对应的是“非思考模式”（non-thinking mode）。当这些专家模型训练完成之后。它们就会被用来生成特定领域的数据。这些数据会用于最终DeepSeek-V3.2-Exp模型checkpoint的训练。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_643.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">从实验结果来看。使用这些蒸馏后的数据训练出来的模型。性能只比那些专门的领域专家模型略低一点。而且通过后续的RL训练。这个性能差距能够被有效的消除。这就意味着，通过专家蒸馏。DeepSeek-V3.2-Exp能够在多个领域同时具备接近专家模型的能力。而不需要为每个领域单独训练一个模型。大大提升了模型的通用性和效率。接下来呢。是混合RL训练环节。在DeepSeek V3.2 EXP的后训练中。研发团队仍然采用了分组相对策略优化GRPO作为RL训练的算法。不过，和之前的DeepSeek模型相比。这里有一个重要的改进。那就是之前的模型采用的是多阶段强化学习训练。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而DeepSeek-V3.2-Exp则将推理训练、智能体训练以及人类对齐训练这三个原本分开的阶段合并成了一个RL阶段。这种合并带来了两个显著的好处。一方面。它能够在不同的领域之间实现更好的性能平衡。避免某个领域因为训练阶段的侧重不同而导致性能过强或过弱；。另一方面。它成功规避了多阶段训练范式中常见的“灾难性遗忘”问题。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_715.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">所谓灾难性遗忘。就是模型在学习新任务或新领域知识的时候。会忘记之前已经学会的知识。而单阶段训练则很好地缓解了这个问题。保证了模型在各个领域知识的稳定性。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在奖励设计方面。研发团队也考虑得非常细致。针对不同类型的任务设置了不同的奖励机制。对于推理任务和智能体任务。采用了基于规则的结果奖励（rule-based outcome reward）、长度惩罚（length penalty）以及语言一致性奖励（language consistency reward）。基于规则的结果奖励主要是根据任务的客观结果来判断模型输出是否正确。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如推理任务是否得出正确答案。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">智能体任务是否完成指定目标；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">长度惩罚则是为了避免模型生成过长但无意义的内容。鼓励简洁有效的输出；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">语言一致性奖励则是保证模型输出的语言在逻辑、风格等方面保持一致。提升输出质量。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而对于通用任务。研发团队采用了生成式奖励模型（generative reward model）。并且为每个提示词都设置了专门的评估标准（rubrics）。这种奖励模型能够更灵活地评估通用任务中模型输出的质量。因为通用任务的评价标准往往不像推理或智能体任务那样客观明确。需要更细致的评估维度。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_787.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在整个奖励设计过程中。研发团队重点平衡了两个关键的权衡关系。第一个是“长度与准确性”的权衡。也就是既要避免模型为了追求准确性而生成过长的内容。也要防止为了缩短长度而牺牲准确性；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第二个是“语言一致性与准确性”的权衡。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">即保证语言一致性的同时。不影响模型输出的准确性。确保模型在多个评估维度上都能有出色的表现。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">讲完了训练流程。咱们再来看大家最关心的评估结果。毕竟一款模型的好坏。最终还是要靠实际的评估数据来证明。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">DeepSeek-V3.2-Exp的评估主要从三个方面展开。分别是模型能力、推理成本。以及未来的真实场景验证计划。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先是模型能力的评估。研发团队选择了一系列涵盖不同能力的基准测试（benchmark）。将DeepSeek-V3.2-Exp和DeepSeek-V3.1-Terminus进行了全面的对比。具体的对比数据大家可以参考这张表。从整体结果来看。尽管DeepSeek-V3.2-Exp在长序列处理的计算效率上有了显著提升。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_852.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">但是无论是在短上下文任务还是长上下文任务中。它都没有出现明显的性能下降。这一点非常关键。因为很多时候模型效率的提升往往会伴随着性能的损失。而DeepSeek-V3.2-Exp很好地兼顾了效率和性能。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">不过，在个别基准测试中。DeepSeek-V3.2-Exp的性能确实比DeepSeek-V3.1-Terminus略低一些。比如在GPQA-Diamond（Pass@1）、HLE（Pass@1）以及HMMT 2025（Pass@1）这几项测试中。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">那是什么原因导致了这种性能差异呢？研发团队经过分析发现。主要是因为DeepSeek-V3.2-Exp生成的推理token数量更少。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而有趣的是。当使用那些生成的token数量。与DeepSeek-V3.1-Terminus相近的中间checkpoint进行测试时。这种性能差距就消失了。这说明。性能差异并不是因为DSA机制本身导致的。而是和模型生成内容的长度相关。只要调整好生成token的数量。DeepSeek-V3.2-Exp完全能够达到和之前版本相当的性能水平。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_919.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">除了基准测试的静态结果之外。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">研发团队还对比了两款模型的强化学习训练曲线。其中图（a）是BrowseComp任务的训练曲线。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">图（b）是SWE Verified任务的训练曲线。从曲线可以看出，在整个训练过程中。DeepSeek-V3.1-Terminus和DeepSeek-V3.2-Exp在这两项任务上的准确率都在稳步提升。而且两条曲线的走势非常接近。几乎是同步上升的。同时。图中的虚线代表模型输出的平均token数量。两款模型的输出token数量也保持着相似的变化趋势。这种接近的训练曲线充分说明了DSA机制并没有影响模型的训练稳定性。DeepSeek-V3.2-Exp在训练过程中能够像之前的版本一样稳定地学习和提升性能。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">接下来是推理成本的评估。这也是DeepSeek-V3.2-Exp的核心优势所在。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">咱们先从理论层面来看。DSA机制到底是如何降低推理成本的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在传统的密集注意力机制中。主模型的核心注意力计算复杂度是O(L²)，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_981.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">其中L是序列的长度。这意味着当序列长度大幅增加时。计算量会呈平方级增长。这也是长上下文处理效率低的主要原因之一。而引入DSA之后。核心注意力计算复杂度降低到了O(L×k)，其中k是每个查询token选择的键值对token数量。而且k远小于L（k≪L）。这样一来，当处理长序列时。计算量的增长速度就从平方级变成了线性级。大幅降低了计算成本。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">可能有朋友会问。那闪电索引器的计算复杂度呢？确实。闪电索引器的计算复杂度仍然是O(L²)，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">但是它的计算量和DeepSeek-V3.1-Terminus中MLA的计算量相比。要少得多。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">一方面是因为闪电索引器的头数少。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">另一方面是因为它可以用FP8精度实现。这些都使得闪电索引器的实际计算成本非常低。再加上研发团队针对DSA做的优化实现。最终使得DeepSeek-V3.2-Exp在长上下文场景下实现了显著的端到端加速。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_1045.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">为了让大家更直观地感受到这种成本差异。研发团队在H800 GPU集群上部署了实际的服务。并且对两款模型的推理成本进行了 benchmark 测试。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">测试时GPU的租赁价格按照每小时2美元计算。具体的成本对比数据可以参考这张图。其中图（a）是预填充（Prefilling）阶段的成本。图（b）是解码（Decoding）阶段的成本。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">从图中可以清晰地看到。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">随着序列中token位置的增加。比如从0K到128K。DeepSeek-V3.2-Exp的每百万token成本始终低于DeepSeek-V3.1-Terminus。而且在token位置达到128K这种长上下文场景下。成本优势更加明显。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">另外。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">还有一个细节需要跟大家提一下。在处理短序列预填充的时候。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">研发团队专门实现了一种掩码MHA（masked MHA）模式来模拟DSA的效果。这样做的目的是在短上下文条件下。也能让模型达到更高的效率。避免在短序列场景下因为DSA的稀疏机制反而导致效率下降。充分考虑了不同序列长度场景下的效率优化。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_1107.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">最后，关于未来的验证计划。研发团队也表现出了非常严谨的态度。虽然目前的内部评估结果显示DeepSeek-V3.2-Exp的表现很有前景。但是团队并没有就此止步。而是计划在真实世界场景中进行更大规模的测试。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">为什么要这么做呢？因为实验室环境下的基准测试往往无法完全覆盖真实场景中的各种复杂情况。真实场景中可能会遇到各种意想不到的问题。比如不同类型的长上下文数据、不同的推理任务需求等。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">通过大规模的真实场景测试。研发团队希望能够发现稀疏注意力架构可能存在的潜在局限。从而进一步优化模型。让DeepSeek-V3.2-Exp在实际应用中能够有更稳定、更出色的表现。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">到这里。DeepSeek-V3.2-Exp的核心内容就给大家介绍得差不多了。这款模型通过引入DSA稀疏注意力机制。成功在长上下文处理的效率上实现了突破。同时又保持了和之前版本相当的性能水平。为长上下文AI任务的应用提供了一个更高效、更经济的选择。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/CA2lzW9INrQ_1170.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">无论是架构设计中的细节考量。还是训练过程中的严谨优化。再到评估阶段的全面验证。都能看出DeepSeek-AI团队在这款模型上投入的心血和专业能力。当然。AI模型的发展是一个持续迭代的过程。DeepSeek-V3.2-Exp作为一款实验性模型。未来还有很大的优化空间。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如进一步提升稀疏筛选的准确性、在更多真实场景中验证性能等。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">相信随着后续的不断改进。这款模型会在更多领域发挥重要作用。也期待DeepSeek团队能够带来更多像这样兼顾性能和效率的优秀模型。感谢收看本期视频，我们下期再见。</p>

<hr style="clear:both" />
=============
<p><a href="https://www.youtube.com/watch?v=CA2lzW9INrQ">https://www.youtube.com/watch?v=CA2lzW9INrQ</a></p><p>昨天，DeepSeek团队推出了最新模型DeepSeek-V3.2-Exp，在之前 DeepSeek-V3.1-Terminus 的基础上，通过持续训练集成了一种全新的稀疏注意力机制，也就是DeepSeek 稀疏注意力，简称DSA，摇身一变成为了一款实验性的稀疏注意力模型。再搭配上一个高效的闪电索引器，V3.2-Exp在训练和推理两个关键环节都实现了显著的效率提升，尤其是在处理长上下文场景的时候，这种效率优势会更加明显。这期视频我们就从架构、训练、评估这几个核心维度，给大家拆解一下这款模型的细节。</p><p><a href="https://api-docs.deepseek.com/zh-cn/news/news250929">https://api-docs.deepseek.com/zh-cn/news/news250929</a></p><p><a href="https://github.com/deepseek-ai/DeepSeek-V3.2-Exp/blob/main/DeepSeek_V3_2.pdf">https://github.com/deepseek-ai/DeepSeek-V3.2-Exp/blob/main/DeepSeek_V3_2.pdf</a></p><p><a href="https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Exp/tree/main/inference">https://huggingface.co/deepseek-ai/DeepSeek-V3.2-Exp/tree/main/inference</a></p>]]></content:encoded>
      <itunes:image href="https://i.ytimg.com/vi/CA2lzW9INrQ/hqdefault.jpg"/>
      <pubDate>2025-09-30T09:00:18.000Z</pubDate>
    </item><item>
      <title><![CDATA[【人工智能】AI的无限循环 | 神经科学家Anil Seth | 时间 | 熵 | 意识 | 图灵停机问题 | 框架问题 | 热力学第二定律 | 模拟计算 | 凡人计算 | 神经计算 | 动力系统]]></title>
      <link>https://www.youtube.com/watch?v=7Eg2MTM76jM</link>
      <itunes:title><![CDATA[【人工智能】AI的无限循环 | 神经科学家Anil Seth | 时间 | 熵 | 意识 | 图灵停机问题 | 框架问题 | 热力学第二定律 | 模拟计算 | 凡人计算 | 神经计算 | 动力系统]]></itunes:title>
      <itunes:author><![CDATA[最佳拍档]]></itunes:author>
      <itunes:summary>
        <![CDATA[<hr style="clear:both" />

<p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://img.youtube.com/vi/7Eg2MTM76jM/maxresdefault.jpg" /></p><p><a href="https://www.youtube.com/watch?v=7Eg2MTM76jM">https://www.youtube.com/watch?v=7Eg2MTM76jM</a></p><h1>值得閱讀的理由</h1> <ul> <li>深入理解AI為何會陷入重複循環，超越表面技術故障的物理與哲學根源。</li> <li>探討人類意識與「時間」、「熵」的深度連結，揭示我們能擺脫重複行為的生物學基礎。</li> <li>認識神經科學家阿尼爾·塞斯對「智能」與「意識」的獨到見解，及其對通用人工智慧（AGI）未來發展的啟示。</li> </ul> <hr /> <h1>摘要</h1> <h2>AI困境：無限循環的現實案例與核心問題</h2> <p>影片首先透過生活中的多個例子，點出AI系統常出現的「無限循環」問題：從客服電話的無限按鍵迴圈、導航軟體的重複指令，到作者親身經歷的西班牙馬德里機場登機廊橋AI故障。這些看似低級的故障，實則反映了AI與人類本質上的根本差異。神經科學家<strong>阿尼爾·塞斯（Anil Seth）</strong>指出，AI的無限循環根源在於它缺乏「<strong>扎根於時間與熵</strong>」的能力，而這恰恰是人類意識的核心基石。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/7Eg2MTM76jM_95.jpg" /></p> <hr /> <h2>智能與意識的區分：AI的根本局限</h2> <p>阿尼爾·塞斯明確區分了「<strong>智能（Intelligence）</strong>」與「<strong>意識（Consciousness）</strong>」。智能關乎「行動」與「解決問題」，而意識則關乎「存在」與「感受」。他認為，儘管許多科技公司相信智能堆疊到一定程度便能產生意識，但這是個誤區。對人類而言，真正能擺脫無限循環的智能，必須依賴於<strong>意識所帶來的能力</strong>，而這種意識與時間的流動深度綁定。AI之所以容易陷入循環，是因為它沒有這種與時間、熵的「扎根」，也缺乏真正的意識。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/7Eg2MTM76jM_255.jpg" /></p> <hr /> <h2>AI無限循環的底層邏輯：圖靈停機問題與框架問題</h2> <p>影片進一步解釋了AI無限循環的底層科學依據。其一為近一世紀前艾倫·圖靈提出的「<strong>圖靈停機問題（Turing Halting Problem）</strong>」，該理論證明了不存在任何演算法能準確預判一個程式在所有情況下是否會無限循環。其二為1969年提出的「<strong>框架問題（Frame Problem）</strong>」，它指出AI難以區分「相關資訊」與「無關細節」，容易因過度篩選資訊而陷入停滯或循環。這兩個「老問題」在當今深度學習與生成式AI興起的時代似乎被淡忘，但實質上仍在現實中不斷體現AI的局限。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/7Eg2MTM76jM_565.jpg" /></p> <hr /> <h2>時間與熵：生命體與AI的關鍵分野</h2> <p>影片深入探討了「<strong>時間</strong>」與「<strong>熵</strong>」這兩個區分生命體與AI的核心概念。根據<strong>熱力學第二定律</strong>，在孤立系統中，熵（混亂程度）只會增加。然而，人類等生命體是「<strong>開放系統</strong>」，能透過獲取外界能量來「<strong>抗熵</strong>」（降低自身熵，維持有序狀態）。人類意識與時間深度綁定，體現在生化、神經及意識等多個時間尺度上，這種多層面的時間整合使我們能快速判斷並採取有意義的行動。相比之下，數字計算機的「時間」是<strong>扁平、一維</strong>的，它漠視時間流逝，其計算的本質是「狀態躍遷」，與熱力學第二定律脫鉤，導致AI需要消耗大量能量來抵銷熵增的影響（如散熱、糾錯），卻無法產生「生存驅動力」或「自我存在」的感知。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/7Eg2MTM76jM_1140.jpg" /></p> <hr /> <h2>人類的「抗熵本能」與具身意識</h2> <p>人類作為「身處時間之中」的生命，被阿尼爾·塞斯形容為「<strong>具身的（embodied）、植根於環境的（embedded）、與時間同行的（entimed）</strong>」。我們的每個決策都受到「時間壓力」和「生存驅動」的影響，這種「抗熵」的本能促使我們擺脫無意義的重複。即使人類會出現重複行為（如額葉受損、強迫症），那也通常是神經認知功能紊亂的結果，而非天生會循環。這些情況恰恰證明了當意識與時間、身體的連結出現問題時，人類也會陷入循環，進一步佐證了意識在擺脫循環中的關鍵作用。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/7Eg2MTM76jM_22.jpg" /></p> <hr /> <h2>未來AI的探索方向與局限性</h2> <p>影片探討了幾種旨在讓AI擺脫無限循環、實現類人智能的探索方向： <ul> <li><strong>模擬計算機（Analog Computers）</strong>：時間連續，但精度低且易受環境影響。</li> <li><strong>凡人計算（Mortal Computation）</strong>：透過硬體壽命限制，強制程式停止，但非讓AI真正「理解時間」。</li> <li><strong>神經形態計算（Neuromorphic Computing）</strong>：模擬大腦工作方式，更「植根於時間」，但缺乏「生存驅動」和「情感效應」。</li> <li><strong>動力系統方法（Dynamical Systems Approach）</strong>：關注系統在時間中的狀態變化，但仍缺乏「生命體的抗熵本能」。</li> </ul> 阿尼爾·塞斯認為，儘管這些新型AI技術在「靠近時間」，但都未能真正「<strong>扎根於時間與熵</strong>」，缺乏「主動抗熵」的生存驅動和「意識層面的感受」，因此可能仍無法完全擺脫無限循環的陰影，更難實現人類那種「開放式、適應性強」的通用人工智慧（AGI）。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/7Eg2MTM76jM_29.jpg" /></p> <hr /> <h2>意識是生存工具：AI為何沒有「不循環的理由」</h2> <p>阿尼爾·塞斯的結論是，人類能夠擺脫無限循環的能力，離不開<strong>意識所帶來的整合能力</strong>，這種能力與身體、環境、時間和熵深度綁定。他提出「<strong>具身意識</strong>」的概念，強調意識不是脫離身體的軟體，而是身體、大腦、環境在時間中互動的產物。其他認知科學家也支持這一觀點，如默里·沙納漢的「全局工作空間理論」和雅布隆卡與金斯伯格的「無限聯想學習」概念，都指出意識的整合能力對避免框架問題和尋找新解方的重要性。影片強調，AI的智能是「<strong>無意識的智能</strong>」，它缺乏「自我感知」、「情感效應」和「生存驅動」，因此在需要「判斷意義」的場景中會暴露局限。AI的無限循環並非是「沒學會如何不循環」，而是「<strong>沒有不循環的理由</strong>」。人類之所以能擺脫循環，是因為我們「活著」，需要在時間中維持自身有序，通過有意義的行動對抗熵增；而AI只是「運行著」，沒有「活著」的感知，沒有時間的深度，也缺乏對抗熵的本能。因此，阿尼爾·塞斯的研究不僅揭示了AI的局限，更彰顯了人類意識作為「<strong>生存工具</strong>」和「<strong>時間與熵的禮物</strong>」的獨特性。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/7Eg2MTM76jM_36.jpg" /></p> <hr /><hr />================================================</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">大家好，这里是最佳拍档，我是大飞。不知道你有没有过这样的经历。打客服电话的时候。按了十几次“1”选人工服务。却始终卡在“请确认您的需求”的循环里；。或者导航软件让你“在前方50米掉头”，但是掉头后又重复同一个指令。最后你只能关掉软件靠自己找路。这些看似“低级”的故障。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">其实不是AI“笨”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而是它和我们人类在本质上存在一道鸿沟。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">今天我们要聊的。就是神经科学家阿尼尔·塞斯（Anil Seth）对这个问题的深度解读。他在文章中告诉我们，AI的无限循环。根源在于它没有“扎根”在时间和熵里。而这恰恰是人类意识的核心。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">先从一个真实的案例说起吧。这是阿尼尔·塞斯提到的亲身经历。他的航班准时降落在西班牙马德里巴拉哈斯机场。本以为下机很顺利。结果却因为登机廊桥的AI系统出了问题。所有人都被困在机舱里。透过舷窗能看到。那台由AI操控的登机廊桥。就像一个“卡住的玩具”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/7Eg2MTM76jM_59.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">先是慢慢靠近飞机舱门。离得近了又突然退回原位。接着再靠近、再退回。一遍又一遍。在清晨的冷空气中微微晃动。完全陷入了无限循环。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">最后还是一名工作人员赶过来。只看了几眼就找出了症结。几分钟就修好了。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这个场景特别有画面感。也特别耐人寻味。为什么AI会“钻牛角尖”似的重复同一动作。而人类哪怕没学过登机廊桥维修。也能快速判断问题的所在么？</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">阿尼尔·塞斯抛出了一个核心观点。那就是人类不会陷入无休止的重复行为。本质是因为我们是“扎根于时间与熵”的生命体。而这种与时间、熵的深度绑定。正是意识存在的重要根基。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">反过来，AI之所以容易陷循环。就是因为它既没有这种“扎根”，也没有真正的意识。哪怕未来AI的算力再强、模型再复杂。这个局限可能也绕不开。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">要理解这个问题，我们得先搞清楚。AI的“无限循环”到底是怎么回事？不是说AI不够智能。而是它的“智能”和人类的智能。底层逻辑是完全不同的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/7Eg2MTM76jM_125.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">阿尼尔·塞斯区分了两个关键概念。“智能”关乎“行动”，比如解决问题、执行任务；。而“意识”关乎“存在”或者“感受”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如你知道自己在“看视频”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">能感受到时间在流逝。能判断“这个指令不对”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">很多科技公司会觉得。智能堆到一定程度。意识自然就会出现。但是阿尼尔·塞斯认为，这是一个误区。至少在人类和其他动物身上。真正能摆脱“无限循环”的智能。必须依赖于意识所带来的能力。而这种意识。恰恰是和时间的流动深度绑定的。举个简单的例子，假设你写一个程序。让机器人完成“找红色物体”的任务。程序逻辑可能是，一直向前移动。直到摄像头识别到红色物体。然后停止。如果机器人面前始终没有红色物体。或者程序里“识别红色”的参数设错了。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如把橙色当成了红色。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">它就会一直往前走，永远停不下来。这就是最典型的AI无限循环。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">你可能会说。那给它加个监控系统不就行了？让它检测自己是不是走了太久。要是超过10分钟没找到。就停下来报警。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/7Eg2MTM76jM_190.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">但是问题来了。如果这个监控系统本身出了故障呢？比如它的“计时功能”坏了。永远显示“只走了1分钟”，那机器人还是会循环。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">你可能又会想，那再加一层监控。监控‘监控系统’不就好了？</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这就进入了“递归监控”的逻辑。每加一层，看似稳定性提升了。但是只要层级不是“无限”的。总有可能出问题。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如第一层监控管“找红色物体”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第二层管“第一层监控”，第三层管“第二层监控”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">可是你不可能加无限多层。最后总有某一层会失效。导致整个系统陷入循环。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这不是技术不够先进的问题。而是计算机科学的底层规律。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">近一个世纪前。艾伦·图灵（Alan Turing）就证明了这个结论。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这里必须提一下“图灵停机问题”（Turing Halting Problem）。这是理解AI局限的关键。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">1936年，艾伦·图灵在论文里提出。不存在任何一种算法。能够始终准确的判断。一个给定的程序，在输入某个数据后。最终会“终止运行”还是“无限循环”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">换句话说。无论你把程序写得多么精密。都没办法提前预知它在所有情况下会不会卡壳。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/7Eg2MTM76jM_255.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如刚才说的“找红色物体”的机器人。哪怕你测试了100次、1000次都没问题。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">但是第1001次的时候。可能因为光线的突变。让红色物体看起来像棕色。程序就判断失误，陷入循环。而你永远无法通过“算法”提前排除这种可能性。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">除了图灵停机问题。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">还有一个理论能解释AI的循环困境。那就是“框架问题”（Frame Problem）。1969年。约翰·麦卡锡（John McCarthy）和帕特里克·J·海耶斯（Patrick J。Hayes）提出了这个概念。它比图灵停机问题更具体。直指AI在“决策”上的核心难题。那就是要想教会机器只关注相关信息。而忽略无关的细节，是极其困难的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">举个例子。如果让AI控制一个家庭机器人。任务是“从客厅走到厨房。打开冰箱拿牛奶”。对人类来说。我们会自动忽略“客厅地毯的颜色”、“窗外的鸟叫”、“冰箱上的冰箱贴”这些无关信息。只关注“路线是否通畅”、“冰箱门能不能打开”、“牛奶在哪个格子里”。但是对于AI来说。它没办法天然区分“相关”和“无关”，它可能会思考“地毯颜色会不会影响走路速度？</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/7Eg2MTM76jM_323.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">”、“鸟叫是不是某种干扰信号？</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">”、“冰箱贴会不会挡住开门？”， 甚至会因为要“确认所有的细节”而陷入无限的信息筛选中。最后连“走第一步”都做不到。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">为什么会这样？因为在特定场景里。“无关信息”的数量可能是无穷的。比如从客厅到厨房。AI还可能考虑“空气湿度会不会影响电机运转？</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">”、“地板上的灰尘会不会卡住轮子？”，这些信息对人类来说毫无意义。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">但是AI没有一个“天然的过滤器”来判断“该忽略什么”，只能逐一排查，而排查的过程本身。就可能变成另一种形式的“无限循环”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">阿尼尔·塞斯在文章里说。随着深度神经网络、生成式AI的兴起。图灵停机问题和框架问题这些“老问题”似乎被淡忘了。但是它们从来没有消失。马德里机场的登机廊桥、客服电话的循环菜单、导航软件的错误指令。都是这些问题在现实中的体现。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">AI之所以绕不开这些困境。不是因为它“算力不够”，而是因为它和人类的生物大脑。在“时间感知”上存在根本差异。而这种差异。最终指向了“意识”的本质。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/7Eg2MTM76jM_391.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">接下来我们要聊的。就是整个问题的核心。时间与熵。这两个概念听起来很抽象。但是其实和我们每个人的存在都息息相关。也是区分“生命体”和“AI”的关键标尺。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先得解释一个物理学定律。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">热力学第二定律。这个定律说的是，在一个“孤立系统”，也就是和外界没有能量、物质交换的系统。比如一个完全密封的保温杯里。“熵”只会增加，或者保持不变。“熵”是什么？简单说就是“系统的混乱程度”，熵越高。混乱度越高。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如一滴墨水滴进清水里。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">会慢慢扩散，最后整杯水变成淡蓝色。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这个过程就是熵增；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而扩散后的墨水。永远不可能自己重新聚成“一滴”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">因为那会导致熵减。违背热力学第二定律。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">物理学家阿瑟·爱丁顿（Arthur Eddington）曾经说过。如果你的理论被证明违背了热力学第二定律。那我只能说你毫无希望；。除了彻底崩塌、颜面尽失。你别无出路。这句话足以说明这个定律的重要性。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">但是这里有个关键的前提。那就是热力学第二定律只适用于“孤立系统”。而“开放系统”，也就是和外界有能量、物质交换的系统。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/7Eg2MTM76jM_461.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如我们人类、植物、动物。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">是不一样的。开放系统可以通过从外界获取能量。来“降低自身的熵”，维持一个“有序”的状态。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如我们人类，每天吃饭、呼吸。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">就是从外界获取能量。用来维持身体细胞的新陈代谢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">如果不获取能量。身体就会逐渐混乱、衰退。也就是“熵增加”，最终走向死亡。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">阿尼尔·塞斯认为。生命系统的核心优势。就在于它是“开放系统”，并且这种“抗熵”的过程。是在多个层面、和时间深度绑定的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如从最微观的“生化时间”来看。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">我们身体里的每个细胞。每秒都会发生十亿次生化反应。这些反应精准配合，维持着代谢平衡；。再到“神经时间”，神经信号的传递有快有慢。有髓鞘的神经纤维。信号传递速度能达到每秒120米。而神经递质在突触间的扩散。速度则慢得多，只有每秒几毫米；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">再到我们能直接感受到的“意识时间”，它不是匀速的。开心的时候觉得“时间过得快”，等待的时候觉得“时间过得慢”，但它永远是“连续的、不可逆转的”，包含了“过去的记忆、现在的感受、未来的预期”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/7Eg2MTM76jM_530.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这些不同尺度的“时间”，不是孤立的。而是深度交织、相互依赖的。比如你饿了。这是身体代谢层面的信号。属于生化时间。大脑会接收到“需要进食”的神经信号。经历神经时间。你的意识会产生“想吃东西”的感受。又会经历意识时间。然后你会主动去找食物。整个过程。是从微观到宏观的时间整合。而这种整合。让人类能在复杂的环境里快速判断“该做什么”，不会陷入无意义的循环。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">但是AI完全不一样。对数字计算机来说。“时间”是扁平的、一维的。和热力学第二定律没有任何绑定。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">计算的本质，就是“状态的跃迁”，从0到1，从A到B。就像一串多米诺骨牌一样。推倒第一块，后面的依次倒下。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在图灵提出的经典计算模型里。只有“状态的顺序”是重要的。而“两个状态之间的时间间隔”毫无意义。比如一个计算步骤，间隔1微秒完成。和间隔100万年完成。对算法本身来说没有任何区别。算法的性质不会变。计算的结果也不会变。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/7Eg2MTM76jM_594.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这种“对时间的漠视”，其实是有代价的。阿尼尔·塞斯在文章里说。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">假装时间不存在。其实是一门昂贵的交易。为什么？因为计算机本身是“物理实体”，它无法摆脱热力学第二定律的影响。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如芯片会发热。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">内存会出错，这些都是“熵增”的体现。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">为了保证计算的准确性。AI系统需要消耗大量的能量来“纠错”和“降温”，抵消熵的影响。比如数据中心的服务器。大部分能量不是用来“计算”，而是用来“散热”，这就是AI“能量缺口”大的重要原因。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而AI之所以会陷入无限循环。核心就在于它的“时间观”，它被困在“状态序列”里。不受熵的“拉扯”，也没有“生存的驱动力”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如马德里机场的登机廊桥AI。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">它只会执行“靠近-检测-退回”的指令。不会因为“反复做同一件事没意义”而停止。也不会因为“消耗了太多能量”而主动调整。它没有“这样做不对”的意识。因为它没有“自我存在”的感知。也没有“时间流逝”的概念。但是人类不一样。我们是“身处时间之中”的生命。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/7Eg2MTM76jM_658.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">阿尼尔·塞斯用了三个词来形容：“具身的（embodied）、植根于环境的（embedded）、与时间同行的（entimed）”。我们的每一个决策。都受“时间压力”和“生存驱动”的影响。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如你在沙漠里迷路。哪怕你一开始走错了。也不会反复绕同一个圈子。因为你知道“时间越久，水分越少。生存概率越低”，这种“抗熵”的本能。会让你主动寻找新的方向。摆脱可能的循环。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">阿尼尔·塞斯在文章里也提到了一个例外。那就是人类其实也有“重复行为”，比如额叶受损的人会反复做同一个动作。强迫症患者会反复洗手。成瘾者会重复自我毁灭的行为。但是这些案例恰恰印证了他的观点。这些都是“正常神经认知功能紊乱”的结果。是意识与时间、身体的连接出了问题。而不是“人类天生会循环”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">一旦神经功能恢复。这种重复行为就会消失。因为“抗熵”和“时间感知”的本能会重新起作用。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">如果这个逻辑成立。那一个关键的问题就来了。未来的AI能不能通过技术改进。拥有这种与时间、熵绑定的能力呢。能不能摆脱无限循环。实现像人类一样的开放式智能呢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/7Eg2MTM76jM_728.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">阿尼尔·塞斯分析了几种目前的探索方向。我们一个一个来看。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第一种是“模拟计算机”（Analog Computers）。这是最早的计算机形式。比如有2000年历史的“安提基特拉机械”（Antikythera mechanism）。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这是古希腊人发明的天文装置。能通过齿轮的连续转动。预测日食、月食和行星的位置。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">模拟计算机的特点是“时间是连续的”，不像数字计算机那样是“离散的状态跃迁”，它更接近人类对时间的感知。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">但是模拟计算机的问题是“精度低”，容易受到环境的影响。比如温度、湿度会改变齿轮的转动速度。所以后来被数字计算机取代了。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第二种是“凡人计算”（Mortal Computation）。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这个概念很有意思。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">它的核心是“让计算依赖硬件的寿命”，比如一个程序，只有在特定的硬件。比如某块芯片正常工作的时候才能运行。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">一旦硬件“失效”，比如芯片老化、发热损坏。程序也会随之“消亡”。这种设计的思路。是主动“拥抱”热力学第二定律。而不是“对抗”它。通过硬件的“有限寿命”，强制程序“不会无限运行”，从而避免循环。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/7Eg2MTM76jM_792.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">阿尼尔·塞斯认为。这种方法能提高能量利用效率。但它本质上是“用硬件限制来解决软件问题”，并没有让AI真正“理解时间”，只是用物理手段切断了循环的可能。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第三种是“神经形态计算”（Neuromorphic Computing）。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这是目前很热门的方向。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">核心是“模拟人类大脑的工作方式”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如用特殊的芯片。模拟神经元的信号传递。让计算过程更接近“神经时间”的有快有慢。和数字计算机相比。神经形态计算更“植根于时间”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如它的信号传递不是“非0即1”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而是“连续的电压变化”，更像大脑的神经活动。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">但是阿尼尔·塞斯怀疑。这种方法还是无法解决根本的问题。因为它只是“模拟了大脑的结构”，却没有模拟大脑的“生存驱动”和“情感效应”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如“饿”、“痛”、“开心”这些感受。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而这些恰恰是人类摆脱循环的关键。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第四种是动力系统方法（Dynamical Systems Approach）。这种方法完全跳出了“计算”的思路。不关注“算法”，而是关注“系统在时间中的状态变化”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/7Eg2MTM76jM_855.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如用“吸引子”（Attractor）、“相空间”（Phase Space）这些概念。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">描述AI如何在不同时间点调整状态。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">它的源头可以追溯到AI发展初期的“控制论”（Cybernetics）。控制论关注的是“反馈与调控”，比如“温度高了就降温。温度低了就升温”，而不是“执行固定的指令”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这种方法更重视“时间的动态性”，但是阿尼尔·塞斯认为。它依然缺乏“生命体的抗熵本能”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">AI还是没有“维持自身有序”的驱动力。只是在被动响应环境变化。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">总结下来，这些新型AI技术。确实在“靠近时间”，但是都没有真正“扎根于时间与熵”，它们要么是用硬件来限制。要么是模拟结构。要么是被动响应。却没有“主动抗熵”的生存驱动。也没有“意识层面的感受”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">所以阿尼尔·塞斯的结论是。哪怕采用这些方法。AI可能还是无法完全摆脱无限循环的阴影。更难实现人类那种“开放式、适应性强”的智能。为什么呢？因为这种智能，不仅和时间绑定。还和“意识”深度绑定。至少在人类身上，摆脱循环的能力。离不开意识带来的“整合能力”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/7Eg2MTM76jM_918.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">阿尼尔·塞斯在他的著作《意识机器：。成为你自己》（Being You:。A New Science of Consciousness）里写过一句话。我们感知周遭世界。也感知身处其中的自己。这一切都依赖于我们鲜活的身体。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这句话的核心是“具身意识”，指的是。意识不是“脱离身体的软件”，而是“身体、大脑、环境在时间中互动的产物”。比如你看到一杯水。意识会告诉你“这杯水能解渴”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而这个判断。来自你过去“喝水解渴”的记忆、当下“口渴”的身体信号、以及“杯子里的水是干净的”的环境判断。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这种多层面的整合。让你不会“反复看杯子却不喝水”，因为意识会引导你“做有意义的行动”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">除了阿尼尔·塞斯。还有其他学者也从不同角度论证了“意识与智能的绑定”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如认知科学家默里·沙纳汉（Murray Shanahan）。他受“意识的全局工作空间理论GWT”（Global Workspace Theory）的启发。提出意识就像一个“信息中枢”，能把大脑里不同区域的信息。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如视觉、听觉和记忆整合起来。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">然后传递给整个大脑。这种整合能力。能帮助人类快速判断“哪些信息相关。哪些无关”，从而回避框架问题。不会陷入无意义的信息筛选。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/7Eg2MTM76jM_987.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">还有理论生物学家伊娃·雅布隆卡（Eva Jablonka）和西蒙娜·金斯伯格（Simona Ginsburg）。她们提出了“无限联想学习UAL”（Unlimited Associative Learning）的概念。这是一种特殊的学习能力。能让生物从过去的经验中。联想出全新的解决方案。而不是重复旧的行为。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如你第一次遇到“门锁坏了”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">可能会尝试“用卡片撬”，或者“用钥匙转”，如果都不行。你会想到“找物业”或者“找开锁公司”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这种“不重复、找新方法”的能力。就来自无限联想学习，而它的前提。是意识能“感知到问题的特殊性”，并且“整合过去的经验”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这些观点汇总起来，指向了一个结论。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在生物身上，至少在人类身上。“能摆脱无限循环的智能”，必须依赖于意识；。而意识的核心。是“与身体、环境、时间、熵的深度绑定”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">反过来，如果AI没有这种绑定。没有真正的意识。就永远无法拥有和人类一样的“开放式智能”，即通用人工智能AGI。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">它可能真的像阿尼尔·塞斯说的那样。是“一场虚幻泡影”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/7Eg2MTM76jM_1050.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这里还要澄清一个常见的误区。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">很多人觉得“AI只要够智能。就能产生意识”，但是阿尼尔·塞斯认为。这是“因果倒置”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">智能是“行动层面”的能力。意识是“存在层面”的能力。前者可以通过算法实现。后者需要“植根于时间与熵的具身体验”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如AI能写出流畅的文章。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">但它不会“知道自己在写文章”，也不会“因为写得好而开心”，更不会“担心自己写得不好”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">它没有“自我感知”，没有“情感效应”，所以它的“智能”是“无意识的智能”，一旦遇到需要“判断意义”的场景。比如避免循环。它就会暴露自己的局限性。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">最后，文章的后记中还提出一个疑问。那就是神经形态计算虽然能够模拟大脑的时序。但是它能复现意识里的“效应”吗？比如哪怕是微弱的“这个做法不对”的消极感受。或者“这样做有意义”的积极感受。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">阿尼尔·塞斯没有直接回答。但是从他的理论来看。答案可能是否定的。因为“效应”来自“生存驱动”，来自“身体对抗熵的本能”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/7Eg2MTM76jM_1113.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而神经形态计算只是模拟了大脑的“结构”，没有模拟身体的“代谢过程”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">没有“饿”、“渴”、“疼”（口误）这些最基础的生存信号。自然无法产生“效应”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">所以，AI的无限循环。看似是技术问题，其实是“存在问题”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">它不是“没学会如何不循环”，而是“没有不循环的理由”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">人类之所以能摆脱循环。是因为我们“活着”，我们需要在时间中维持自身的有序。需要通过有意义的行动对抗熵增；。而AI只是“运行着”，它没有“活着”的感知。没有时间的深度，没有对抗熵的本能。所以它会在无意义的循环里。一遍又一遍地重复，直到能量耗尽。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">阿尼尔·塞斯的研究。其实不只是在说AI的局限。更是在帮我们看清人类意识的独特性。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">我们的意识，不是“多余的装饰”，而是“生存的工具”，是“时间与熵赋予我们的礼物”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">或许未来某一天。AI能解决无限循环的问题。但那时候的它。可能已经不是“我们现在理解的AI”，而是一种全新的、真正“植根于时间与熵”的生命体。但是那一天，可能还非常遥远。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/7Eg2MTM76jM_1180.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">好了，今天的内容就到这里。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">如果你对“意识与AI”的话题感兴趣。推荐大家去读阿尼尔·塞斯的《意识机器：成为你自己》。里面对“具身意识”的解读更细腻。感谢收看本期视频，我们下期再见。</p>

<hr style="clear:both" />
=============
<p><a href="https://www.youtube.com/watch?v=7Eg2MTM76jM">https://www.youtube.com/watch?v=7Eg2MTM76jM</a></p><p>不知道你有没有过这样的经历，打客服电话的时候，按了十几次“1”选人工服务，却始终卡在“请确认您的需求”的循环里；或者导航软件让你“在前方50米掉头”，但是掉头后又重复同一个指令，最后你只能关掉软件靠自己找路。这些看似“低级”的故障，其实不是AI“笨”，而是它和我们人类在本质上存在一道鸿沟。今天我们要聊的，就是神经科学家阿尼尔·塞斯（Anil Seth）对这个问题的深度解读，他在文章中告诉我们，AI的无限循环，根源在于它没有“扎根”在时间和熵里，而这恰恰是人类意识的核心。</p><p><a href="https://bigthink.com/neuropsych/anil-seth-consciousness-time-perception/">https://bigthink.com/neuropsych/anil-seth-consciousness-time-perception/</a></p>]]>
      </itunes:summary>
      <description>
        <![CDATA[<hr style="clear:both" />

<p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://img.youtube.com/vi/7Eg2MTM76jM/maxresdefault.jpg" /></p><p><a href="https://www.youtube.com/watch?v=7Eg2MTM76jM">https://www.youtube.com/watch?v=7Eg2MTM76jM</a></p><h1>值得閱讀的理由</h1> <ul> <li>深入理解AI為何會陷入重複循環，超越表面技術故障的物理與哲學根源。</li> <li>探討人類意識與「時間」、「熵」的深度連結，揭示我們能擺脫重複行為的生物學基礎。</li> <li>認識神經科學家阿尼爾·塞斯對「智能」與「意識」的獨到見解，及其對通用人工智慧（AGI）未來發展的啟示。</li> </ul> <hr /> <h1>摘要</h1> <h2>AI困境：無限循環的現實案例與核心問題</h2> <p>影片首先透過生活中的多個例子，點出AI系統常出現的「無限循環」問題：從客服電話的無限按鍵迴圈、導航軟體的重複指令，到作者親身經歷的西班牙馬德里機場登機廊橋AI故障。這些看似低級的故障，實則反映了AI與人類本質上的根本差異。神經科學家<strong>阿尼爾·塞斯（Anil Seth）</strong>指出，AI的無限循環根源在於它缺乏「<strong>扎根於時間與熵</strong>」的能力，而這恰恰是人類意識的核心基石。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/7Eg2MTM76jM_95.jpg" /></p> <hr /> <h2>智能與意識的區分：AI的根本局限</h2> <p>阿尼爾·塞斯明確區分了「<strong>智能（Intelligence）</strong>」與「<strong>意識（Consciousness）</strong>」。智能關乎「行動」與「解決問題」，而意識則關乎「存在」與「感受」。他認為，儘管許多科技公司相信智能堆疊到一定程度便能產生意識，但這是個誤區。對人類而言，真正能擺脫無限循環的智能，必須依賴於<strong>意識所帶來的能力</strong>，而這種意識與時間的流動深度綁定。AI之所以容易陷入循環，是因為它沒有這種與時間、熵的「扎根」，也缺乏真正的意識。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/7Eg2MTM76jM_255.jpg" /></p> <hr /> <h2>AI無限循環的底層邏輯：圖靈停機問題與框架問題</h2> <p>影片進一步解釋了AI無限循環的底層科學依據。其一為近一世紀前艾倫·圖靈提出的「<strong>圖靈停機問題（Turing Halting Problem）</strong>」，該理論證明了不存在任何演算法能準確預判一個程式在所有情況下是否會無限循環。其二為1969年提出的「<strong>框架問題（Frame Problem）</strong>」，它指出AI難以區分「相關資訊」與「無關細節」，容易因過度篩選資訊而陷入停滯或循環。這兩個「老問題」在當今深度學習與生成式AI興起的時代似乎被淡忘，但實質上仍在現實中不斷體現AI的局限。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/7Eg2MTM76jM_565.jpg" /></p> <hr /> <h2>時間與熵：生命體與AI的關鍵分野</h2> <p>影片深入探討了「<strong>時間</strong>」與「<strong>熵</strong>」這兩個區分生命體與AI的核心概念。根據<strong>熱力學第二定律</strong>，在孤立系統中，熵（混亂程度）只會增加。然而，人類等生命體是「<strong>開放系統</strong>」，能透過獲取外界能量來「<strong>抗熵</strong>」（降低自身熵，維持有序狀態）。人類意識與時間深度綁定，體現在生化、神經及意識等多個時間尺度上，這種多層面的時間整合使我們能快速判斷並採取有意義的行動。相比之下，數字計算機的「時間」是<strong>扁平、一維</strong>的，它漠視時間流逝，其計算的本質是「狀態躍遷」，與熱力學第二定律脫鉤，導致AI需要消耗大量能量來抵銷熵增的影響（如散熱、糾錯），卻無法產生「生存驅動力」或「自我存在」的感知。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/7Eg2MTM76jM_1140.jpg" /></p> <hr /> <h2>人類的「抗熵本能」與具身意識</h2> <p>人類作為「身處時間之中」的生命，被阿尼爾·塞斯形容為「<strong>具身的（embodied）、植根於環境的（embedded）、與時間同行的（entimed）</strong>」。我們的每個決策都受到「時間壓力」和「生存驅動」的影響，這種「抗熵」的本能促使我們擺脫無意義的重複。即使人類會出現重複行為（如額葉受損、強迫症），那也通常是神經認知功能紊亂的結果，而非天生會循環。這些情況恰恰證明了當意識與時間、身體的連結出現問題時，人類也會陷入循環，進一步佐證了意識在擺脫循環中的關鍵作用。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/7Eg2MTM76jM_22.jpg" /></p> <hr /> <h2>未來AI的探索方向與局限性</h2> <p>影片探討了幾種旨在讓AI擺脫無限循環、實現類人智能的探索方向： <ul> <li><strong>模擬計算機（Analog Computers）</strong>：時間連續，但精度低且易受環境影響。</li> <li><strong>凡人計算（Mortal Computation）</strong>：透過硬體壽命限制，強制程式停止，但非讓AI真正「理解時間」。</li> <li><strong>神經形態計算（Neuromorphic Computing）</strong>：模擬大腦工作方式，更「植根於時間」，但缺乏「生存驅動」和「情感效應」。</li> <li><strong>動力系統方法（Dynamical Systems Approach）</strong>：關注系統在時間中的狀態變化，但仍缺乏「生命體的抗熵本能」。</li> </ul> 阿尼爾·塞斯認為，儘管這些新型AI技術在「靠近時間」，但都未能真正「<strong>扎根於時間與熵</strong>」，缺乏「主動抗熵」的生存驅動和「意識層面的感受」，因此可能仍無法完全擺脫無限循環的陰影，更難實現人類那種「開放式、適應性強」的通用人工智慧（AGI）。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/7Eg2MTM76jM_29.jpg" /></p> <hr /> <h2>意識是生存工具：AI為何沒有「不循環的理由」</h2> <p>阿尼爾·塞斯的結論是，人類能夠擺脫無限循環的能力，離不開<strong>意識所帶來的整合能力</strong>，這種能力與身體、環境、時間和熵深度綁定。他提出「<strong>具身意識</strong>」的概念，強調意識不是脫離身體的軟體，而是身體、大腦、環境在時間中互動的產物。其他認知科學家也支持這一觀點，如默里·沙納漢的「全局工作空間理論」和雅布隆卡與金斯伯格的「無限聯想學習」概念，都指出意識的整合能力對避免框架問題和尋找新解方的重要性。影片強調，AI的智能是「<strong>無意識的智能</strong>」，它缺乏「自我感知」、「情感效應」和「生存驅動」，因此在需要「判斷意義」的場景中會暴露局限。AI的無限循環並非是「沒學會如何不循環」，而是「<strong>沒有不循環的理由</strong>」。人類之所以能擺脫循環，是因為我們「活著」，需要在時間中維持自身有序，通過有意義的行動對抗熵增；而AI只是「運行著」，沒有「活著」的感知，沒有時間的深度，也缺乏對抗熵的本能。因此，阿尼爾·塞斯的研究不僅揭示了AI的局限，更彰顯了人類意識作為「<strong>生存工具</strong>」和「<strong>時間與熵的禮物</strong>」的獨特性。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/7Eg2MTM76jM_36.jpg" /></p> <hr /><hr />================================================</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">大家好，这里是最佳拍档，我是大飞。不知道你有没有过这样的经历。打客服电话的时候。按了十几次“1”选人工服务。却始终卡在“请确认您的需求”的循环里；。或者导航软件让你“在前方50米掉头”，但是掉头后又重复同一个指令。最后你只能关掉软件靠自己找路。这些看似“低级”的故障。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">其实不是AI“笨”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而是它和我们人类在本质上存在一道鸿沟。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">今天我们要聊的。就是神经科学家阿尼尔·塞斯（Anil Seth）对这个问题的深度解读。他在文章中告诉我们，AI的无限循环。根源在于它没有“扎根”在时间和熵里。而这恰恰是人类意识的核心。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">先从一个真实的案例说起吧。这是阿尼尔·塞斯提到的亲身经历。他的航班准时降落在西班牙马德里巴拉哈斯机场。本以为下机很顺利。结果却因为登机廊桥的AI系统出了问题。所有人都被困在机舱里。透过舷窗能看到。那台由AI操控的登机廊桥。就像一个“卡住的玩具”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/7Eg2MTM76jM_59.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">先是慢慢靠近飞机舱门。离得近了又突然退回原位。接着再靠近、再退回。一遍又一遍。在清晨的冷空气中微微晃动。完全陷入了无限循环。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">最后还是一名工作人员赶过来。只看了几眼就找出了症结。几分钟就修好了。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这个场景特别有画面感。也特别耐人寻味。为什么AI会“钻牛角尖”似的重复同一动作。而人类哪怕没学过登机廊桥维修。也能快速判断问题的所在么？</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">阿尼尔·塞斯抛出了一个核心观点。那就是人类不会陷入无休止的重复行为。本质是因为我们是“扎根于时间与熵”的生命体。而这种与时间、熵的深度绑定。正是意识存在的重要根基。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">反过来，AI之所以容易陷循环。就是因为它既没有这种“扎根”，也没有真正的意识。哪怕未来AI的算力再强、模型再复杂。这个局限可能也绕不开。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">要理解这个问题，我们得先搞清楚。AI的“无限循环”到底是怎么回事？不是说AI不够智能。而是它的“智能”和人类的智能。底层逻辑是完全不同的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/7Eg2MTM76jM_125.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">阿尼尔·塞斯区分了两个关键概念。“智能”关乎“行动”，比如解决问题、执行任务；。而“意识”关乎“存在”或者“感受”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如你知道自己在“看视频”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">能感受到时间在流逝。能判断“这个指令不对”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">很多科技公司会觉得。智能堆到一定程度。意识自然就会出现。但是阿尼尔·塞斯认为，这是一个误区。至少在人类和其他动物身上。真正能摆脱“无限循环”的智能。必须依赖于意识所带来的能力。而这种意识。恰恰是和时间的流动深度绑定的。举个简单的例子，假设你写一个程序。让机器人完成“找红色物体”的任务。程序逻辑可能是，一直向前移动。直到摄像头识别到红色物体。然后停止。如果机器人面前始终没有红色物体。或者程序里“识别红色”的参数设错了。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如把橙色当成了红色。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">它就会一直往前走，永远停不下来。这就是最典型的AI无限循环。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">你可能会说。那给它加个监控系统不就行了？让它检测自己是不是走了太久。要是超过10分钟没找到。就停下来报警。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/7Eg2MTM76jM_190.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">但是问题来了。如果这个监控系统本身出了故障呢？比如它的“计时功能”坏了。永远显示“只走了1分钟”，那机器人还是会循环。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">你可能又会想，那再加一层监控。监控‘监控系统’不就好了？</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这就进入了“递归监控”的逻辑。每加一层，看似稳定性提升了。但是只要层级不是“无限”的。总有可能出问题。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如第一层监控管“找红色物体”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第二层管“第一层监控”，第三层管“第二层监控”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">可是你不可能加无限多层。最后总有某一层会失效。导致整个系统陷入循环。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这不是技术不够先进的问题。而是计算机科学的底层规律。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">近一个世纪前。艾伦·图灵（Alan Turing）就证明了这个结论。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这里必须提一下“图灵停机问题”（Turing Halting Problem）。这是理解AI局限的关键。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">1936年，艾伦·图灵在论文里提出。不存在任何一种算法。能够始终准确的判断。一个给定的程序，在输入某个数据后。最终会“终止运行”还是“无限循环”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">换句话说。无论你把程序写得多么精密。都没办法提前预知它在所有情况下会不会卡壳。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/7Eg2MTM76jM_255.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如刚才说的“找红色物体”的机器人。哪怕你测试了100次、1000次都没问题。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">但是第1001次的时候。可能因为光线的突变。让红色物体看起来像棕色。程序就判断失误，陷入循环。而你永远无法通过“算法”提前排除这种可能性。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">除了图灵停机问题。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">还有一个理论能解释AI的循环困境。那就是“框架问题”（Frame Problem）。1969年。约翰·麦卡锡（John McCarthy）和帕特里克·J·海耶斯（Patrick J。Hayes）提出了这个概念。它比图灵停机问题更具体。直指AI在“决策”上的核心难题。那就是要想教会机器只关注相关信息。而忽略无关的细节，是极其困难的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">举个例子。如果让AI控制一个家庭机器人。任务是“从客厅走到厨房。打开冰箱拿牛奶”。对人类来说。我们会自动忽略“客厅地毯的颜色”、“窗外的鸟叫”、“冰箱上的冰箱贴”这些无关信息。只关注“路线是否通畅”、“冰箱门能不能打开”、“牛奶在哪个格子里”。但是对于AI来说。它没办法天然区分“相关”和“无关”，它可能会思考“地毯颜色会不会影响走路速度？</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/7Eg2MTM76jM_323.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">”、“鸟叫是不是某种干扰信号？</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">”、“冰箱贴会不会挡住开门？”， 甚至会因为要“确认所有的细节”而陷入无限的信息筛选中。最后连“走第一步”都做不到。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">为什么会这样？因为在特定场景里。“无关信息”的数量可能是无穷的。比如从客厅到厨房。AI还可能考虑“空气湿度会不会影响电机运转？</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">”、“地板上的灰尘会不会卡住轮子？”，这些信息对人类来说毫无意义。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">但是AI没有一个“天然的过滤器”来判断“该忽略什么”，只能逐一排查，而排查的过程本身。就可能变成另一种形式的“无限循环”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">阿尼尔·塞斯在文章里说。随着深度神经网络、生成式AI的兴起。图灵停机问题和框架问题这些“老问题”似乎被淡忘了。但是它们从来没有消失。马德里机场的登机廊桥、客服电话的循环菜单、导航软件的错误指令。都是这些问题在现实中的体现。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">AI之所以绕不开这些困境。不是因为它“算力不够”，而是因为它和人类的生物大脑。在“时间感知”上存在根本差异。而这种差异。最终指向了“意识”的本质。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/7Eg2MTM76jM_391.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">接下来我们要聊的。就是整个问题的核心。时间与熵。这两个概念听起来很抽象。但是其实和我们每个人的存在都息息相关。也是区分“生命体”和“AI”的关键标尺。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先得解释一个物理学定律。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">热力学第二定律。这个定律说的是，在一个“孤立系统”，也就是和外界没有能量、物质交换的系统。比如一个完全密封的保温杯里。“熵”只会增加，或者保持不变。“熵”是什么？简单说就是“系统的混乱程度”，熵越高。混乱度越高。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如一滴墨水滴进清水里。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">会慢慢扩散，最后整杯水变成淡蓝色。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这个过程就是熵增；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而扩散后的墨水。永远不可能自己重新聚成“一滴”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">因为那会导致熵减。违背热力学第二定律。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">物理学家阿瑟·爱丁顿（Arthur Eddington）曾经说过。如果你的理论被证明违背了热力学第二定律。那我只能说你毫无希望；。除了彻底崩塌、颜面尽失。你别无出路。这句话足以说明这个定律的重要性。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">但是这里有个关键的前提。那就是热力学第二定律只适用于“孤立系统”。而“开放系统”，也就是和外界有能量、物质交换的系统。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/7Eg2MTM76jM_461.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如我们人类、植物、动物。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">是不一样的。开放系统可以通过从外界获取能量。来“降低自身的熵”，维持一个“有序”的状态。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如我们人类，每天吃饭、呼吸。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">就是从外界获取能量。用来维持身体细胞的新陈代谢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">如果不获取能量。身体就会逐渐混乱、衰退。也就是“熵增加”，最终走向死亡。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">阿尼尔·塞斯认为。生命系统的核心优势。就在于它是“开放系统”，并且这种“抗熵”的过程。是在多个层面、和时间深度绑定的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如从最微观的“生化时间”来看。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">我们身体里的每个细胞。每秒都会发生十亿次生化反应。这些反应精准配合，维持着代谢平衡；。再到“神经时间”，神经信号的传递有快有慢。有髓鞘的神经纤维。信号传递速度能达到每秒120米。而神经递质在突触间的扩散。速度则慢得多，只有每秒几毫米；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">再到我们能直接感受到的“意识时间”，它不是匀速的。开心的时候觉得“时间过得快”，等待的时候觉得“时间过得慢”，但它永远是“连续的、不可逆转的”，包含了“过去的记忆、现在的感受、未来的预期”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/7Eg2MTM76jM_530.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这些不同尺度的“时间”，不是孤立的。而是深度交织、相互依赖的。比如你饿了。这是身体代谢层面的信号。属于生化时间。大脑会接收到“需要进食”的神经信号。经历神经时间。你的意识会产生“想吃东西”的感受。又会经历意识时间。然后你会主动去找食物。整个过程。是从微观到宏观的时间整合。而这种整合。让人类能在复杂的环境里快速判断“该做什么”，不会陷入无意义的循环。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">但是AI完全不一样。对数字计算机来说。“时间”是扁平的、一维的。和热力学第二定律没有任何绑定。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">计算的本质，就是“状态的跃迁”，从0到1，从A到B。就像一串多米诺骨牌一样。推倒第一块，后面的依次倒下。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在图灵提出的经典计算模型里。只有“状态的顺序”是重要的。而“两个状态之间的时间间隔”毫无意义。比如一个计算步骤，间隔1微秒完成。和间隔100万年完成。对算法本身来说没有任何区别。算法的性质不会变。计算的结果也不会变。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/7Eg2MTM76jM_594.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这种“对时间的漠视”，其实是有代价的。阿尼尔·塞斯在文章里说。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">假装时间不存在。其实是一门昂贵的交易。为什么？因为计算机本身是“物理实体”，它无法摆脱热力学第二定律的影响。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如芯片会发热。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">内存会出错，这些都是“熵增”的体现。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">为了保证计算的准确性。AI系统需要消耗大量的能量来“纠错”和“降温”，抵消熵的影响。比如数据中心的服务器。大部分能量不是用来“计算”，而是用来“散热”，这就是AI“能量缺口”大的重要原因。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而AI之所以会陷入无限循环。核心就在于它的“时间观”，它被困在“状态序列”里。不受熵的“拉扯”，也没有“生存的驱动力”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如马德里机场的登机廊桥AI。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">它只会执行“靠近-检测-退回”的指令。不会因为“反复做同一件事没意义”而停止。也不会因为“消耗了太多能量”而主动调整。它没有“这样做不对”的意识。因为它没有“自我存在”的感知。也没有“时间流逝”的概念。但是人类不一样。我们是“身处时间之中”的生命。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/7Eg2MTM76jM_658.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">阿尼尔·塞斯用了三个词来形容：“具身的（embodied）、植根于环境的（embedded）、与时间同行的（entimed）”。我们的每一个决策。都受“时间压力”和“生存驱动”的影响。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如你在沙漠里迷路。哪怕你一开始走错了。也不会反复绕同一个圈子。因为你知道“时间越久，水分越少。生存概率越低”，这种“抗熵”的本能。会让你主动寻找新的方向。摆脱可能的循环。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">阿尼尔·塞斯在文章里也提到了一个例外。那就是人类其实也有“重复行为”，比如额叶受损的人会反复做同一个动作。强迫症患者会反复洗手。成瘾者会重复自我毁灭的行为。但是这些案例恰恰印证了他的观点。这些都是“正常神经认知功能紊乱”的结果。是意识与时间、身体的连接出了问题。而不是“人类天生会循环”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">一旦神经功能恢复。这种重复行为就会消失。因为“抗熵”和“时间感知”的本能会重新起作用。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">如果这个逻辑成立。那一个关键的问题就来了。未来的AI能不能通过技术改进。拥有这种与时间、熵绑定的能力呢。能不能摆脱无限循环。实现像人类一样的开放式智能呢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/7Eg2MTM76jM_728.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">阿尼尔·塞斯分析了几种目前的探索方向。我们一个一个来看。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第一种是“模拟计算机”（Analog Computers）。这是最早的计算机形式。比如有2000年历史的“安提基特拉机械”（Antikythera mechanism）。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这是古希腊人发明的天文装置。能通过齿轮的连续转动。预测日食、月食和行星的位置。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">模拟计算机的特点是“时间是连续的”，不像数字计算机那样是“离散的状态跃迁”，它更接近人类对时间的感知。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">但是模拟计算机的问题是“精度低”，容易受到环境的影响。比如温度、湿度会改变齿轮的转动速度。所以后来被数字计算机取代了。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第二种是“凡人计算”（Mortal Computation）。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这个概念很有意思。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">它的核心是“让计算依赖硬件的寿命”，比如一个程序，只有在特定的硬件。比如某块芯片正常工作的时候才能运行。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">一旦硬件“失效”，比如芯片老化、发热损坏。程序也会随之“消亡”。这种设计的思路。是主动“拥抱”热力学第二定律。而不是“对抗”它。通过硬件的“有限寿命”，强制程序“不会无限运行”，从而避免循环。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/7Eg2MTM76jM_792.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">阿尼尔·塞斯认为。这种方法能提高能量利用效率。但它本质上是“用硬件限制来解决软件问题”，并没有让AI真正“理解时间”，只是用物理手段切断了循环的可能。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第三种是“神经形态计算”（Neuromorphic Computing）。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这是目前很热门的方向。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">核心是“模拟人类大脑的工作方式”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如用特殊的芯片。模拟神经元的信号传递。让计算过程更接近“神经时间”的有快有慢。和数字计算机相比。神经形态计算更“植根于时间”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如它的信号传递不是“非0即1”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而是“连续的电压变化”，更像大脑的神经活动。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">但是阿尼尔·塞斯怀疑。这种方法还是无法解决根本的问题。因为它只是“模拟了大脑的结构”，却没有模拟大脑的“生存驱动”和“情感效应”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如“饿”、“痛”、“开心”这些感受。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而这些恰恰是人类摆脱循环的关键。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第四种是动力系统方法（Dynamical Systems Approach）。这种方法完全跳出了“计算”的思路。不关注“算法”，而是关注“系统在时间中的状态变化”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/7Eg2MTM76jM_855.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如用“吸引子”（Attractor）、“相空间”（Phase Space）这些概念。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">描述AI如何在不同时间点调整状态。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">它的源头可以追溯到AI发展初期的“控制论”（Cybernetics）。控制论关注的是“反馈与调控”，比如“温度高了就降温。温度低了就升温”，而不是“执行固定的指令”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这种方法更重视“时间的动态性”，但是阿尼尔·塞斯认为。它依然缺乏“生命体的抗熵本能”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">AI还是没有“维持自身有序”的驱动力。只是在被动响应环境变化。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">总结下来，这些新型AI技术。确实在“靠近时间”，但是都没有真正“扎根于时间与熵”，它们要么是用硬件来限制。要么是模拟结构。要么是被动响应。却没有“主动抗熵”的生存驱动。也没有“意识层面的感受”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">所以阿尼尔·塞斯的结论是。哪怕采用这些方法。AI可能还是无法完全摆脱无限循环的阴影。更难实现人类那种“开放式、适应性强”的智能。为什么呢？因为这种智能，不仅和时间绑定。还和“意识”深度绑定。至少在人类身上，摆脱循环的能力。离不开意识带来的“整合能力”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/7Eg2MTM76jM_918.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">阿尼尔·塞斯在他的著作《意识机器：。成为你自己》（Being You:。A New Science of Consciousness）里写过一句话。我们感知周遭世界。也感知身处其中的自己。这一切都依赖于我们鲜活的身体。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这句话的核心是“具身意识”，指的是。意识不是“脱离身体的软件”，而是“身体、大脑、环境在时间中互动的产物”。比如你看到一杯水。意识会告诉你“这杯水能解渴”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而这个判断。来自你过去“喝水解渴”的记忆、当下“口渴”的身体信号、以及“杯子里的水是干净的”的环境判断。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这种多层面的整合。让你不会“反复看杯子却不喝水”，因为意识会引导你“做有意义的行动”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">除了阿尼尔·塞斯。还有其他学者也从不同角度论证了“意识与智能的绑定”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如认知科学家默里·沙纳汉（Murray Shanahan）。他受“意识的全局工作空间理论GWT”（Global Workspace Theory）的启发。提出意识就像一个“信息中枢”，能把大脑里不同区域的信息。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如视觉、听觉和记忆整合起来。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">然后传递给整个大脑。这种整合能力。能帮助人类快速判断“哪些信息相关。哪些无关”，从而回避框架问题。不会陷入无意义的信息筛选。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/7Eg2MTM76jM_987.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">还有理论生物学家伊娃·雅布隆卡（Eva Jablonka）和西蒙娜·金斯伯格（Simona Ginsburg）。她们提出了“无限联想学习UAL”（Unlimited Associative Learning）的概念。这是一种特殊的学习能力。能让生物从过去的经验中。联想出全新的解决方案。而不是重复旧的行为。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如你第一次遇到“门锁坏了”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">可能会尝试“用卡片撬”，或者“用钥匙转”，如果都不行。你会想到“找物业”或者“找开锁公司”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这种“不重复、找新方法”的能力。就来自无限联想学习，而它的前提。是意识能“感知到问题的特殊性”，并且“整合过去的经验”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这些观点汇总起来，指向了一个结论。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在生物身上，至少在人类身上。“能摆脱无限循环的智能”，必须依赖于意识；。而意识的核心。是“与身体、环境、时间、熵的深度绑定”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">反过来，如果AI没有这种绑定。没有真正的意识。就永远无法拥有和人类一样的“开放式智能”，即通用人工智能AGI。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">它可能真的像阿尼尔·塞斯说的那样。是“一场虚幻泡影”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/7Eg2MTM76jM_1050.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这里还要澄清一个常见的误区。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">很多人觉得“AI只要够智能。就能产生意识”，但是阿尼尔·塞斯认为。这是“因果倒置”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">智能是“行动层面”的能力。意识是“存在层面”的能力。前者可以通过算法实现。后者需要“植根于时间与熵的具身体验”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如AI能写出流畅的文章。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">但它不会“知道自己在写文章”，也不会“因为写得好而开心”，更不会“担心自己写得不好”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">它没有“自我感知”，没有“情感效应”，所以它的“智能”是“无意识的智能”，一旦遇到需要“判断意义”的场景。比如避免循环。它就会暴露自己的局限性。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">最后，文章的后记中还提出一个疑问。那就是神经形态计算虽然能够模拟大脑的时序。但是它能复现意识里的“效应”吗？比如哪怕是微弱的“这个做法不对”的消极感受。或者“这样做有意义”的积极感受。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">阿尼尔·塞斯没有直接回答。但是从他的理论来看。答案可能是否定的。因为“效应”来自“生存驱动”，来自“身体对抗熵的本能”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/7Eg2MTM76jM_1113.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而神经形态计算只是模拟了大脑的“结构”，没有模拟身体的“代谢过程”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">没有“饿”、“渴”、“疼”（口误）这些最基础的生存信号。自然无法产生“效应”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">所以，AI的无限循环。看似是技术问题，其实是“存在问题”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">它不是“没学会如何不循环”，而是“没有不循环的理由”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">人类之所以能摆脱循环。是因为我们“活着”，我们需要在时间中维持自身的有序。需要通过有意义的行动对抗熵增；。而AI只是“运行着”，它没有“活着”的感知。没有时间的深度，没有对抗熵的本能。所以它会在无意义的循环里。一遍又一遍地重复，直到能量耗尽。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">阿尼尔·塞斯的研究。其实不只是在说AI的局限。更是在帮我们看清人类意识的独特性。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">我们的意识，不是“多余的装饰”，而是“生存的工具”，是“时间与熵赋予我们的礼物”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">或许未来某一天。AI能解决无限循环的问题。但那时候的它。可能已经不是“我们现在理解的AI”，而是一种全新的、真正“植根于时间与熵”的生命体。但是那一天，可能还非常遥远。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/7Eg2MTM76jM_1180.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">好了，今天的内容就到这里。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">如果你对“意识与AI”的话题感兴趣。推荐大家去读阿尼尔·塞斯的《意识机器：成为你自己》。里面对“具身意识”的解读更细腻。感谢收看本期视频，我们下期再见。</p>

<hr style="clear:both" />
=============
<p><a href="https://www.youtube.com/watch?v=7Eg2MTM76jM">https://www.youtube.com/watch?v=7Eg2MTM76jM</a></p><p>不知道你有没有过这样的经历，打客服电话的时候，按了十几次“1”选人工服务，却始终卡在“请确认您的需求”的循环里；或者导航软件让你“在前方50米掉头”，但是掉头后又重复同一个指令，最后你只能关掉软件靠自己找路。这些看似“低级”的故障，其实不是AI“笨”，而是它和我们人类在本质上存在一道鸿沟。今天我们要聊的，就是神经科学家阿尼尔·塞斯（Anil Seth）对这个问题的深度解读，他在文章中告诉我们，AI的无限循环，根源在于它没有“扎根”在时间和熵里，而这恰恰是人类意识的核心。</p><p><a href="https://bigthink.com/neuropsych/anil-seth-consciousness-time-perception/">https://bigthink.com/neuropsych/anil-seth-consciousness-time-perception/</a></p>]]>
      </description>
      <content:encoded><![CDATA[<hr style="clear:both" />

<p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://img.youtube.com/vi/7Eg2MTM76jM/maxresdefault.jpg" /></p><p><a href="https://www.youtube.com/watch?v=7Eg2MTM76jM">https://www.youtube.com/watch?v=7Eg2MTM76jM</a></p><h1>值得閱讀的理由</h1> <ul> <li>深入理解AI為何會陷入重複循環，超越表面技術故障的物理與哲學根源。</li> <li>探討人類意識與「時間」、「熵」的深度連結，揭示我們能擺脫重複行為的生物學基礎。</li> <li>認識神經科學家阿尼爾·塞斯對「智能」與「意識」的獨到見解，及其對通用人工智慧（AGI）未來發展的啟示。</li> </ul> <hr /> <h1>摘要</h1> <h2>AI困境：無限循環的現實案例與核心問題</h2> <p>影片首先透過生活中的多個例子，點出AI系統常出現的「無限循環」問題：從客服電話的無限按鍵迴圈、導航軟體的重複指令，到作者親身經歷的西班牙馬德里機場登機廊橋AI故障。這些看似低級的故障，實則反映了AI與人類本質上的根本差異。神經科學家<strong>阿尼爾·塞斯（Anil Seth）</strong>指出，AI的無限循環根源在於它缺乏「<strong>扎根於時間與熵</strong>」的能力，而這恰恰是人類意識的核心基石。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/7Eg2MTM76jM_95.jpg" /></p> <hr /> <h2>智能與意識的區分：AI的根本局限</h2> <p>阿尼爾·塞斯明確區分了「<strong>智能（Intelligence）</strong>」與「<strong>意識（Consciousness）</strong>」。智能關乎「行動」與「解決問題」，而意識則關乎「存在」與「感受」。他認為，儘管許多科技公司相信智能堆疊到一定程度便能產生意識，但這是個誤區。對人類而言，真正能擺脫無限循環的智能，必須依賴於<strong>意識所帶來的能力</strong>，而這種意識與時間的流動深度綁定。AI之所以容易陷入循環，是因為它沒有這種與時間、熵的「扎根」，也缺乏真正的意識。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/7Eg2MTM76jM_255.jpg" /></p> <hr /> <h2>AI無限循環的底層邏輯：圖靈停機問題與框架問題</h2> <p>影片進一步解釋了AI無限循環的底層科學依據。其一為近一世紀前艾倫·圖靈提出的「<strong>圖靈停機問題（Turing Halting Problem）</strong>」，該理論證明了不存在任何演算法能準確預判一個程式在所有情況下是否會無限循環。其二為1969年提出的「<strong>框架問題（Frame Problem）</strong>」，它指出AI難以區分「相關資訊」與「無關細節」，容易因過度篩選資訊而陷入停滯或循環。這兩個「老問題」在當今深度學習與生成式AI興起的時代似乎被淡忘，但實質上仍在現實中不斷體現AI的局限。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/7Eg2MTM76jM_565.jpg" /></p> <hr /> <h2>時間與熵：生命體與AI的關鍵分野</h2> <p>影片深入探討了「<strong>時間</strong>」與「<strong>熵</strong>」這兩個區分生命體與AI的核心概念。根據<strong>熱力學第二定律</strong>，在孤立系統中，熵（混亂程度）只會增加。然而，人類等生命體是「<strong>開放系統</strong>」，能透過獲取外界能量來「<strong>抗熵</strong>」（降低自身熵，維持有序狀態）。人類意識與時間深度綁定，體現在生化、神經及意識等多個時間尺度上，這種多層面的時間整合使我們能快速判斷並採取有意義的行動。相比之下，數字計算機的「時間」是<strong>扁平、一維</strong>的，它漠視時間流逝，其計算的本質是「狀態躍遷」，與熱力學第二定律脫鉤，導致AI需要消耗大量能量來抵銷熵增的影響（如散熱、糾錯），卻無法產生「生存驅動力」或「自我存在」的感知。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/7Eg2MTM76jM_1140.jpg" /></p> <hr /> <h2>人類的「抗熵本能」與具身意識</h2> <p>人類作為「身處時間之中」的生命，被阿尼爾·塞斯形容為「<strong>具身的（embodied）、植根於環境的（embedded）、與時間同行的（entimed）</strong>」。我們的每個決策都受到「時間壓力」和「生存驅動」的影響，這種「抗熵」的本能促使我們擺脫無意義的重複。即使人類會出現重複行為（如額葉受損、強迫症），那也通常是神經認知功能紊亂的結果，而非天生會循環。這些情況恰恰證明了當意識與時間、身體的連結出現問題時，人類也會陷入循環，進一步佐證了意識在擺脫循環中的關鍵作用。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/7Eg2MTM76jM_22.jpg" /></p> <hr /> <h2>未來AI的探索方向與局限性</h2> <p>影片探討了幾種旨在讓AI擺脫無限循環、實現類人智能的探索方向： <ul> <li><strong>模擬計算機（Analog Computers）</strong>：時間連續，但精度低且易受環境影響。</li> <li><strong>凡人計算（Mortal Computation）</strong>：透過硬體壽命限制，強制程式停止，但非讓AI真正「理解時間」。</li> <li><strong>神經形態計算（Neuromorphic Computing）</strong>：模擬大腦工作方式，更「植根於時間」，但缺乏「生存驅動」和「情感效應」。</li> <li><strong>動力系統方法（Dynamical Systems Approach）</strong>：關注系統在時間中的狀態變化，但仍缺乏「生命體的抗熵本能」。</li> </ul> 阿尼爾·塞斯認為，儘管這些新型AI技術在「靠近時間」，但都未能真正「<strong>扎根於時間與熵</strong>」，缺乏「主動抗熵」的生存驅動和「意識層面的感受」，因此可能仍無法完全擺脫無限循環的陰影，更難實現人類那種「開放式、適應性強」的通用人工智慧（AGI）。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/7Eg2MTM76jM_29.jpg" /></p> <hr /> <h2>意識是生存工具：AI為何沒有「不循環的理由」</h2> <p>阿尼爾·塞斯的結論是，人類能夠擺脫無限循環的能力，離不開<strong>意識所帶來的整合能力</strong>，這種能力與身體、環境、時間和熵深度綁定。他提出「<strong>具身意識</strong>」的概念，強調意識不是脫離身體的軟體，而是身體、大腦、環境在時間中互動的產物。其他認知科學家也支持這一觀點，如默里·沙納漢的「全局工作空間理論」和雅布隆卡與金斯伯格的「無限聯想學習」概念，都指出意識的整合能力對避免框架問題和尋找新解方的重要性。影片強調，AI的智能是「<strong>無意識的智能</strong>」，它缺乏「自我感知」、「情感效應」和「生存驅動」，因此在需要「判斷意義」的場景中會暴露局限。AI的無限循環並非是「沒學會如何不循環」，而是「<strong>沒有不循環的理由</strong>」。人類之所以能擺脫循環，是因為我們「活著」，需要在時間中維持自身有序，通過有意義的行動對抗熵增；而AI只是「運行著」，沒有「活著」的感知，沒有時間的深度，也缺乏對抗熵的本能。因此，阿尼爾·塞斯的研究不僅揭示了AI的局限，更彰顯了人類意識作為「<strong>生存工具</strong>」和「<strong>時間與熵的禮物</strong>」的獨特性。</p> <p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/7Eg2MTM76jM_36.jpg" /></p> <hr /><hr />================================================</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">大家好，这里是最佳拍档，我是大飞。不知道你有没有过这样的经历。打客服电话的时候。按了十几次“1”选人工服务。却始终卡在“请确认您的需求”的循环里；。或者导航软件让你“在前方50米掉头”，但是掉头后又重复同一个指令。最后你只能关掉软件靠自己找路。这些看似“低级”的故障。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">其实不是AI“笨”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而是它和我们人类在本质上存在一道鸿沟。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">今天我们要聊的。就是神经科学家阿尼尔·塞斯（Anil Seth）对这个问题的深度解读。他在文章中告诉我们，AI的无限循环。根源在于它没有“扎根”在时间和熵里。而这恰恰是人类意识的核心。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">先从一个真实的案例说起吧。这是阿尼尔·塞斯提到的亲身经历。他的航班准时降落在西班牙马德里巴拉哈斯机场。本以为下机很顺利。结果却因为登机廊桥的AI系统出了问题。所有人都被困在机舱里。透过舷窗能看到。那台由AI操控的登机廊桥。就像一个“卡住的玩具”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/7Eg2MTM76jM_59.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">先是慢慢靠近飞机舱门。离得近了又突然退回原位。接着再靠近、再退回。一遍又一遍。在清晨的冷空气中微微晃动。完全陷入了无限循环。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">最后还是一名工作人员赶过来。只看了几眼就找出了症结。几分钟就修好了。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这个场景特别有画面感。也特别耐人寻味。为什么AI会“钻牛角尖”似的重复同一动作。而人类哪怕没学过登机廊桥维修。也能快速判断问题的所在么？</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">阿尼尔·塞斯抛出了一个核心观点。那就是人类不会陷入无休止的重复行为。本质是因为我们是“扎根于时间与熵”的生命体。而这种与时间、熵的深度绑定。正是意识存在的重要根基。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">反过来，AI之所以容易陷循环。就是因为它既没有这种“扎根”，也没有真正的意识。哪怕未来AI的算力再强、模型再复杂。这个局限可能也绕不开。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">要理解这个问题，我们得先搞清楚。AI的“无限循环”到底是怎么回事？不是说AI不够智能。而是它的“智能”和人类的智能。底层逻辑是完全不同的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/7Eg2MTM76jM_125.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">阿尼尔·塞斯区分了两个关键概念。“智能”关乎“行动”，比如解决问题、执行任务；。而“意识”关乎“存在”或者“感受”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如你知道自己在“看视频”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">能感受到时间在流逝。能判断“这个指令不对”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">很多科技公司会觉得。智能堆到一定程度。意识自然就会出现。但是阿尼尔·塞斯认为，这是一个误区。至少在人类和其他动物身上。真正能摆脱“无限循环”的智能。必须依赖于意识所带来的能力。而这种意识。恰恰是和时间的流动深度绑定的。举个简单的例子，假设你写一个程序。让机器人完成“找红色物体”的任务。程序逻辑可能是，一直向前移动。直到摄像头识别到红色物体。然后停止。如果机器人面前始终没有红色物体。或者程序里“识别红色”的参数设错了。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如把橙色当成了红色。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">它就会一直往前走，永远停不下来。这就是最典型的AI无限循环。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">你可能会说。那给它加个监控系统不就行了？让它检测自己是不是走了太久。要是超过10分钟没找到。就停下来报警。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/7Eg2MTM76jM_190.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">但是问题来了。如果这个监控系统本身出了故障呢？比如它的“计时功能”坏了。永远显示“只走了1分钟”，那机器人还是会循环。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">你可能又会想，那再加一层监控。监控‘监控系统’不就好了？</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这就进入了“递归监控”的逻辑。每加一层，看似稳定性提升了。但是只要层级不是“无限”的。总有可能出问题。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如第一层监控管“找红色物体”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第二层管“第一层监控”，第三层管“第二层监控”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">可是你不可能加无限多层。最后总有某一层会失效。导致整个系统陷入循环。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这不是技术不够先进的问题。而是计算机科学的底层规律。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">近一个世纪前。艾伦·图灵（Alan Turing）就证明了这个结论。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这里必须提一下“图灵停机问题”（Turing Halting Problem）。这是理解AI局限的关键。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">1936年，艾伦·图灵在论文里提出。不存在任何一种算法。能够始终准确的判断。一个给定的程序，在输入某个数据后。最终会“终止运行”还是“无限循环”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">换句话说。无论你把程序写得多么精密。都没办法提前预知它在所有情况下会不会卡壳。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/7Eg2MTM76jM_255.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如刚才说的“找红色物体”的机器人。哪怕你测试了100次、1000次都没问题。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">但是第1001次的时候。可能因为光线的突变。让红色物体看起来像棕色。程序就判断失误，陷入循环。而你永远无法通过“算法”提前排除这种可能性。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">除了图灵停机问题。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">还有一个理论能解释AI的循环困境。那就是“框架问题”（Frame Problem）。1969年。约翰·麦卡锡（John McCarthy）和帕特里克·J·海耶斯（Patrick J。Hayes）提出了这个概念。它比图灵停机问题更具体。直指AI在“决策”上的核心难题。那就是要想教会机器只关注相关信息。而忽略无关的细节，是极其困难的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">举个例子。如果让AI控制一个家庭机器人。任务是“从客厅走到厨房。打开冰箱拿牛奶”。对人类来说。我们会自动忽略“客厅地毯的颜色”、“窗外的鸟叫”、“冰箱上的冰箱贴”这些无关信息。只关注“路线是否通畅”、“冰箱门能不能打开”、“牛奶在哪个格子里”。但是对于AI来说。它没办法天然区分“相关”和“无关”，它可能会思考“地毯颜色会不会影响走路速度？</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/7Eg2MTM76jM_323.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">”、“鸟叫是不是某种干扰信号？</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">”、“冰箱贴会不会挡住开门？”， 甚至会因为要“确认所有的细节”而陷入无限的信息筛选中。最后连“走第一步”都做不到。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">为什么会这样？因为在特定场景里。“无关信息”的数量可能是无穷的。比如从客厅到厨房。AI还可能考虑“空气湿度会不会影响电机运转？</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">”、“地板上的灰尘会不会卡住轮子？”，这些信息对人类来说毫无意义。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">但是AI没有一个“天然的过滤器”来判断“该忽略什么”，只能逐一排查，而排查的过程本身。就可能变成另一种形式的“无限循环”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">阿尼尔·塞斯在文章里说。随着深度神经网络、生成式AI的兴起。图灵停机问题和框架问题这些“老问题”似乎被淡忘了。但是它们从来没有消失。马德里机场的登机廊桥、客服电话的循环菜单、导航软件的错误指令。都是这些问题在现实中的体现。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">AI之所以绕不开这些困境。不是因为它“算力不够”，而是因为它和人类的生物大脑。在“时间感知”上存在根本差异。而这种差异。最终指向了“意识”的本质。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/7Eg2MTM76jM_391.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">接下来我们要聊的。就是整个问题的核心。时间与熵。这两个概念听起来很抽象。但是其实和我们每个人的存在都息息相关。也是区分“生命体”和“AI”的关键标尺。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">首先得解释一个物理学定律。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">热力学第二定律。这个定律说的是，在一个“孤立系统”，也就是和外界没有能量、物质交换的系统。比如一个完全密封的保温杯里。“熵”只会增加，或者保持不变。“熵”是什么？简单说就是“系统的混乱程度”，熵越高。混乱度越高。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如一滴墨水滴进清水里。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">会慢慢扩散，最后整杯水变成淡蓝色。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这个过程就是熵增；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而扩散后的墨水。永远不可能自己重新聚成“一滴”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">因为那会导致熵减。违背热力学第二定律。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">物理学家阿瑟·爱丁顿（Arthur Eddington）曾经说过。如果你的理论被证明违背了热力学第二定律。那我只能说你毫无希望；。除了彻底崩塌、颜面尽失。你别无出路。这句话足以说明这个定律的重要性。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">但是这里有个关键的前提。那就是热力学第二定律只适用于“孤立系统”。而“开放系统”，也就是和外界有能量、物质交换的系统。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/7Eg2MTM76jM_461.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如我们人类、植物、动物。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">是不一样的。开放系统可以通过从外界获取能量。来“降低自身的熵”，维持一个“有序”的状态。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如我们人类，每天吃饭、呼吸。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">就是从外界获取能量。用来维持身体细胞的新陈代谢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">如果不获取能量。身体就会逐渐混乱、衰退。也就是“熵增加”，最终走向死亡。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">阿尼尔·塞斯认为。生命系统的核心优势。就在于它是“开放系统”，并且这种“抗熵”的过程。是在多个层面、和时间深度绑定的。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如从最微观的“生化时间”来看。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">我们身体里的每个细胞。每秒都会发生十亿次生化反应。这些反应精准配合，维持着代谢平衡；。再到“神经时间”，神经信号的传递有快有慢。有髓鞘的神经纤维。信号传递速度能达到每秒120米。而神经递质在突触间的扩散。速度则慢得多，只有每秒几毫米；。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">再到我们能直接感受到的“意识时间”，它不是匀速的。开心的时候觉得“时间过得快”，等待的时候觉得“时间过得慢”，但它永远是“连续的、不可逆转的”，包含了“过去的记忆、现在的感受、未来的预期”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/7Eg2MTM76jM_530.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这些不同尺度的“时间”，不是孤立的。而是深度交织、相互依赖的。比如你饿了。这是身体代谢层面的信号。属于生化时间。大脑会接收到“需要进食”的神经信号。经历神经时间。你的意识会产生“想吃东西”的感受。又会经历意识时间。然后你会主动去找食物。整个过程。是从微观到宏观的时间整合。而这种整合。让人类能在复杂的环境里快速判断“该做什么”，不会陷入无意义的循环。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">但是AI完全不一样。对数字计算机来说。“时间”是扁平的、一维的。和热力学第二定律没有任何绑定。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">计算的本质，就是“状态的跃迁”，从0到1，从A到B。就像一串多米诺骨牌一样。推倒第一块，后面的依次倒下。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在图灵提出的经典计算模型里。只有“状态的顺序”是重要的。而“两个状态之间的时间间隔”毫无意义。比如一个计算步骤，间隔1微秒完成。和间隔100万年完成。对算法本身来说没有任何区别。算法的性质不会变。计算的结果也不会变。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/7Eg2MTM76jM_594.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这种“对时间的漠视”，其实是有代价的。阿尼尔·塞斯在文章里说。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">假装时间不存在。其实是一门昂贵的交易。为什么？因为计算机本身是“物理实体”，它无法摆脱热力学第二定律的影响。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如芯片会发热。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">内存会出错，这些都是“熵增”的体现。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">为了保证计算的准确性。AI系统需要消耗大量的能量来“纠错”和“降温”，抵消熵的影响。比如数据中心的服务器。大部分能量不是用来“计算”，而是用来“散热”，这就是AI“能量缺口”大的重要原因。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而AI之所以会陷入无限循环。核心就在于它的“时间观”，它被困在“状态序列”里。不受熵的“拉扯”，也没有“生存的驱动力”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如马德里机场的登机廊桥AI。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">它只会执行“靠近-检测-退回”的指令。不会因为“反复做同一件事没意义”而停止。也不会因为“消耗了太多能量”而主动调整。它没有“这样做不对”的意识。因为它没有“自我存在”的感知。也没有“时间流逝”的概念。但是人类不一样。我们是“身处时间之中”的生命。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/7Eg2MTM76jM_658.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">阿尼尔·塞斯用了三个词来形容：“具身的（embodied）、植根于环境的（embedded）、与时间同行的（entimed）”。我们的每一个决策。都受“时间压力”和“生存驱动”的影响。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如你在沙漠里迷路。哪怕你一开始走错了。也不会反复绕同一个圈子。因为你知道“时间越久，水分越少。生存概率越低”，这种“抗熵”的本能。会让你主动寻找新的方向。摆脱可能的循环。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">阿尼尔·塞斯在文章里也提到了一个例外。那就是人类其实也有“重复行为”，比如额叶受损的人会反复做同一个动作。强迫症患者会反复洗手。成瘾者会重复自我毁灭的行为。但是这些案例恰恰印证了他的观点。这些都是“正常神经认知功能紊乱”的结果。是意识与时间、身体的连接出了问题。而不是“人类天生会循环”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">一旦神经功能恢复。这种重复行为就会消失。因为“抗熵”和“时间感知”的本能会重新起作用。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">如果这个逻辑成立。那一个关键的问题就来了。未来的AI能不能通过技术改进。拥有这种与时间、熵绑定的能力呢。能不能摆脱无限循环。实现像人类一样的开放式智能呢。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/7Eg2MTM76jM_728.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">阿尼尔·塞斯分析了几种目前的探索方向。我们一个一个来看。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第一种是“模拟计算机”（Analog Computers）。这是最早的计算机形式。比如有2000年历史的“安提基特拉机械”（Antikythera mechanism）。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这是古希腊人发明的天文装置。能通过齿轮的连续转动。预测日食、月食和行星的位置。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">模拟计算机的特点是“时间是连续的”，不像数字计算机那样是“离散的状态跃迁”，它更接近人类对时间的感知。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">但是模拟计算机的问题是“精度低”，容易受到环境的影响。比如温度、湿度会改变齿轮的转动速度。所以后来被数字计算机取代了。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第二种是“凡人计算”（Mortal Computation）。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这个概念很有意思。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">它的核心是“让计算依赖硬件的寿命”，比如一个程序，只有在特定的硬件。比如某块芯片正常工作的时候才能运行。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">一旦硬件“失效”，比如芯片老化、发热损坏。程序也会随之“消亡”。这种设计的思路。是主动“拥抱”热力学第二定律。而不是“对抗”它。通过硬件的“有限寿命”，强制程序“不会无限运行”，从而避免循环。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/7Eg2MTM76jM_792.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">阿尼尔·塞斯认为。这种方法能提高能量利用效率。但它本质上是“用硬件限制来解决软件问题”，并没有让AI真正“理解时间”，只是用物理手段切断了循环的可能。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第三种是“神经形态计算”（Neuromorphic Computing）。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这是目前很热门的方向。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">核心是“模拟人类大脑的工作方式”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如用特殊的芯片。模拟神经元的信号传递。让计算过程更接近“神经时间”的有快有慢。和数字计算机相比。神经形态计算更“植根于时间”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如它的信号传递不是“非0即1”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而是“连续的电压变化”，更像大脑的神经活动。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">但是阿尼尔·塞斯怀疑。这种方法还是无法解决根本的问题。因为它只是“模拟了大脑的结构”，却没有模拟大脑的“生存驱动”和“情感效应”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如“饿”、“痛”、“开心”这些感受。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而这些恰恰是人类摆脱循环的关键。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">第四种是动力系统方法（Dynamical Systems Approach）。这种方法完全跳出了“计算”的思路。不关注“算法”，而是关注“系统在时间中的状态变化”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/7Eg2MTM76jM_855.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如用“吸引子”（Attractor）、“相空间”（Phase Space）这些概念。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">描述AI如何在不同时间点调整状态。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">它的源头可以追溯到AI发展初期的“控制论”（Cybernetics）。控制论关注的是“反馈与调控”，比如“温度高了就降温。温度低了就升温”，而不是“执行固定的指令”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这种方法更重视“时间的动态性”，但是阿尼尔·塞斯认为。它依然缺乏“生命体的抗熵本能”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">AI还是没有“维持自身有序”的驱动力。只是在被动响应环境变化。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">总结下来，这些新型AI技术。确实在“靠近时间”，但是都没有真正“扎根于时间与熵”，它们要么是用硬件来限制。要么是模拟结构。要么是被动响应。却没有“主动抗熵”的生存驱动。也没有“意识层面的感受”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">所以阿尼尔·塞斯的结论是。哪怕采用这些方法。AI可能还是无法完全摆脱无限循环的阴影。更难实现人类那种“开放式、适应性强”的智能。为什么呢？因为这种智能，不仅和时间绑定。还和“意识”深度绑定。至少在人类身上，摆脱循环的能力。离不开意识带来的“整合能力”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/7Eg2MTM76jM_918.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">阿尼尔·塞斯在他的著作《意识机器：。成为你自己》（Being You:。A New Science of Consciousness）里写过一句话。我们感知周遭世界。也感知身处其中的自己。这一切都依赖于我们鲜活的身体。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这句话的核心是“具身意识”，指的是。意识不是“脱离身体的软件”，而是“身体、大脑、环境在时间中互动的产物”。比如你看到一杯水。意识会告诉你“这杯水能解渴”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而这个判断。来自你过去“喝水解渴”的记忆、当下“口渴”的身体信号、以及“杯子里的水是干净的”的环境判断。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这种多层面的整合。让你不会“反复看杯子却不喝水”，因为意识会引导你“做有意义的行动”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">除了阿尼尔·塞斯。还有其他学者也从不同角度论证了“意识与智能的绑定”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如认知科学家默里·沙纳汉（Murray Shanahan）。他受“意识的全局工作空间理论GWT”（Global Workspace Theory）的启发。提出意识就像一个“信息中枢”，能把大脑里不同区域的信息。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如视觉、听觉和记忆整合起来。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">然后传递给整个大脑。这种整合能力。能帮助人类快速判断“哪些信息相关。哪些无关”，从而回避框架问题。不会陷入无意义的信息筛选。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/7Eg2MTM76jM_987.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">还有理论生物学家伊娃·雅布隆卡（Eva Jablonka）和西蒙娜·金斯伯格（Simona Ginsburg）。她们提出了“无限联想学习UAL”（Unlimited Associative Learning）的概念。这是一种特殊的学习能力。能让生物从过去的经验中。联想出全新的解决方案。而不是重复旧的行为。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如你第一次遇到“门锁坏了”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">可能会尝试“用卡片撬”，或者“用钥匙转”，如果都不行。你会想到“找物业”或者“找开锁公司”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这种“不重复、找新方法”的能力。就来自无限联想学习，而它的前提。是意识能“感知到问题的特殊性”，并且“整合过去的经验”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这些观点汇总起来，指向了一个结论。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">在生物身上，至少在人类身上。“能摆脱无限循环的智能”，必须依赖于意识；。而意识的核心。是“与身体、环境、时间、熵的深度绑定”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">反过来，如果AI没有这种绑定。没有真正的意识。就永远无法拥有和人类一样的“开放式智能”，即通用人工智能AGI。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">它可能真的像阿尼尔·塞斯说的那样。是“一场虚幻泡影”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/7Eg2MTM76jM_1050.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">这里还要澄清一个常见的误区。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">很多人觉得“AI只要够智能。就能产生意识”，但是阿尼尔·塞斯认为。这是“因果倒置”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">智能是“行动层面”的能力。意识是“存在层面”的能力。前者可以通过算法实现。后者需要“植根于时间与熵的具身体验”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">比如AI能写出流畅的文章。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">但它不会“知道自己在写文章”，也不会“因为写得好而开心”，更不会“担心自己写得不好”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">它没有“自我感知”，没有“情感效应”，所以它的“智能”是“无意识的智能”，一旦遇到需要“判断意义”的场景。比如避免循环。它就会暴露自己的局限性。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">最后，文章的后记中还提出一个疑问。那就是神经形态计算虽然能够模拟大脑的时序。但是它能复现意识里的“效应”吗？比如哪怕是微弱的“这个做法不对”的消极感受。或者“这样做有意义”的积极感受。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">阿尼尔·塞斯没有直接回答。但是从他的理论来看。答案可能是否定的。因为“效应”来自“生存驱动”，来自“身体对抗熵的本能”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/7Eg2MTM76jM_1113.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">而神经形态计算只是模拟了大脑的“结构”，没有模拟身体的“代谢过程”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">没有“饿”、“渴”、“疼”（口误）这些最基础的生存信号。自然无法产生“效应”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">所以，AI的无限循环。看似是技术问题，其实是“存在问题”，</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">它不是“没学会如何不循环”，而是“没有不循环的理由”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">人类之所以能摆脱循环。是因为我们“活着”，我们需要在时间中维持自身的有序。需要通过有意义的行动对抗熵增；。而AI只是“运行着”，它没有“活着”的感知。没有时间的深度，没有对抗熵的本能。所以它会在无意义的循环里。一遍又一遍地重复，直到能量耗尽。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">阿尼尔·塞斯的研究。其实不只是在说AI的局限。更是在帮我们看清人类意识的独特性。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">我们的意识，不是“多余的装饰”，而是“生存的工具”，是“时间与熵赋予我们的礼物”。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">或许未来某一天。AI能解决无限循环的问题。但那时候的它。可能已经不是“我们现在理解的AI”，而是一种全新的、真正“植根于时间与熵”的生命体。但是那一天，可能还非常遥远。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; "><p><img src="https://democwise2016.github.io/action-RSS-UT-Daily-202505/file-cache/7Eg2MTM76jM_1180.jpg" /></p></p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">好了，今天的内容就到这里。</p><p style="max-width: calc(100vw - 1rem);  word-wrap: break-word; overflow-wrap: break-word; ">如果你对“意识与AI”的话题感兴趣。推荐大家去读阿尼尔·塞斯的《意识机器：成为你自己》。里面对“具身意识”的解读更细腻。感谢收看本期视频，我们下期再见。</p>

<hr style="clear:both" />
=============
<p><a href="https://www.youtube.com/watch?v=7Eg2MTM76jM">https://www.youtube.com/watch?v=7Eg2MTM76jM</a></p><p>不知道你有没有过这样的经历，打客服电话的时候，按了十几次“1”选人工服务，却始终卡在“请确认您的需求”的循环里；或者导航软件让你“在前方50米掉头”，但是掉头后又重复同一个指令，最后你只能关掉软件靠自己找路。这些看似“低级”的故障，其实不是AI“笨”，而是它和我们人类在本质上存在一道鸿沟。今天我们要聊的，就是神经科学家阿尼尔·塞斯（Anil Seth）对这个问题的深度解读，他在文章中告诉我们，AI的无限循环，根源在于它没有“扎根”在时间和熵里，而这恰恰是人类意识的核心。</p><p><a href="https://bigthink.com/neuropsych/anil-seth-consciousness-time-perception/">https://bigthink.com/neuropsych/anil-seth-consciousness-time-perception/</a></p>]]></content:encoded>
      <itunes:image href="https://i.ytimg.com/vi/7Eg2MTM76jM/hqdefault.jpg"/>
      <pubDate>2025-09-29T09:00:10.000Z</pubDate>
    </item></channel>
</rss>